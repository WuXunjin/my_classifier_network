{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主要是为了尝试一下 重构原图对conv5特征做约束对特征的影响,利用欧式距离计算损失吧\n",
      "如果这个实验效果不好的话，准备实现将conv1的特征加入conv5的特征做联合，再进行分类约束和重构约束\n",
      "加载数据集完成\n"
     ]
    }
   ],
   "source": [
    "print '主要是为了尝试一下 重构原图对conv5特征做约束对特征的影响,利用欧式距离计算损失吧'\n",
    "print '如果这个实验效果不好的话，准备实现将conv1的特征加入conv5的特征做联合，再进行分类约束和重构约束'\n",
    "import lib.Experiment as ex\n",
    "from datasets.VOC_dataset_aug import VOC_dataset_aug_rec_conv5\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score\n",
    "from torch.autograd.variable import Variable\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import networks.VGG16_rec_conv5 as vgg_model\n",
    "from lib.loss import rec_conv5_criterion\n",
    "import os\n",
    "import networks.test as ts\n",
    "def compute_mAP(labels,outputs):\n",
    "    y_true = labels.cpu().numpy()\n",
    "    y_pred = outputs.cpu().numpy()\n",
    "    AP = []\n",
    "    for i in range(y_true.shape[0]):\n",
    "        AP.append(average_precision_score(y_true[i],y_pred[i]))\n",
    "    return np.mean(AP)\n",
    "ex_dir='./data/training_luo_rec_conv5_10582aug'\n",
    "batch_size=4\n",
    "workers=4\n",
    "epoch_num=20\n",
    "ex.check_dir(ex_dir)\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "voc_train=VOC_dataset_aug_rec_conv5(train='train', transform=transforms.Compose([transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "    ]), label_transform=None )\n",
    "voc_val=VOC_dataset_aug_rec_conv5(train='val', transform=transforms.Compose([transforms.Resize((224, 224)),transforms.ToTensor()]),\n",
    "                    label_transform=None  )\n",
    "train_loader = torch.utils.data.DataLoader(voc_train,\n",
    "    batch_size=batch_size, shuffle=True,\n",
    "    num_workers=workers, pin_memory=True)\n",
    "val_loader = torch.utils.data.DataLoader(voc_val,\n",
    "    batch_size=batch_size, shuffle=False,\n",
    "    num_workers=workers, pin_memory=True)\n",
    "print '加载数据集完成'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载模型和损失完成\n"
     ]
    }
   ],
   "source": [
    "model=vgg_model.get_VGG_rec_conv5()\n",
    "model.cuda()\n",
    "#criterion = rec_conv5_criterion()\n",
    "cls_criterion = nn.MultiLabelSoftMarginLoss()\n",
    "# 重构损失 先用均方误差\n",
    "rec_criterion=nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "print '加载模型和损失完成'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练\n",
      "red==train;green==val...initial plt\n",
      "train epoch 0 step 49,cls loss=0.097866,rec_loss=0.026429,loss=0.124295,mAp=86.2819\n",
      "train epoch 0 step 99,cls loss=0.103319,rec_loss=0.025749,loss=0.129068,mAp=85.9522\n",
      "train epoch 0 step 149,cls loss=0.102351,rec_loss=0.025191,loss=0.127542,mAp=85.5957\n",
      "train epoch 0 step 199,cls loss=0.102773,rec_loss=0.025476,loss=0.128249,mAp=85.4079\n",
      "train epoch 0 step 249,cls loss=0.103799,rec_loss=0.025202,loss=0.129001,mAp=85.0727\n",
      "train epoch 0 step 299,cls loss=0.103662,rec_loss=0.025129,loss=0.128791,mAp=85.0888\n",
      "train epoch 0 step 349,cls loss=0.102162,rec_loss=0.025157,loss=0.127318,mAp=85.1753\n",
      "train epoch 0 step 399,cls loss=0.100278,rec_loss=0.025058,loss=0.125336,mAp=85.9187\n",
      "train epoch 0 step 449,cls loss=0.102384,rec_loss=0.025036,loss=0.127421,mAp=85.4743\n",
      "train epoch 0 step 499,cls loss=0.104006,rec_loss=0.024981,loss=0.128987,mAp=85.0816\n",
      "train epoch 0 step 549,cls loss=0.104097,rec_loss=0.025095,loss=0.129192,mAp=85.2157\n",
      "train epoch 0 step 599,cls loss=0.104259,rec_loss=0.024954,loss=0.129213,mAp=85.2180\n",
      "train epoch 0 step 649,cls loss=0.104945,rec_loss=0.024891,loss=0.129836,mAp=84.9571\n",
      "train epoch 0 step 699,cls loss=0.104590,rec_loss=0.024785,loss=0.129376,mAp=85.0099\n",
      "train epoch 0 step 749,cls loss=0.105356,rec_loss=0.024744,loss=0.130101,mAp=84.7019\n",
      "train epoch 0 step 799,cls loss=0.105875,rec_loss=0.024751,loss=0.130627,mAp=84.6085\n",
      "train epoch 0 step 849,cls loss=0.106099,rec_loss=0.024756,loss=0.130854,mAp=84.5190\n",
      "train epoch 0 step 899,cls loss=0.106281,rec_loss=0.024778,loss=0.131059,mAp=84.4816\n",
      "train epoch 0 step 949,cls loss=0.106862,rec_loss=0.024738,loss=0.131600,mAp=84.1327\n",
      "train epoch 0 step 999,cls loss=0.107047,rec_loss=0.024766,loss=0.131812,mAp=84.0635\n",
      "train epoch 0 step 1049,cls loss=0.107107,rec_loss=0.024691,loss=0.131798,mAp=84.0414\n",
      "train epoch 0 step 1099,cls loss=0.107001,rec_loss=0.024705,loss=0.131706,mAp=84.1237\n",
      "train epoch 0 step 1149,cls loss=0.106657,rec_loss=0.024701,loss=0.131358,mAp=84.2389\n",
      "train epoch 0 step 1199,cls loss=0.106142,rec_loss=0.024707,loss=0.130849,mAp=84.3564\n",
      "train epoch 0 step 1249,cls loss=0.106382,rec_loss=0.024681,loss=0.131063,mAp=84.3876\n",
      "train epoch 0 step 1299,cls loss=0.106575,rec_loss=0.024639,loss=0.131214,mAp=84.4278\n",
      "train epoch 0 step 1349,cls loss=0.106584,rec_loss=0.024668,loss=0.131252,mAp=84.4342\n",
      "train epoch 0 step 1399,cls loss=0.106368,rec_loss=0.024696,loss=0.131064,mAp=84.4192\n",
      "train epoch 0 step 1449,cls loss=0.107089,rec_loss=0.024730,loss=0.131819,mAp=84.2015\n",
      "train epoch 0 step 1499,cls loss=0.106977,rec_loss=0.024726,loss=0.131703,mAp=84.2251\n",
      "train epoch 0 step 1549,cls loss=0.106946,rec_loss=0.024710,loss=0.131656,mAp=84.2077\n",
      "train epoch 0 step 1599,cls loss=0.106795,rec_loss=0.024703,loss=0.131497,mAp=84.2745\n",
      "train epoch 0 step 1649,cls loss=0.106831,rec_loss=0.024672,loss=0.131503,mAp=84.2404\n",
      "train epoch 0 step 1699,cls loss=0.106126,rec_loss=0.024625,loss=0.130752,mAp=84.4050\n",
      "train epoch 0 step 1749,cls loss=0.105999,rec_loss=0.024574,loss=0.130573,mAp=84.4039\n",
      "train epoch 0 step 1799,cls loss=0.106247,rec_loss=0.024522,loss=0.130769,mAp=84.4109\n",
      "train epoch 0 step 1849,cls loss=0.106190,rec_loss=0.024476,loss=0.130666,mAp=84.3817\n",
      "train epoch 0 step 1899,cls loss=0.106200,rec_loss=0.024442,loss=0.130641,mAp=84.4658\n",
      "train epoch 0 step 1949,cls loss=0.106040,rec_loss=0.024418,loss=0.130459,mAp=84.4366\n",
      "train epoch 0 step 1999,cls loss=0.105747,rec_loss=0.024398,loss=0.130145,mAp=84.4775\n",
      "train epoch 0 step 2049,cls loss=0.105727,rec_loss=0.024375,loss=0.130102,mAp=84.5369\n",
      "train epoch 0 step 2099,cls loss=0.105769,rec_loss=0.024339,loss=0.130108,mAp=84.5504\n",
      "train epoch 0 step 2149,cls loss=0.105684,rec_loss=0.024290,loss=0.129974,mAp=84.5798\n",
      "train epoch 0 step 2199,cls loss=0.105356,rec_loss=0.024225,loss=0.129581,mAp=84.6258\n",
      "train epoch 0 step 2249,cls loss=0.105314,rec_loss=0.024218,loss=0.129532,mAp=84.6018\n",
      "train epoch 0 step 2299,cls loss=0.105315,rec_loss=0.024214,loss=0.129529,mAp=84.6273\n",
      "train epoch 0 step 2349,cls loss=0.105150,rec_loss=0.024203,loss=0.129353,mAp=84.6492\n",
      "train epoch 0 step 2399,cls loss=0.105217,rec_loss=0.024197,loss=0.129414,mAp=84.6562\n",
      "train epoch 0 step 2449,cls loss=0.105084,rec_loss=0.024168,loss=0.129252,mAp=84.6525\n",
      "train epoch 0 step 2499,cls loss=0.105037,rec_loss=0.024157,loss=0.129194,mAp=84.6370\n",
      "train epoch 0 step 2549,cls loss=0.104945,rec_loss=0.024125,loss=0.129070,mAp=84.6969\n",
      "train epoch 0 step 2599,cls loss=0.104567,rec_loss=0.024094,loss=0.128662,mAp=84.7487\n",
      "val epoch 0 step 19,cls loss=0.126035,rec_loss=0.027255,loss=0.153291,mAp=84.3988\n",
      "val epoch 0 step 39,cls loss=0.122405,rec_loss=0.026461,loss=0.148865,mAp=83.9315\n",
      "val epoch 0 step 59,cls loss=0.122365,rec_loss=0.025947,loss=0.148312,mAp=82.9083\n",
      "val epoch 0 step 79,cls loss=0.119185,rec_loss=0.026298,loss=0.145483,mAp=83.4800\n",
      "val epoch 0 step 99,cls loss=0.111499,rec_loss=0.025922,loss=0.137421,mAp=84.7905\n",
      "val epoch 0 step 119,cls loss=0.105141,rec_loss=0.025564,loss=0.130705,mAp=85.9446\n",
      "val epoch 0 step 139,cls loss=0.102518,rec_loss=0.025167,loss=0.127685,mAp=85.7953\n",
      "val epoch 0 step 159,cls loss=0.104928,rec_loss=0.024855,loss=0.129783,mAp=84.5619\n",
      "val epoch 0 step 179,cls loss=0.103708,rec_loss=0.024645,loss=0.128353,mAp=84.3781\n",
      "val epoch 0 step 199,cls loss=0.101713,rec_loss=0.024609,loss=0.126322,mAp=84.7038\n",
      "val epoch 0 step 219,cls loss=0.101477,rec_loss=0.024591,loss=0.126068,mAp=85.0951\n",
      "val epoch 0 step 239,cls loss=0.100712,rec_loss=0.024506,loss=0.125218,mAp=85.1463\n",
      "val epoch 0 step 259,cls loss=0.100587,rec_loss=0.024487,loss=0.125074,mAp=85.1885\n",
      "val epoch 0 step 279,cls loss=0.102479,rec_loss=0.024574,loss=0.127054,mAp=85.3381\n",
      "val epoch 0 step 299,cls loss=0.103105,rec_loss=0.024466,loss=0.127571,mAp=85.4470\n",
      "val epoch 0 step 319,cls loss=0.104746,rec_loss=0.024645,loss=0.129391,mAp=85.3178\n",
      "val epoch 0 step 339,cls loss=0.106714,rec_loss=0.024604,loss=0.131318,mAp=84.9358\n",
      "val epoch 0 step 359,cls loss=0.109012,rec_loss=0.024501,loss=0.133514,mAp=84.6141\n",
      "epoch 0 done!\n",
      "train epoch 1 step 49,cls loss=0.082784,rec_loss=0.022574,loss=0.105358,mAp=89.1091\n",
      "train epoch 1 step 99,cls loss=0.084172,rec_loss=0.022578,loss=0.106750,mAp=88.8890\n",
      "train epoch 1 step 149,cls loss=0.084702,rec_loss=0.022613,loss=0.107315,mAp=88.7338\n",
      "train epoch 1 step 199,cls loss=0.083440,rec_loss=0.022869,loss=0.106308,mAp=89.1893\n",
      "train epoch 1 step 249,cls loss=0.080501,rec_loss=0.022933,loss=0.103434,mAp=89.7839\n",
      "train epoch 1 step 299,cls loss=0.081878,rec_loss=0.022903,loss=0.104781,mAp=89.5372\n",
      "train epoch 1 step 349,cls loss=0.082655,rec_loss=0.022890,loss=0.105545,mAp=89.3523\n",
      "train epoch 1 step 399,cls loss=0.083866,rec_loss=0.022684,loss=0.106550,mAp=89.2359\n",
      "train epoch 1 step 449,cls loss=0.084546,rec_loss=0.022741,loss=0.107287,mAp=89.0912\n",
      "train epoch 1 step 499,cls loss=0.084819,rec_loss=0.022690,loss=0.107509,mAp=89.0960\n",
      "train epoch 1 step 549,cls loss=0.085487,rec_loss=0.022761,loss=0.108249,mAp=88.9466\n",
      "train epoch 1 step 599,cls loss=0.085079,rec_loss=0.022652,loss=0.107731,mAp=89.2029\n",
      "train epoch 1 step 649,cls loss=0.084947,rec_loss=0.022628,loss=0.107575,mAp=89.1979\n",
      "train epoch 1 step 699,cls loss=0.085150,rec_loss=0.022666,loss=0.107816,mAp=89.1888\n",
      "train epoch 1 step 749,cls loss=0.085293,rec_loss=0.022584,loss=0.107877,mAp=89.3465\n",
      "train epoch 1 step 799,cls loss=0.085063,rec_loss=0.022586,loss=0.107649,mAp=89.3817\n",
      "train epoch 1 step 849,cls loss=0.085355,rec_loss=0.022642,loss=0.107997,mAp=89.3190\n",
      "train epoch 1 step 899,cls loss=0.085834,rec_loss=0.022629,loss=0.108463,mAp=89.2460\n",
      "train epoch 1 step 949,cls loss=0.085417,rec_loss=0.022568,loss=0.107985,mAp=89.2395\n",
      "train epoch 1 step 999,cls loss=0.085466,rec_loss=0.022491,loss=0.107957,mAp=89.1499\n",
      "train epoch 1 step 1049,cls loss=0.085317,rec_loss=0.022430,loss=0.107746,mAp=89.1653\n",
      "train epoch 1 step 1099,cls loss=0.085739,rec_loss=0.022405,loss=0.108144,mAp=89.1113\n",
      "train epoch 1 step 1149,cls loss=0.085406,rec_loss=0.022422,loss=0.107828,mAp=89.2398\n",
      "train epoch 1 step 1199,cls loss=0.085763,rec_loss=0.022401,loss=0.108164,mAp=89.1940\n",
      "train epoch 1 step 1249,cls loss=0.085844,rec_loss=0.022375,loss=0.108219,mAp=89.0866\n",
      "train epoch 1 step 1299,cls loss=0.086061,rec_loss=0.022351,loss=0.108411,mAp=88.9954\n",
      "train epoch 1 step 1349,cls loss=0.085930,rec_loss=0.022362,loss=0.108292,mAp=88.9909\n",
      "train epoch 1 step 1399,cls loss=0.086296,rec_loss=0.022349,loss=0.108644,mAp=88.9289\n",
      "train epoch 1 step 1449,cls loss=0.086081,rec_loss=0.022240,loss=0.108321,mAp=88.9309\n",
      "train epoch 1 step 1499,cls loss=0.086084,rec_loss=0.022257,loss=0.108340,mAp=88.9315\n",
      "train epoch 1 step 1549,cls loss=0.085750,rec_loss=0.022265,loss=0.108015,mAp=88.9886\n",
      "train epoch 1 step 1599,cls loss=0.086123,rec_loss=0.022325,loss=0.108448,mAp=88.9038\n",
      "train epoch 1 step 1649,cls loss=0.086098,rec_loss=0.022316,loss=0.108414,mAp=88.8390\n",
      "train epoch 1 step 1699,cls loss=0.086174,rec_loss=0.022275,loss=0.108449,mAp=88.8114\n",
      "train epoch 1 step 1749,cls loss=0.086167,rec_loss=0.022255,loss=0.108422,mAp=88.7606\n",
      "train epoch 1 step 1799,cls loss=0.086155,rec_loss=0.022229,loss=0.108383,mAp=88.7771\n",
      "train epoch 1 step 1849,cls loss=0.086232,rec_loss=0.022214,loss=0.108446,mAp=88.7834\n",
      "train epoch 1 step 1899,cls loss=0.086004,rec_loss=0.022183,loss=0.108187,mAp=88.8424\n",
      "train epoch 1 step 1949,cls loss=0.086215,rec_loss=0.022165,loss=0.108380,mAp=88.8216\n",
      "train epoch 1 step 1999,cls loss=0.086191,rec_loss=0.022130,loss=0.108321,mAp=88.8141\n",
      "train epoch 1 step 2049,cls loss=0.086319,rec_loss=0.022129,loss=0.108448,mAp=88.7625\n",
      "train epoch 1 step 2099,cls loss=0.086230,rec_loss=0.022121,loss=0.108351,mAp=88.7306\n",
      "train epoch 1 step 2149,cls loss=0.086258,rec_loss=0.022109,loss=0.108367,mAp=88.7045\n",
      "train epoch 1 step 2199,cls loss=0.086209,rec_loss=0.022088,loss=0.108297,mAp=88.7373\n",
      "train epoch 1 step 2249,cls loss=0.086249,rec_loss=0.022104,loss=0.108354,mAp=88.7029\n",
      "train epoch 1 step 2299,cls loss=0.086185,rec_loss=0.022108,loss=0.108293,mAp=88.7112\n",
      "train epoch 1 step 2349,cls loss=0.086202,rec_loss=0.022057,loss=0.108259,mAp=88.7283\n",
      "train epoch 1 step 2399,cls loss=0.086240,rec_loss=0.022035,loss=0.108275,mAp=88.7738\n",
      "train epoch 1 step 2449,cls loss=0.086164,rec_loss=0.022011,loss=0.108175,mAp=88.7904\n",
      "train epoch 1 step 2499,cls loss=0.086233,rec_loss=0.022010,loss=0.108243,mAp=88.7667\n",
      "train epoch 1 step 2549,cls loss=0.086134,rec_loss=0.021995,loss=0.108128,mAp=88.7929\n",
      "train epoch 1 step 2599,cls loss=0.085978,rec_loss=0.021973,loss=0.107951,mAp=88.8009\n",
      "val epoch 1 step 19,cls loss=0.133533,rec_loss=0.023624,loss=0.157157,mAp=82.4239\n",
      "val epoch 1 step 39,cls loss=0.124144,rec_loss=0.023049,loss=0.147193,mAp=83.5342\n",
      "val epoch 1 step 59,cls loss=0.121633,rec_loss=0.022804,loss=0.144437,mAp=83.3583\n",
      "val epoch 1 step 79,cls loss=0.123386,rec_loss=0.023160,loss=0.146546,mAp=83.0365\n",
      "val epoch 1 step 99,cls loss=0.117110,rec_loss=0.023011,loss=0.140121,mAp=84.0633\n",
      "val epoch 1 step 119,cls loss=0.110917,rec_loss=0.022588,loss=0.133505,mAp=84.9277\n",
      "val epoch 1 step 139,cls loss=0.107484,rec_loss=0.022300,loss=0.129784,mAp=84.7986\n",
      "val epoch 1 step 159,cls loss=0.107667,rec_loss=0.022041,loss=0.129708,mAp=84.1840\n",
      "val epoch 1 step 179,cls loss=0.104671,rec_loss=0.021861,loss=0.126532,mAp=84.2968\n",
      "val epoch 1 step 199,cls loss=0.103447,rec_loss=0.021812,loss=0.125259,mAp=84.2945\n",
      "val epoch 1 step 219,cls loss=0.103748,rec_loss=0.021836,loss=0.125583,mAp=84.4472\n",
      "val epoch 1 step 239,cls loss=0.101967,rec_loss=0.021719,loss=0.123685,mAp=84.7827\n",
      "val epoch 1 step 259,cls loss=0.101765,rec_loss=0.021769,loss=0.123534,mAp=84.9312\n",
      "val epoch 1 step 279,cls loss=0.103553,rec_loss=0.021895,loss=0.125448,mAp=85.0868\n",
      "val epoch 1 step 299,cls loss=0.104514,rec_loss=0.021785,loss=0.126299,mAp=85.2037\n",
      "val epoch 1 step 319,cls loss=0.106805,rec_loss=0.021936,loss=0.128742,mAp=84.8176\n",
      "val epoch 1 step 339,cls loss=0.108929,rec_loss=0.021938,loss=0.130866,mAp=84.5326\n",
      "val epoch 1 step 359,cls loss=0.111243,rec_loss=0.021860,loss=0.133103,mAp=84.1872\n",
      "epoch 1 done!\n",
      "train epoch 2 step 49,cls loss=0.072205,rec_loss=0.021481,loss=0.093686,mAp=92.1203\n",
      "train epoch 2 step 99,cls loss=0.061601,rec_loss=0.021145,loss=0.082747,mAp=94.2052\n",
      "train epoch 2 step 149,cls loss=0.062671,rec_loss=0.020831,loss=0.083502,mAp=93.8956\n",
      "train epoch 2 step 199,cls loss=0.066083,rec_loss=0.021155,loss=0.087239,mAp=92.7665\n",
      "train epoch 2 step 249,cls loss=0.066530,rec_loss=0.021144,loss=0.087674,mAp=92.5939\n",
      "train epoch 2 step 299,cls loss=0.068040,rec_loss=0.021415,loss=0.089455,mAp=92.3497\n",
      "train epoch 2 step 349,cls loss=0.068525,rec_loss=0.021330,loss=0.089855,mAp=92.0329\n",
      "train epoch 2 step 399,cls loss=0.069259,rec_loss=0.021217,loss=0.090476,mAp=91.8956\n",
      "train epoch 2 step 449,cls loss=0.069744,rec_loss=0.021056,loss=0.090800,mAp=91.9007\n",
      "train epoch 2 step 499,cls loss=0.069954,rec_loss=0.020941,loss=0.090895,mAp=91.9111\n",
      "train epoch 2 step 549,cls loss=0.069574,rec_loss=0.020955,loss=0.090529,mAp=91.9346\n",
      "train epoch 2 step 599,cls loss=0.069909,rec_loss=0.020948,loss=0.090857,mAp=91.6690\n",
      "train epoch 2 step 649,cls loss=0.068976,rec_loss=0.020891,loss=0.089867,mAp=91.8647\n",
      "train epoch 2 step 699,cls loss=0.068736,rec_loss=0.020813,loss=0.089549,mAp=91.9743\n",
      "train epoch 2 step 749,cls loss=0.069221,rec_loss=0.020765,loss=0.089986,mAp=92.0392\n",
      "train epoch 2 step 799,cls loss=0.069148,rec_loss=0.020659,loss=0.089808,mAp=92.1131\n",
      "train epoch 2 step 849,cls loss=0.068717,rec_loss=0.020644,loss=0.089361,mAp=92.2545\n",
      "train epoch 2 step 899,cls loss=0.068538,rec_loss=0.020649,loss=0.089187,mAp=92.3707\n",
      "train epoch 2 step 949,cls loss=0.068448,rec_loss=0.020669,loss=0.089118,mAp=92.4589\n",
      "train epoch 2 step 999,cls loss=0.068619,rec_loss=0.020666,loss=0.089285,mAp=92.4426\n",
      "train epoch 2 step 1049,cls loss=0.068808,rec_loss=0.020738,loss=0.089547,mAp=92.2914\n",
      "train epoch 2 step 1099,cls loss=0.069731,rec_loss=0.020793,loss=0.090524,mAp=92.1192\n",
      "train epoch 2 step 1149,cls loss=0.069513,rec_loss=0.020740,loss=0.090253,mAp=92.1594\n",
      "train epoch 2 step 1199,cls loss=0.069768,rec_loss=0.020767,loss=0.090535,mAp=92.1407\n",
      "train epoch 2 step 1249,cls loss=0.069610,rec_loss=0.020760,loss=0.090370,mAp=92.1040\n",
      "train epoch 2 step 1299,cls loss=0.069655,rec_loss=0.020777,loss=0.090433,mAp=92.1107\n",
      "train epoch 2 step 1349,cls loss=0.070154,rec_loss=0.020840,loss=0.090993,mAp=91.9267\n",
      "train epoch 2 step 1399,cls loss=0.070301,rec_loss=0.020841,loss=0.091142,mAp=91.9429\n",
      "train epoch 2 step 1449,cls loss=0.070737,rec_loss=0.020863,loss=0.091600,mAp=91.8310\n",
      "train epoch 2 step 1499,cls loss=0.071266,rec_loss=0.020936,loss=0.092203,mAp=91.7437\n",
      "train epoch 2 step 1549,cls loss=0.071473,rec_loss=0.020955,loss=0.092428,mAp=91.6742\n",
      "train epoch 2 step 1599,cls loss=0.072099,rec_loss=0.020960,loss=0.093059,mAp=91.6175\n",
      "train epoch 2 step 1649,cls loss=0.072490,rec_loss=0.020950,loss=0.093440,mAp=91.5482\n",
      "train epoch 2 step 1699,cls loss=0.072258,rec_loss=0.020957,loss=0.093215,mAp=91.5091\n",
      "train epoch 2 step 1749,cls loss=0.072505,rec_loss=0.020935,loss=0.093441,mAp=91.4580\n",
      "train epoch 2 step 1799,cls loss=0.072347,rec_loss=0.020906,loss=0.093252,mAp=91.4865\n",
      "train epoch 2 step 1849,cls loss=0.071756,rec_loss=0.020856,loss=0.092612,mAp=91.6077\n",
      "train epoch 2 step 1899,cls loss=0.071619,rec_loss=0.020844,loss=0.092463,mAp=91.6207\n",
      "train epoch 2 step 1949,cls loss=0.071783,rec_loss=0.020826,loss=0.092608,mAp=91.5692\n",
      "train epoch 2 step 1999,cls loss=0.071974,rec_loss=0.020817,loss=0.092791,mAp=91.5486\n",
      "train epoch 2 step 2049,cls loss=0.071869,rec_loss=0.020814,loss=0.092683,mAp=91.5771\n",
      "train epoch 2 step 2099,cls loss=0.071733,rec_loss=0.020824,loss=0.092557,mAp=91.5797\n",
      "train epoch 2 step 2149,cls loss=0.071823,rec_loss=0.020811,loss=0.092633,mAp=91.5688\n",
      "train epoch 2 step 2199,cls loss=0.071873,rec_loss=0.020812,loss=0.092685,mAp=91.5246\n",
      "train epoch 2 step 2249,cls loss=0.071910,rec_loss=0.020792,loss=0.092702,mAp=91.5072\n",
      "train epoch 2 step 2299,cls loss=0.072059,rec_loss=0.020805,loss=0.092864,mAp=91.4599\n",
      "train epoch 2 step 2349,cls loss=0.072061,rec_loss=0.020785,loss=0.092846,mAp=91.4855\n",
      "train epoch 2 step 2399,cls loss=0.072073,rec_loss=0.020765,loss=0.092838,mAp=91.4911\n",
      "train epoch 2 step 2449,cls loss=0.072004,rec_loss=0.020748,loss=0.092752,mAp=91.5264\n",
      "train epoch 2 step 2499,cls loss=0.072068,rec_loss=0.020766,loss=0.092835,mAp=91.5174\n",
      "train epoch 2 step 2549,cls loss=0.072199,rec_loss=0.020766,loss=0.092965,mAp=91.4961\n",
      "train epoch 2 step 2599,cls loss=0.072407,rec_loss=0.020751,loss=0.093158,mAp=91.4632\n",
      "val epoch 2 step 19,cls loss=0.149921,rec_loss=0.021303,loss=0.171224,mAp=79.7337\n",
      "val epoch 2 step 39,cls loss=0.139660,rec_loss=0.021212,loss=0.160872,mAp=80.6263\n",
      "val epoch 2 step 59,cls loss=0.139548,rec_loss=0.020892,loss=0.160440,mAp=80.6426\n",
      "val epoch 2 step 79,cls loss=0.136143,rec_loss=0.020976,loss=0.157119,mAp=81.7233\n",
      "val epoch 2 step 99,cls loss=0.127674,rec_loss=0.020556,loss=0.148231,mAp=82.8042\n",
      "val epoch 2 step 119,cls loss=0.120316,rec_loss=0.020125,loss=0.140441,mAp=83.7281\n",
      "val epoch 2 step 139,cls loss=0.115445,rec_loss=0.019922,loss=0.135367,mAp=83.8483\n",
      "val epoch 2 step 159,cls loss=0.116742,rec_loss=0.019632,loss=0.136374,mAp=83.1794\n",
      "val epoch 2 step 179,cls loss=0.114133,rec_loss=0.019509,loss=0.133642,mAp=83.3198\n",
      "val epoch 2 step 199,cls loss=0.113008,rec_loss=0.019455,loss=0.132464,mAp=83.4174\n",
      "val epoch 2 step 219,cls loss=0.113353,rec_loss=0.019493,loss=0.132846,mAp=83.6111\n",
      "val epoch 2 step 239,cls loss=0.111918,rec_loss=0.019433,loss=0.131351,mAp=83.6400\n",
      "val epoch 2 step 259,cls loss=0.112365,rec_loss=0.019465,loss=0.131830,mAp=83.6900\n",
      "val epoch 2 step 279,cls loss=0.113352,rec_loss=0.019517,loss=0.132869,mAp=83.9522\n",
      "val epoch 2 step 299,cls loss=0.113107,rec_loss=0.019427,loss=0.132534,mAp=84.1273\n",
      "val epoch 2 step 319,cls loss=0.115219,rec_loss=0.019561,loss=0.134780,mAp=83.9111\n",
      "val epoch 2 step 339,cls loss=0.116230,rec_loss=0.019518,loss=0.135748,mAp=83.9243\n",
      "val epoch 2 step 359,cls loss=0.119472,rec_loss=0.019450,loss=0.138922,mAp=83.6034\n",
      "epoch 2 done!\n",
      "train epoch 3 step 49,cls loss=0.060654,rec_loss=0.019893,loss=0.080547,mAp=92.9998\n",
      "train epoch 3 step 99,cls loss=0.056707,rec_loss=0.020018,loss=0.076725,mAp=93.8142\n",
      "train epoch 3 step 149,cls loss=0.060861,rec_loss=0.019690,loss=0.080552,mAp=93.5077\n",
      "train epoch 3 step 199,cls loss=0.060312,rec_loss=0.019795,loss=0.080107,mAp=93.4696\n",
      "train epoch 3 step 249,cls loss=0.061543,rec_loss=0.019578,loss=0.081121,mAp=93.3885\n",
      "train epoch 3 step 299,cls loss=0.058396,rec_loss=0.019619,loss=0.078015,mAp=93.9052\n",
      "train epoch 3 step 349,cls loss=0.057418,rec_loss=0.019680,loss=0.077098,mAp=93.9611\n",
      "train epoch 3 step 399,cls loss=0.055676,rec_loss=0.019659,loss=0.075335,mAp=94.3791\n",
      "train epoch 3 step 449,cls loss=0.054837,rec_loss=0.019716,loss=0.074554,mAp=94.5265\n",
      "train epoch 3 step 499,cls loss=0.056979,rec_loss=0.019861,loss=0.076840,mAp=94.1952\n",
      "train epoch 3 step 549,cls loss=0.055830,rec_loss=0.019840,loss=0.075670,mAp=94.3697\n",
      "train epoch 3 step 599,cls loss=0.054680,rec_loss=0.019826,loss=0.074506,mAp=94.5046\n",
      "train epoch 3 step 649,cls loss=0.054526,rec_loss=0.019827,loss=0.074352,mAp=94.5950\n",
      "train epoch 3 step 699,cls loss=0.055961,rec_loss=0.019779,loss=0.075740,mAp=94.3103\n",
      "train epoch 3 step 749,cls loss=0.056264,rec_loss=0.019834,loss=0.076097,mAp=94.3148\n",
      "train epoch 3 step 799,cls loss=0.056186,rec_loss=0.019823,loss=0.076009,mAp=94.3419\n",
      "train epoch 3 step 849,cls loss=0.057054,rec_loss=0.019865,loss=0.076919,mAp=94.2371\n",
      "train epoch 3 step 899,cls loss=0.057100,rec_loss=0.019913,loss=0.077013,mAp=94.2736\n",
      "train epoch 3 step 949,cls loss=0.057557,rec_loss=0.019897,loss=0.077454,mAp=94.2250\n",
      "train epoch 3 step 999,cls loss=0.057681,rec_loss=0.019934,loss=0.077615,mAp=94.2533\n",
      "train epoch 3 step 1049,cls loss=0.057738,rec_loss=0.019990,loss=0.077728,mAp=94.2134\n",
      "train epoch 3 step 1099,cls loss=0.057515,rec_loss=0.019965,loss=0.077480,mAp=94.2562\n",
      "train epoch 3 step 1149,cls loss=0.057384,rec_loss=0.019928,loss=0.077313,mAp=94.3296\n",
      "train epoch 3 step 1199,cls loss=0.057555,rec_loss=0.019883,loss=0.077438,mAp=94.2875\n",
      "train epoch 3 step 1249,cls loss=0.057703,rec_loss=0.019893,loss=0.077596,mAp=94.2510\n",
      "train epoch 3 step 1299,cls loss=0.058046,rec_loss=0.019858,loss=0.077904,mAp=94.1974\n",
      "train epoch 3 step 1349,cls loss=0.057914,rec_loss=0.019842,loss=0.077756,mAp=94.1868\n",
      "train epoch 3 step 1399,cls loss=0.058021,rec_loss=0.019826,loss=0.077847,mAp=94.1991\n",
      "train epoch 3 step 1449,cls loss=0.058248,rec_loss=0.019792,loss=0.078040,mAp=94.2088\n",
      "train epoch 3 step 1499,cls loss=0.058150,rec_loss=0.019787,loss=0.077937,mAp=94.2104\n",
      "train epoch 3 step 1549,cls loss=0.058271,rec_loss=0.019809,loss=0.078080,mAp=94.1367\n",
      "train epoch 3 step 1599,cls loss=0.058358,rec_loss=0.019834,loss=0.078192,mAp=94.1621\n",
      "train epoch 3 step 1649,cls loss=0.058543,rec_loss=0.019865,loss=0.078408,mAp=94.1450\n",
      "train epoch 3 step 1699,cls loss=0.058775,rec_loss=0.019893,loss=0.078668,mAp=94.0594\n",
      "train epoch 3 step 1749,cls loss=0.058754,rec_loss=0.019900,loss=0.078655,mAp=94.0332\n",
      "train epoch 3 step 1799,cls loss=0.059157,rec_loss=0.019871,loss=0.079028,mAp=93.9729\n",
      "train epoch 3 step 1849,cls loss=0.059009,rec_loss=0.019841,loss=0.078850,mAp=93.9835\n",
      "train epoch 3 step 1899,cls loss=0.059137,rec_loss=0.019847,loss=0.078984,mAp=93.9805\n",
      "train epoch 3 step 1949,cls loss=0.058860,rec_loss=0.019828,loss=0.078688,mAp=93.9898\n",
      "train epoch 3 step 1999,cls loss=0.059315,rec_loss=0.019820,loss=0.079135,mAp=93.9365\n",
      "train epoch 3 step 2049,cls loss=0.059538,rec_loss=0.019869,loss=0.079406,mAp=93.9081\n",
      "train epoch 3 step 2099,cls loss=0.059550,rec_loss=0.019883,loss=0.079433,mAp=93.9057\n",
      "train epoch 3 step 2149,cls loss=0.060016,rec_loss=0.019915,loss=0.079931,mAp=93.7999\n",
      "train epoch 3 step 2199,cls loss=0.060152,rec_loss=0.019913,loss=0.080066,mAp=93.7865\n",
      "train epoch 3 step 2249,cls loss=0.060093,rec_loss=0.019887,loss=0.079979,mAp=93.7891\n",
      "train epoch 3 step 2299,cls loss=0.059903,rec_loss=0.019875,loss=0.079778,mAp=93.8147\n",
      "train epoch 3 step 2349,cls loss=0.060045,rec_loss=0.019859,loss=0.079905,mAp=93.7853\n",
      "train epoch 3 step 2399,cls loss=0.060124,rec_loss=0.019878,loss=0.080002,mAp=93.7795\n",
      "train epoch 3 step 2449,cls loss=0.059976,rec_loss=0.019886,loss=0.079862,mAp=93.7734\n",
      "train epoch 3 step 2499,cls loss=0.060000,rec_loss=0.019894,loss=0.079894,mAp=93.7558\n",
      "train epoch 3 step 2549,cls loss=0.059985,rec_loss=0.019886,loss=0.079872,mAp=93.7618\n",
      "train epoch 3 step 2599,cls loss=0.059902,rec_loss=0.019870,loss=0.079772,mAp=93.7559\n",
      "val epoch 3 step 19,cls loss=0.153471,rec_loss=0.022520,loss=0.175991,mAp=81.7040\n",
      "val epoch 3 step 39,cls loss=0.143898,rec_loss=0.022159,loss=0.166057,mAp=81.1229\n",
      "val epoch 3 step 59,cls loss=0.137431,rec_loss=0.021793,loss=0.159225,mAp=81.9031\n",
      "val epoch 3 step 79,cls loss=0.141532,rec_loss=0.021907,loss=0.163439,mAp=81.2492\n",
      "val epoch 3 step 99,cls loss=0.130141,rec_loss=0.021596,loss=0.151737,mAp=82.4695\n",
      "val epoch 3 step 119,cls loss=0.124379,rec_loss=0.021137,loss=0.145516,mAp=83.0417\n",
      "val epoch 3 step 139,cls loss=0.119125,rec_loss=0.020840,loss=0.139965,mAp=83.3116\n",
      "val epoch 3 step 159,cls loss=0.118870,rec_loss=0.020633,loss=0.139503,mAp=82.4175\n",
      "val epoch 3 step 179,cls loss=0.115913,rec_loss=0.020474,loss=0.136387,mAp=82.7104\n",
      "val epoch 3 step 199,cls loss=0.115633,rec_loss=0.020425,loss=0.136058,mAp=82.7317\n",
      "val epoch 3 step 219,cls loss=0.114865,rec_loss=0.020451,loss=0.135315,mAp=83.0968\n",
      "val epoch 3 step 239,cls loss=0.114071,rec_loss=0.020382,loss=0.134453,mAp=83.0047\n",
      "val epoch 3 step 259,cls loss=0.114790,rec_loss=0.020377,loss=0.135166,mAp=83.1338\n",
      "val epoch 3 step 279,cls loss=0.117035,rec_loss=0.020411,loss=0.137446,mAp=83.3593\n",
      "val epoch 3 step 299,cls loss=0.117588,rec_loss=0.020350,loss=0.137938,mAp=83.3567\n",
      "val epoch 3 step 319,cls loss=0.119876,rec_loss=0.020486,loss=0.140362,mAp=83.2189\n",
      "val epoch 3 step 339,cls loss=0.121062,rec_loss=0.020479,loss=0.141541,mAp=83.0669\n",
      "val epoch 3 step 359,cls loss=0.123811,rec_loss=0.020425,loss=0.144236,mAp=82.6960\n",
      "epoch 3 done!\n",
      "train epoch 4 step 49,cls loss=0.047355,rec_loss=0.018714,loss=0.066069,mAp=92.8756\n",
      "train epoch 4 step 99,cls loss=0.053724,rec_loss=0.019300,loss=0.073023,mAp=93.1221\n",
      "train epoch 4 step 149,cls loss=0.053215,rec_loss=0.019559,loss=0.072774,mAp=93.8080\n",
      "train epoch 4 step 199,cls loss=0.051902,rec_loss=0.019478,loss=0.071380,mAp=94.0606\n",
      "train epoch 4 step 249,cls loss=0.051026,rec_loss=0.019563,loss=0.070588,mAp=94.3253\n",
      "train epoch 4 step 299,cls loss=0.049960,rec_loss=0.019542,loss=0.069502,mAp=94.7222\n",
      "train epoch 4 step 349,cls loss=0.049533,rec_loss=0.019352,loss=0.068886,mAp=94.9053\n",
      "train epoch 4 step 399,cls loss=0.048034,rec_loss=0.019275,loss=0.067309,mAp=95.1619\n",
      "train epoch 4 step 449,cls loss=0.047808,rec_loss=0.019340,loss=0.067148,mAp=95.3413\n",
      "train epoch 4 step 499,cls loss=0.047866,rec_loss=0.019404,loss=0.067270,mAp=95.3615\n",
      "train epoch 4 step 549,cls loss=0.049208,rec_loss=0.019317,loss=0.068525,mAp=95.3229\n",
      "train epoch 4 step 599,cls loss=0.049469,rec_loss=0.019295,loss=0.068764,mAp=95.3379\n",
      "train epoch 4 step 649,cls loss=0.049907,rec_loss=0.019228,loss=0.069135,mAp=95.3131\n",
      "train epoch 4 step 699,cls loss=0.050218,rec_loss=0.019240,loss=0.069458,mAp=95.2092\n",
      "train epoch 4 step 749,cls loss=0.049764,rec_loss=0.019137,loss=0.068901,mAp=95.2291\n",
      "train epoch 4 step 799,cls loss=0.049271,rec_loss=0.019175,loss=0.068446,mAp=95.2351\n",
      "train epoch 4 step 849,cls loss=0.049242,rec_loss=0.019179,loss=0.068420,mAp=95.1539\n",
      "train epoch 4 step 899,cls loss=0.049715,rec_loss=0.019212,loss=0.068927,mAp=95.2123\n",
      "train epoch 4 step 949,cls loss=0.049600,rec_loss=0.019184,loss=0.068784,mAp=95.2397\n",
      "train epoch 4 step 999,cls loss=0.049936,rec_loss=0.019212,loss=0.069148,mAp=95.1543\n",
      "train epoch 4 step 1049,cls loss=0.049603,rec_loss=0.019109,loss=0.068712,mAp=95.2267\n",
      "train epoch 4 step 1099,cls loss=0.050001,rec_loss=0.019144,loss=0.069145,mAp=95.1612\n",
      "train epoch 4 step 1149,cls loss=0.049566,rec_loss=0.019158,loss=0.068724,mAp=95.2391\n",
      "train epoch 4 step 1199,cls loss=0.049658,rec_loss=0.019174,loss=0.068831,mAp=95.2169\n",
      "train epoch 4 step 1249,cls loss=0.049790,rec_loss=0.019212,loss=0.069001,mAp=95.1907\n",
      "train epoch 4 step 1299,cls loss=0.049707,rec_loss=0.019255,loss=0.068963,mAp=95.2348\n",
      "train epoch 4 step 1349,cls loss=0.049702,rec_loss=0.019226,loss=0.068928,mAp=95.2241\n",
      "train epoch 4 step 1399,cls loss=0.050034,rec_loss=0.019227,loss=0.069261,mAp=95.1536\n",
      "train epoch 4 step 1449,cls loss=0.050254,rec_loss=0.019231,loss=0.069485,mAp=95.0644\n",
      "train epoch 4 step 1499,cls loss=0.050246,rec_loss=0.019190,loss=0.069436,mAp=95.0283\n",
      "train epoch 4 step 1549,cls loss=0.050240,rec_loss=0.019176,loss=0.069417,mAp=95.0098\n",
      "train epoch 4 step 1599,cls loss=0.050087,rec_loss=0.019149,loss=0.069236,mAp=95.0118\n",
      "train epoch 4 step 1649,cls loss=0.049966,rec_loss=0.019162,loss=0.069128,mAp=95.0065\n",
      "train epoch 4 step 1699,cls loss=0.049585,rec_loss=0.019157,loss=0.068741,mAp=95.0837\n",
      "train epoch 4 step 1749,cls loss=0.049828,rec_loss=0.019168,loss=0.068996,mAp=95.1005\n",
      "train epoch 4 step 1799,cls loss=0.049859,rec_loss=0.019178,loss=0.069037,mAp=95.1136\n",
      "train epoch 4 step 1849,cls loss=0.049778,rec_loss=0.019178,loss=0.068956,mAp=95.1363\n",
      "train epoch 4 step 1899,cls loss=0.049809,rec_loss=0.019173,loss=0.068982,mAp=95.1033\n",
      "train epoch 4 step 1949,cls loss=0.049816,rec_loss=0.019184,loss=0.069000,mAp=95.1086\n",
      "train epoch 4 step 1999,cls loss=0.049678,rec_loss=0.019197,loss=0.068875,mAp=95.1377\n",
      "train epoch 4 step 2049,cls loss=0.049586,rec_loss=0.019207,loss=0.068793,mAp=95.1774\n",
      "train epoch 4 step 2099,cls loss=0.049640,rec_loss=0.019204,loss=0.068844,mAp=95.1764\n",
      "train epoch 4 step 2149,cls loss=0.049572,rec_loss=0.019185,loss=0.068757,mAp=95.1895\n",
      "train epoch 4 step 2199,cls loss=0.049476,rec_loss=0.019177,loss=0.068653,mAp=95.2114\n",
      "train epoch 4 step 2249,cls loss=0.049528,rec_loss=0.019177,loss=0.068705,mAp=95.2339\n",
      "train epoch 4 step 2299,cls loss=0.049564,rec_loss=0.019178,loss=0.068742,mAp=95.2279\n",
      "train epoch 4 step 2349,cls loss=0.049674,rec_loss=0.019193,loss=0.068867,mAp=95.2352\n",
      "train epoch 4 step 2399,cls loss=0.049710,rec_loss=0.019186,loss=0.068895,mAp=95.2369\n",
      "train epoch 4 step 2449,cls loss=0.049662,rec_loss=0.019191,loss=0.068853,mAp=95.2600\n",
      "train epoch 4 step 2499,cls loss=0.049970,rec_loss=0.019214,loss=0.069184,mAp=95.2000\n",
      "train epoch 4 step 2549,cls loss=0.050214,rec_loss=0.019234,loss=0.069448,mAp=95.1511\n",
      "train epoch 4 step 2599,cls loss=0.050070,rec_loss=0.019227,loss=0.069298,mAp=95.1520\n",
      "val epoch 4 step 19,cls loss=0.173543,rec_loss=0.020337,loss=0.193880,mAp=81.7216\n",
      "val epoch 4 step 39,cls loss=0.156123,rec_loss=0.020014,loss=0.176137,mAp=82.6369\n",
      "val epoch 4 step 59,cls loss=0.143649,rec_loss=0.019809,loss=0.163458,mAp=83.4195\n",
      "val epoch 4 step 79,cls loss=0.147258,rec_loss=0.020024,loss=0.167281,mAp=83.2388\n",
      "val epoch 4 step 99,cls loss=0.137038,rec_loss=0.019630,loss=0.156668,mAp=83.9603\n",
      "val epoch 4 step 119,cls loss=0.126936,rec_loss=0.019286,loss=0.146222,mAp=84.9883\n",
      "val epoch 4 step 139,cls loss=0.122743,rec_loss=0.018964,loss=0.141706,mAp=84.6451\n",
      "val epoch 4 step 159,cls loss=0.122321,rec_loss=0.018686,loss=0.141007,mAp=84.2856\n",
      "val epoch 4 step 179,cls loss=0.120957,rec_loss=0.018486,loss=0.139443,mAp=84.2263\n",
      "val epoch 4 step 199,cls loss=0.119204,rec_loss=0.018425,loss=0.137629,mAp=84.6196\n",
      "val epoch 4 step 219,cls loss=0.120058,rec_loss=0.018481,loss=0.138540,mAp=84.6292\n",
      "val epoch 4 step 239,cls loss=0.117670,rec_loss=0.018382,loss=0.136053,mAp=85.1795\n",
      "val epoch 4 step 259,cls loss=0.120450,rec_loss=0.018407,loss=0.138857,mAp=84.8063\n",
      "val epoch 4 step 279,cls loss=0.122690,rec_loss=0.018476,loss=0.141167,mAp=84.8664\n",
      "val epoch 4 step 299,cls loss=0.122789,rec_loss=0.018387,loss=0.141176,mAp=85.0153\n",
      "val epoch 4 step 319,cls loss=0.125605,rec_loss=0.018480,loss=0.144085,mAp=84.8711\n",
      "val epoch 4 step 339,cls loss=0.127430,rec_loss=0.018487,loss=0.145918,mAp=84.6876\n",
      "val epoch 4 step 359,cls loss=0.130615,rec_loss=0.018446,loss=0.149061,mAp=84.3481\n",
      "epoch 4 done!\n",
      "train epoch 5 step 49,cls loss=0.030373,rec_loss=0.017268,loss=0.047642,mAp=98.5659\n",
      "train epoch 5 step 99,cls loss=0.035170,rec_loss=0.017548,loss=0.052718,mAp=97.8975\n",
      "train epoch 5 step 149,cls loss=0.033988,rec_loss=0.017434,loss=0.051421,mAp=97.8067\n",
      "train epoch 5 step 199,cls loss=0.033552,rec_loss=0.017603,loss=0.051155,mAp=97.9352\n",
      "train epoch 5 step 249,cls loss=0.034130,rec_loss=0.017886,loss=0.052016,mAp=97.8596\n",
      "train epoch 5 step 299,cls loss=0.032237,rec_loss=0.018003,loss=0.050240,mAp=97.9116\n",
      "train epoch 5 step 349,cls loss=0.033679,rec_loss=0.018041,loss=0.051719,mAp=97.6907\n",
      "train epoch 5 step 399,cls loss=0.033783,rec_loss=0.017954,loss=0.051737,mAp=97.7498\n",
      "train epoch 5 step 449,cls loss=0.033964,rec_loss=0.018028,loss=0.051992,mAp=97.6415\n",
      "train epoch 5 step 499,cls loss=0.034485,rec_loss=0.018097,loss=0.052581,mAp=97.5429\n",
      "train epoch 5 step 549,cls loss=0.034106,rec_loss=0.018115,loss=0.052221,mAp=97.6137\n",
      "train epoch 5 step 599,cls loss=0.035126,rec_loss=0.018098,loss=0.053224,mAp=97.4794\n",
      "train epoch 5 step 649,cls loss=0.035494,rec_loss=0.018201,loss=0.053694,mAp=97.3818\n",
      "train epoch 5 step 699,cls loss=0.036384,rec_loss=0.018314,loss=0.054698,mAp=97.1782\n",
      "train epoch 5 step 749,cls loss=0.036627,rec_loss=0.018439,loss=0.055066,mAp=97.1090\n",
      "train epoch 5 step 799,cls loss=0.036622,rec_loss=0.018450,loss=0.055072,mAp=97.1630\n",
      "train epoch 5 step 849,cls loss=0.036522,rec_loss=0.018484,loss=0.055006,mAp=97.1215\n",
      "train epoch 5 step 899,cls loss=0.036398,rec_loss=0.018480,loss=0.054878,mAp=97.1679\n",
      "train epoch 5 step 949,cls loss=0.036450,rec_loss=0.018481,loss=0.054931,mAp=97.2023\n",
      "train epoch 5 step 999,cls loss=0.036510,rec_loss=0.018527,loss=0.055037,mAp=97.1793\n",
      "train epoch 5 step 1049,cls loss=0.036584,rec_loss=0.018552,loss=0.055136,mAp=97.1616\n",
      "train epoch 5 step 1099,cls loss=0.037069,rec_loss=0.018522,loss=0.055591,mAp=97.1047\n",
      "train epoch 5 step 1149,cls loss=0.037009,rec_loss=0.018481,loss=0.055490,mAp=97.0970\n",
      "train epoch 5 step 1199,cls loss=0.037175,rec_loss=0.018518,loss=0.055694,mAp=97.0989\n",
      "train epoch 5 step 1249,cls loss=0.037681,rec_loss=0.018551,loss=0.056232,mAp=97.0440\n",
      "train epoch 5 step 1299,cls loss=0.038182,rec_loss=0.018591,loss=0.056772,mAp=96.9684\n",
      "train epoch 5 step 1349,cls loss=0.038363,rec_loss=0.018592,loss=0.056955,mAp=96.9338\n",
      "train epoch 5 step 1399,cls loss=0.038672,rec_loss=0.018563,loss=0.057236,mAp=96.8967\n",
      "train epoch 5 step 1449,cls loss=0.039021,rec_loss=0.018541,loss=0.057562,mAp=96.8693\n",
      "train epoch 5 step 1499,cls loss=0.039185,rec_loss=0.018521,loss=0.057706,mAp=96.8622\n",
      "train epoch 5 step 1549,cls loss=0.039198,rec_loss=0.018482,loss=0.057680,mAp=96.8559\n",
      "train epoch 5 step 1599,cls loss=0.039253,rec_loss=0.018452,loss=0.057706,mAp=96.8309\n",
      "train epoch 5 step 1649,cls loss=0.039057,rec_loss=0.018427,loss=0.057485,mAp=96.8484\n",
      "train epoch 5 step 1699,cls loss=0.039044,rec_loss=0.018444,loss=0.057488,mAp=96.8553\n",
      "train epoch 5 step 1749,cls loss=0.039260,rec_loss=0.018429,loss=0.057688,mAp=96.8047\n",
      "train epoch 5 step 1799,cls loss=0.039481,rec_loss=0.018456,loss=0.057936,mAp=96.7706\n",
      "train epoch 5 step 1849,cls loss=0.039898,rec_loss=0.018482,loss=0.058380,mAp=96.7274\n",
      "train epoch 5 step 1899,cls loss=0.040124,rec_loss=0.018485,loss=0.058610,mAp=96.6954\n",
      "train epoch 5 step 1949,cls loss=0.039987,rec_loss=0.018508,loss=0.058496,mAp=96.7231\n",
      "train epoch 5 step 1999,cls loss=0.039975,rec_loss=0.018527,loss=0.058502,mAp=96.6956\n",
      "train epoch 5 step 2049,cls loss=0.039855,rec_loss=0.018503,loss=0.058358,mAp=96.6962\n",
      "train epoch 5 step 2099,cls loss=0.039979,rec_loss=0.018504,loss=0.058483,mAp=96.6403\n",
      "train epoch 5 step 2149,cls loss=0.040062,rec_loss=0.018507,loss=0.058569,mAp=96.6314\n",
      "train epoch 5 step 2199,cls loss=0.040054,rec_loss=0.018504,loss=0.058558,mAp=96.6306\n",
      "train epoch 5 step 2249,cls loss=0.039933,rec_loss=0.018517,loss=0.058449,mAp=96.6593\n",
      "train epoch 5 step 2299,cls loss=0.039860,rec_loss=0.018511,loss=0.058371,mAp=96.6918\n",
      "train epoch 5 step 2349,cls loss=0.040092,rec_loss=0.018524,loss=0.058617,mAp=96.6834\n",
      "train epoch 5 step 2399,cls loss=0.040134,rec_loss=0.018511,loss=0.058644,mAp=96.6732\n",
      "train epoch 5 step 2449,cls loss=0.040305,rec_loss=0.018493,loss=0.058798,mAp=96.6588\n",
      "train epoch 5 step 2499,cls loss=0.040294,rec_loss=0.018480,loss=0.058774,mAp=96.6630\n",
      "train epoch 5 step 2549,cls loss=0.040627,rec_loss=0.018506,loss=0.059133,mAp=96.5980\n",
      "train epoch 5 step 2599,cls loss=0.040507,rec_loss=0.018503,loss=0.059010,mAp=96.6280\n",
      "val epoch 5 step 19,cls loss=0.142259,rec_loss=0.020795,loss=0.163054,mAp=85.5856\n",
      "val epoch 5 step 39,cls loss=0.140529,rec_loss=0.020815,loss=0.161344,mAp=82.2658\n",
      "val epoch 5 step 59,cls loss=0.137855,rec_loss=0.020502,loss=0.158356,mAp=82.8680\n",
      "val epoch 5 step 79,cls loss=0.141445,rec_loss=0.020696,loss=0.162141,mAp=83.4107\n",
      "val epoch 5 step 99,cls loss=0.130189,rec_loss=0.020257,loss=0.150446,mAp=85.1139\n",
      "val epoch 5 step 119,cls loss=0.123906,rec_loss=0.019788,loss=0.143694,mAp=85.4759\n",
      "val epoch 5 step 139,cls loss=0.120933,rec_loss=0.019451,loss=0.140384,mAp=85.2873\n",
      "val epoch 5 step 159,cls loss=0.122627,rec_loss=0.019183,loss=0.141810,mAp=84.9115\n",
      "val epoch 5 step 179,cls loss=0.121906,rec_loss=0.019056,loss=0.140962,mAp=84.8147\n",
      "val epoch 5 step 199,cls loss=0.121247,rec_loss=0.018948,loss=0.140195,mAp=84.7979\n",
      "val epoch 5 step 219,cls loss=0.124151,rec_loss=0.019086,loss=0.143237,mAp=84.6862\n",
      "val epoch 5 step 239,cls loss=0.121695,rec_loss=0.019013,loss=0.140708,mAp=85.0538\n",
      "val epoch 5 step 259,cls loss=0.121165,rec_loss=0.019016,loss=0.140181,mAp=85.2567\n",
      "val epoch 5 step 279,cls loss=0.122893,rec_loss=0.019051,loss=0.141944,mAp=85.3414\n",
      "val epoch 5 step 299,cls loss=0.124400,rec_loss=0.018964,loss=0.143364,mAp=85.1613\n",
      "val epoch 5 step 319,cls loss=0.127100,rec_loss=0.019119,loss=0.146219,mAp=84.8483\n",
      "val epoch 5 step 339,cls loss=0.128640,rec_loss=0.019069,loss=0.147710,mAp=84.6752\n",
      "val epoch 5 step 359,cls loss=0.131361,rec_loss=0.019013,loss=0.150373,mAp=84.5129\n",
      "epoch 5 done!\n",
      "train epoch 6 step 49,cls loss=0.030393,rec_loss=0.018659,loss=0.049052,mAp=97.1833\n",
      "train epoch 6 step 99,cls loss=0.028229,rec_loss=0.017995,loss=0.046224,mAp=97.7875\n",
      "train epoch 6 step 149,cls loss=0.029072,rec_loss=0.017676,loss=0.046749,mAp=97.6639\n",
      "train epoch 6 step 199,cls loss=0.026690,rec_loss=0.017764,loss=0.044454,mAp=97.8875\n",
      "train epoch 6 step 249,cls loss=0.028454,rec_loss=0.017660,loss=0.046115,mAp=97.7672\n",
      "train epoch 6 step 299,cls loss=0.030327,rec_loss=0.017623,loss=0.047950,mAp=97.4769\n",
      "train epoch 6 step 349,cls loss=0.030023,rec_loss=0.017493,loss=0.047516,mAp=97.6552\n",
      "train epoch 6 step 399,cls loss=0.031115,rec_loss=0.017599,loss=0.048714,mAp=97.6337\n",
      "train epoch 6 step 449,cls loss=0.030380,rec_loss=0.017660,loss=0.048040,mAp=97.7417\n",
      "train epoch 6 step 499,cls loss=0.030650,rec_loss=0.017688,loss=0.048337,mAp=97.7185\n",
      "train epoch 6 step 549,cls loss=0.031516,rec_loss=0.017789,loss=0.049305,mAp=97.6244\n",
      "train epoch 6 step 599,cls loss=0.032035,rec_loss=0.017882,loss=0.049918,mAp=97.5030\n",
      "train epoch 6 step 649,cls loss=0.032056,rec_loss=0.018017,loss=0.050072,mAp=97.4805\n",
      "train epoch 6 step 699,cls loss=0.033176,rec_loss=0.018004,loss=0.051180,mAp=97.3787\n",
      "train epoch 6 step 749,cls loss=0.032868,rec_loss=0.017995,loss=0.050863,mAp=97.4574\n",
      "train epoch 6 step 799,cls loss=0.032832,rec_loss=0.017998,loss=0.050831,mAp=97.4166\n",
      "train epoch 6 step 849,cls loss=0.032982,rec_loss=0.018031,loss=0.051013,mAp=97.4188\n",
      "train epoch 6 step 899,cls loss=0.033153,rec_loss=0.017972,loss=0.051125,mAp=97.3354\n",
      "train epoch 6 step 949,cls loss=0.033676,rec_loss=0.017958,loss=0.051634,mAp=97.2579\n",
      "train epoch 6 step 999,cls loss=0.034230,rec_loss=0.017993,loss=0.052223,mAp=97.1992\n",
      "train epoch 6 step 1049,cls loss=0.034661,rec_loss=0.018027,loss=0.052688,mAp=97.0954\n",
      "train epoch 6 step 1099,cls loss=0.035003,rec_loss=0.018043,loss=0.053046,mAp=97.0175\n",
      "train epoch 6 step 1149,cls loss=0.034874,rec_loss=0.018021,loss=0.052895,mAp=97.0833\n",
      "train epoch 6 step 1199,cls loss=0.034678,rec_loss=0.018038,loss=0.052716,mAp=97.1280\n",
      "train epoch 6 step 1249,cls loss=0.034579,rec_loss=0.018003,loss=0.052582,mAp=97.1455\n",
      "train epoch 6 step 1299,cls loss=0.034346,rec_loss=0.017982,loss=0.052328,mAp=97.1822\n",
      "train epoch 6 step 1349,cls loss=0.034404,rec_loss=0.017996,loss=0.052400,mAp=97.1208\n",
      "train epoch 6 step 1399,cls loss=0.034256,rec_loss=0.017986,loss=0.052242,mAp=97.1285\n",
      "train epoch 6 step 1449,cls loss=0.034436,rec_loss=0.017981,loss=0.052417,mAp=97.1014\n",
      "train epoch 6 step 1499,cls loss=0.034066,rec_loss=0.018008,loss=0.052073,mAp=97.1369\n",
      "train epoch 6 step 1549,cls loss=0.033969,rec_loss=0.018019,loss=0.051988,mAp=97.1491\n",
      "train epoch 6 step 1599,cls loss=0.034185,rec_loss=0.018031,loss=0.052217,mAp=97.1049\n",
      "train epoch 6 step 1649,cls loss=0.034205,rec_loss=0.018038,loss=0.052243,mAp=97.1167\n",
      "train epoch 6 step 1699,cls loss=0.034409,rec_loss=0.018034,loss=0.052443,mAp=97.0854\n",
      "train epoch 6 step 1749,cls loss=0.034093,rec_loss=0.018033,loss=0.052126,mAp=97.1085\n",
      "train epoch 6 step 1799,cls loss=0.034183,rec_loss=0.018051,loss=0.052234,mAp=97.0867\n",
      "train epoch 6 step 1849,cls loss=0.034175,rec_loss=0.018036,loss=0.052211,mAp=97.0704\n",
      "train epoch 6 step 1899,cls loss=0.034099,rec_loss=0.018028,loss=0.052127,mAp=97.0921\n",
      "train epoch 6 step 1949,cls loss=0.033883,rec_loss=0.018040,loss=0.051923,mAp=97.0937\n",
      "train epoch 6 step 1999,cls loss=0.033845,rec_loss=0.018071,loss=0.051917,mAp=97.1310\n",
      "train epoch 6 step 2049,cls loss=0.033742,rec_loss=0.018046,loss=0.051788,mAp=97.1512\n",
      "train epoch 6 step 2099,cls loss=0.033682,rec_loss=0.018061,loss=0.051743,mAp=97.1920\n",
      "train epoch 6 step 2149,cls loss=0.033671,rec_loss=0.018072,loss=0.051743,mAp=97.1929\n",
      "train epoch 6 step 2199,cls loss=0.034064,rec_loss=0.018109,loss=0.052173,mAp=97.1065\n",
      "train epoch 6 step 2249,cls loss=0.034308,rec_loss=0.018119,loss=0.052427,mAp=97.0643\n",
      "train epoch 6 step 2299,cls loss=0.034324,rec_loss=0.018120,loss=0.052444,mAp=97.0564\n",
      "train epoch 6 step 2349,cls loss=0.034442,rec_loss=0.018125,loss=0.052566,mAp=97.0347\n",
      "train epoch 6 step 2399,cls loss=0.034420,rec_loss=0.018124,loss=0.052543,mAp=97.0271\n",
      "train epoch 6 step 2449,cls loss=0.034842,rec_loss=0.018144,loss=0.052987,mAp=96.9463\n",
      "train epoch 6 step 2499,cls loss=0.034714,rec_loss=0.018131,loss=0.052844,mAp=96.9647\n",
      "train epoch 6 step 2549,cls loss=0.034929,rec_loss=0.018138,loss=0.053067,mAp=96.9295\n",
      "train epoch 6 step 2599,cls loss=0.034909,rec_loss=0.018150,loss=0.053059,mAp=96.9355\n",
      "val epoch 6 step 19,cls loss=0.154886,rec_loss=0.019900,loss=0.174786,mAp=85.1148\n",
      "val epoch 6 step 39,cls loss=0.145519,rec_loss=0.020017,loss=0.165535,mAp=83.4748\n",
      "val epoch 6 step 59,cls loss=0.147672,rec_loss=0.019767,loss=0.167439,mAp=84.0699\n",
      "val epoch 6 step 79,cls loss=0.148401,rec_loss=0.019848,loss=0.168249,mAp=84.3595\n",
      "val epoch 6 step 99,cls loss=0.138521,rec_loss=0.019440,loss=0.157961,mAp=84.9460\n",
      "val epoch 6 step 119,cls loss=0.131780,rec_loss=0.018979,loss=0.150760,mAp=85.8531\n",
      "val epoch 6 step 139,cls loss=0.129275,rec_loss=0.018701,loss=0.147976,mAp=85.8009\n",
      "val epoch 6 step 159,cls loss=0.128024,rec_loss=0.018415,loss=0.146439,mAp=85.7620\n",
      "val epoch 6 step 179,cls loss=0.124350,rec_loss=0.018246,loss=0.142596,mAp=85.8834\n",
      "val epoch 6 step 199,cls loss=0.123043,rec_loss=0.018114,loss=0.141158,mAp=85.9084\n",
      "val epoch 6 step 219,cls loss=0.124492,rec_loss=0.018235,loss=0.142727,mAp=85.7548\n",
      "val epoch 6 step 239,cls loss=0.122773,rec_loss=0.018154,loss=0.140927,mAp=85.7398\n",
      "val epoch 6 step 259,cls loss=0.124603,rec_loss=0.018132,loss=0.142735,mAp=85.7611\n",
      "val epoch 6 step 279,cls loss=0.125869,rec_loss=0.018139,loss=0.144008,mAp=85.9165\n",
      "val epoch 6 step 299,cls loss=0.127540,rec_loss=0.018038,loss=0.145578,mAp=85.9135\n",
      "val epoch 6 step 319,cls loss=0.130964,rec_loss=0.018197,loss=0.149161,mAp=85.6795\n",
      "val epoch 6 step 339,cls loss=0.133165,rec_loss=0.018165,loss=0.151330,mAp=85.4810\n",
      "val epoch 6 step 359,cls loss=0.137120,rec_loss=0.018101,loss=0.155221,mAp=85.0447\n",
      "epoch 6 done!\n",
      "train epoch 7 step 49,cls loss=0.019980,rec_loss=0.017331,loss=0.037312,mAp=98.8889\n",
      "train epoch 7 step 99,cls loss=0.020258,rec_loss=0.017029,loss=0.037287,mAp=98.4132\n",
      "train epoch 7 step 149,cls loss=0.021724,rec_loss=0.017199,loss=0.038924,mAp=98.3449\n",
      "train epoch 7 step 199,cls loss=0.021432,rec_loss=0.017452,loss=0.038884,mAp=98.4224\n",
      "train epoch 7 step 249,cls loss=0.020860,rec_loss=0.017348,loss=0.038208,mAp=98.5266\n",
      "train epoch 7 step 299,cls loss=0.022523,rec_loss=0.017469,loss=0.039992,mAp=98.4625\n",
      "train epoch 7 step 349,cls loss=0.025332,rec_loss=0.017657,loss=0.042989,mAp=98.1241\n",
      "train epoch 7 step 399,cls loss=0.025740,rec_loss=0.017737,loss=0.043477,mAp=98.1277\n",
      "train epoch 7 step 449,cls loss=0.025538,rec_loss=0.017772,loss=0.043310,mAp=98.1277\n",
      "train epoch 7 step 499,cls loss=0.026540,rec_loss=0.017741,loss=0.044282,mAp=98.0565\n",
      "train epoch 7 step 549,cls loss=0.026898,rec_loss=0.017807,loss=0.044705,mAp=97.9824\n",
      "train epoch 7 step 599,cls loss=0.026414,rec_loss=0.017784,loss=0.044198,mAp=97.9856\n",
      "train epoch 7 step 649,cls loss=0.025998,rec_loss=0.017735,loss=0.043733,mAp=98.0689\n",
      "train epoch 7 step 699,cls loss=0.026759,rec_loss=0.017688,loss=0.044447,mAp=97.9616\n",
      "train epoch 7 step 749,cls loss=0.026859,rec_loss=0.017646,loss=0.044505,mAp=97.9649\n",
      "train epoch 7 step 799,cls loss=0.026879,rec_loss=0.017643,loss=0.044523,mAp=98.0490\n",
      "train epoch 7 step 849,cls loss=0.026598,rec_loss=0.017663,loss=0.044260,mAp=98.0624\n",
      "train epoch 7 step 899,cls loss=0.026677,rec_loss=0.017616,loss=0.044293,mAp=98.0381\n",
      "train epoch 7 step 949,cls loss=0.027170,rec_loss=0.017601,loss=0.044771,mAp=97.9917\n",
      "train epoch 7 step 999,cls loss=0.027072,rec_loss=0.017595,loss=0.044667,mAp=97.9890\n",
      "train epoch 7 step 1049,cls loss=0.027081,rec_loss=0.017594,loss=0.044675,mAp=98.0071\n",
      "train epoch 7 step 1099,cls loss=0.026865,rec_loss=0.017595,loss=0.044460,mAp=98.0360\n",
      "train epoch 7 step 1149,cls loss=0.026998,rec_loss=0.017596,loss=0.044594,mAp=98.0145\n",
      "train epoch 7 step 1199,cls loss=0.026943,rec_loss=0.017590,loss=0.044533,mAp=97.9880\n",
      "train epoch 7 step 1249,cls loss=0.026650,rec_loss=0.017577,loss=0.044226,mAp=98.0168\n",
      "train epoch 7 step 1299,cls loss=0.026506,rec_loss=0.017581,loss=0.044087,mAp=98.0482\n",
      "train epoch 7 step 1349,cls loss=0.027012,rec_loss=0.017609,loss=0.044621,mAp=97.9713\n",
      "train epoch 7 step 1399,cls loss=0.027096,rec_loss=0.017653,loss=0.044750,mAp=97.9687\n",
      "train epoch 7 step 1449,cls loss=0.027311,rec_loss=0.017645,loss=0.044956,mAp=97.9523\n",
      "train epoch 7 step 1499,cls loss=0.027302,rec_loss=0.017639,loss=0.044941,mAp=97.9341\n",
      "train epoch 7 step 1549,cls loss=0.027257,rec_loss=0.017639,loss=0.044897,mAp=97.9604\n",
      "train epoch 7 step 1599,cls loss=0.027316,rec_loss=0.017641,loss=0.044958,mAp=97.9662\n",
      "train epoch 7 step 1649,cls loss=0.027253,rec_loss=0.017643,loss=0.044896,mAp=97.9845\n",
      "train epoch 7 step 1699,cls loss=0.027278,rec_loss=0.017664,loss=0.044942,mAp=97.9821\n",
      "train epoch 7 step 1749,cls loss=0.027243,rec_loss=0.017650,loss=0.044893,mAp=97.9899\n",
      "train epoch 7 step 1799,cls loss=0.027454,rec_loss=0.017648,loss=0.045102,mAp=97.9539\n",
      "train epoch 7 step 1849,cls loss=0.027377,rec_loss=0.017658,loss=0.045035,mAp=97.9580\n",
      "train epoch 7 step 1899,cls loss=0.027332,rec_loss=0.017673,loss=0.045005,mAp=97.9600\n",
      "train epoch 7 step 1949,cls loss=0.027746,rec_loss=0.017658,loss=0.045404,mAp=97.8776\n",
      "train epoch 7 step 1999,cls loss=0.027833,rec_loss=0.017676,loss=0.045509,mAp=97.8600\n",
      "train epoch 7 step 2049,cls loss=0.027621,rec_loss=0.017668,loss=0.045289,mAp=97.8761\n",
      "train epoch 7 step 2099,cls loss=0.027680,rec_loss=0.017690,loss=0.045371,mAp=97.8803\n",
      "train epoch 7 step 2149,cls loss=0.027858,rec_loss=0.017675,loss=0.045533,mAp=97.8460\n",
      "train epoch 7 step 2199,cls loss=0.027837,rec_loss=0.017727,loss=0.045564,mAp=97.8612\n",
      "train epoch 7 step 2249,cls loss=0.028017,rec_loss=0.017730,loss=0.045748,mAp=97.8644\n",
      "train epoch 7 step 2299,cls loss=0.028114,rec_loss=0.017730,loss=0.045844,mAp=97.8379\n",
      "train epoch 7 step 2349,cls loss=0.028354,rec_loss=0.017727,loss=0.046081,mAp=97.8042\n",
      "train epoch 7 step 2399,cls loss=0.028406,rec_loss=0.017739,loss=0.046145,mAp=97.7988\n",
      "train epoch 7 step 2449,cls loss=0.028450,rec_loss=0.017781,loss=0.046231,mAp=97.7959\n",
      "train epoch 7 step 2499,cls loss=0.028552,rec_loss=0.017785,loss=0.046338,mAp=97.7951\n",
      "train epoch 7 step 2549,cls loss=0.028632,rec_loss=0.017761,loss=0.046393,mAp=97.7848\n",
      "train epoch 7 step 2599,cls loss=0.028699,rec_loss=0.017770,loss=0.046468,mAp=97.7937\n",
      "val epoch 7 step 19,cls loss=0.166936,rec_loss=0.019765,loss=0.186701,mAp=85.7832\n",
      "val epoch 7 step 39,cls loss=0.168575,rec_loss=0.019399,loss=0.187973,mAp=83.8665\n",
      "val epoch 7 step 59,cls loss=0.172429,rec_loss=0.019207,loss=0.191635,mAp=83.2455\n",
      "val epoch 7 step 79,cls loss=0.175088,rec_loss=0.019340,loss=0.194427,mAp=83.1441\n",
      "val epoch 7 step 99,cls loss=0.160789,rec_loss=0.018891,loss=0.179680,mAp=83.9560\n",
      "val epoch 7 step 119,cls loss=0.148848,rec_loss=0.018371,loss=0.167219,mAp=84.9148\n",
      "val epoch 7 step 139,cls loss=0.142049,rec_loss=0.018062,loss=0.160111,mAp=84.7205\n",
      "val epoch 7 step 159,cls loss=0.141346,rec_loss=0.017818,loss=0.159164,mAp=84.0085\n",
      "val epoch 7 step 179,cls loss=0.136147,rec_loss=0.017599,loss=0.153746,mAp=84.4046\n",
      "val epoch 7 step 199,cls loss=0.137170,rec_loss=0.017472,loss=0.154642,mAp=84.1930\n",
      "val epoch 7 step 219,cls loss=0.138372,rec_loss=0.017557,loss=0.155929,mAp=84.3486\n",
      "val epoch 7 step 239,cls loss=0.136771,rec_loss=0.017524,loss=0.154295,mAp=84.6480\n",
      "val epoch 7 step 259,cls loss=0.136561,rec_loss=0.017511,loss=0.154072,mAp=84.7319\n",
      "val epoch 7 step 279,cls loss=0.140336,rec_loss=0.017551,loss=0.157887,mAp=84.8165\n",
      "val epoch 7 step 299,cls loss=0.141903,rec_loss=0.017478,loss=0.159381,mAp=84.9137\n",
      "val epoch 7 step 319,cls loss=0.144936,rec_loss=0.017630,loss=0.162567,mAp=84.6827\n",
      "val epoch 7 step 339,cls loss=0.145535,rec_loss=0.017622,loss=0.163157,mAp=84.7396\n",
      "val epoch 7 step 359,cls loss=0.148978,rec_loss=0.017593,loss=0.166571,mAp=84.4997\n",
      "epoch 7 done!\n",
      "train epoch 8 step 49,cls loss=0.014258,rec_loss=0.015796,loss=0.030054,mAp=98.7500\n",
      "train epoch 8 step 99,cls loss=0.015907,rec_loss=0.015983,loss=0.031890,mAp=98.8625\n",
      "train epoch 8 step 149,cls loss=0.016587,rec_loss=0.015991,loss=0.032579,mAp=98.9000\n",
      "train epoch 8 step 199,cls loss=0.018750,rec_loss=0.016199,loss=0.034950,mAp=98.6437\n",
      "train epoch 8 step 249,cls loss=0.019162,rec_loss=0.016489,loss=0.035652,mAp=98.7550\n",
      "train epoch 8 step 299,cls loss=0.017698,rec_loss=0.016531,loss=0.034229,mAp=98.9069\n",
      "train epoch 8 step 349,cls loss=0.017248,rec_loss=0.016509,loss=0.033757,mAp=98.9560\n",
      "train epoch 8 step 399,cls loss=0.017056,rec_loss=0.016729,loss=0.033785,mAp=99.0027\n",
      "train epoch 8 step 449,cls loss=0.017654,rec_loss=0.016676,loss=0.034330,mAp=98.9330\n",
      "train epoch 8 step 499,cls loss=0.018513,rec_loss=0.016752,loss=0.035265,mAp=98.9223\n",
      "train epoch 8 step 549,cls loss=0.019195,rec_loss=0.016884,loss=0.036080,mAp=98.8574\n",
      "train epoch 8 step 599,cls loss=0.019485,rec_loss=0.016957,loss=0.036442,mAp=98.8415\n",
      "train epoch 8 step 649,cls loss=0.020079,rec_loss=0.017031,loss=0.037110,mAp=98.7590\n",
      "train epoch 8 step 699,cls loss=0.020064,rec_loss=0.017077,loss=0.037141,mAp=98.7372\n",
      "train epoch 8 step 749,cls loss=0.020573,rec_loss=0.017088,loss=0.037661,mAp=98.6667\n",
      "train epoch 8 step 799,cls loss=0.020705,rec_loss=0.017084,loss=0.037789,mAp=98.6147\n",
      "train epoch 8 step 849,cls loss=0.021044,rec_loss=0.017150,loss=0.038194,mAp=98.6270\n",
      "train epoch 8 step 899,cls loss=0.021069,rec_loss=0.017165,loss=0.038234,mAp=98.6524\n",
      "train epoch 8 step 949,cls loss=0.020906,rec_loss=0.017126,loss=0.038031,mAp=98.6539\n",
      "train epoch 8 step 999,cls loss=0.020748,rec_loss=0.017153,loss=0.037901,mAp=98.6816\n",
      "train epoch 8 step 1049,cls loss=0.020560,rec_loss=0.017116,loss=0.037676,mAp=98.7080\n",
      "train epoch 8 step 1099,cls loss=0.021012,rec_loss=0.017105,loss=0.038116,mAp=98.6902\n",
      "train epoch 8 step 1149,cls loss=0.021230,rec_loss=0.017154,loss=0.038384,mAp=98.6642\n",
      "train epoch 8 step 1199,cls loss=0.021230,rec_loss=0.017202,loss=0.038432,mAp=98.6738\n",
      "train epoch 8 step 1249,cls loss=0.021491,rec_loss=0.017178,loss=0.038669,mAp=98.6464\n",
      "train epoch 8 step 1299,cls loss=0.021537,rec_loss=0.017201,loss=0.038738,mAp=98.6471\n",
      "train epoch 8 step 1349,cls loss=0.021623,rec_loss=0.017192,loss=0.038815,mAp=98.6307\n",
      "train epoch 8 step 1399,cls loss=0.021653,rec_loss=0.017166,loss=0.038819,mAp=98.6491\n",
      "train epoch 8 step 1449,cls loss=0.021708,rec_loss=0.017129,loss=0.038836,mAp=98.6165\n",
      "train epoch 8 step 1499,cls loss=0.022345,rec_loss=0.017182,loss=0.039526,mAp=98.5550\n",
      "train epoch 8 step 1549,cls loss=0.022329,rec_loss=0.017229,loss=0.039558,mAp=98.5452\n",
      "train epoch 8 step 1599,cls loss=0.022302,rec_loss=0.017233,loss=0.039536,mAp=98.5451\n",
      "train epoch 8 step 1649,cls loss=0.022729,rec_loss=0.017230,loss=0.039959,mAp=98.5147\n",
      "train epoch 8 step 1699,cls loss=0.023051,rec_loss=0.017263,loss=0.040314,mAp=98.4750\n",
      "train epoch 8 step 1749,cls loss=0.023111,rec_loss=0.017295,loss=0.040405,mAp=98.4400\n",
      "train epoch 8 step 1799,cls loss=0.023296,rec_loss=0.017315,loss=0.040611,mAp=98.4066\n",
      "train epoch 8 step 1849,cls loss=0.023378,rec_loss=0.017322,loss=0.040701,mAp=98.4136\n",
      "train epoch 8 step 1899,cls loss=0.023519,rec_loss=0.017324,loss=0.040842,mAp=98.4077\n",
      "train epoch 8 step 1949,cls loss=0.023518,rec_loss=0.017306,loss=0.040824,mAp=98.4133\n",
      "train epoch 8 step 1999,cls loss=0.023509,rec_loss=0.017285,loss=0.040794,mAp=98.4169\n",
      "train epoch 8 step 2049,cls loss=0.023467,rec_loss=0.017266,loss=0.040733,mAp=98.4106\n",
      "train epoch 8 step 2099,cls loss=0.023506,rec_loss=0.017249,loss=0.040755,mAp=98.4123\n",
      "train epoch 8 step 2149,cls loss=0.023435,rec_loss=0.017260,loss=0.040695,mAp=98.4283\n",
      "train epoch 8 step 2199,cls loss=0.023283,rec_loss=0.017253,loss=0.040536,mAp=98.4313\n",
      "train epoch 8 step 2249,cls loss=0.023199,rec_loss=0.017248,loss=0.040447,mAp=98.4560\n",
      "train epoch 8 step 2299,cls loss=0.023396,rec_loss=0.017247,loss=0.040643,mAp=98.4497\n",
      "train epoch 8 step 2349,cls loss=0.023425,rec_loss=0.017251,loss=0.040676,mAp=98.4119\n",
      "train epoch 8 step 2399,cls loss=0.023485,rec_loss=0.017266,loss=0.040751,mAp=98.3924\n",
      "train epoch 8 step 2449,cls loss=0.023652,rec_loss=0.017294,loss=0.040947,mAp=98.3698\n",
      "train epoch 8 step 2499,cls loss=0.023590,rec_loss=0.017301,loss=0.040891,mAp=98.3736\n",
      "train epoch 8 step 2549,cls loss=0.023717,rec_loss=0.017330,loss=0.041047,mAp=98.3785\n",
      "train epoch 8 step 2599,cls loss=0.023951,rec_loss=0.017380,loss=0.041331,mAp=98.3463\n",
      "val epoch 8 step 19,cls loss=0.230070,rec_loss=0.018830,loss=0.248900,mAp=82.0094\n",
      "val epoch 8 step 39,cls loss=0.205427,rec_loss=0.018973,loss=0.224400,mAp=80.3784\n",
      "val epoch 8 step 59,cls loss=0.196824,rec_loss=0.018769,loss=0.215593,mAp=81.3961\n",
      "val epoch 8 step 79,cls loss=0.191889,rec_loss=0.018915,loss=0.210804,mAp=81.4386\n",
      "val epoch 8 step 99,cls loss=0.177747,rec_loss=0.018500,loss=0.196247,mAp=82.2227\n",
      "val epoch 8 step 119,cls loss=0.164426,rec_loss=0.018083,loss=0.182510,mAp=83.3195\n",
      "val epoch 8 step 139,cls loss=0.155772,rec_loss=0.017945,loss=0.173717,mAp=83.8009\n",
      "val epoch 8 step 159,cls loss=0.157483,rec_loss=0.017680,loss=0.175163,mAp=83.1374\n",
      "val epoch 8 step 179,cls loss=0.150745,rec_loss=0.017496,loss=0.168241,mAp=83.6119\n",
      "val epoch 8 step 199,cls loss=0.149033,rec_loss=0.017401,loss=0.166434,mAp=83.9049\n",
      "val epoch 8 step 219,cls loss=0.150036,rec_loss=0.017515,loss=0.167551,mAp=83.6674\n",
      "val epoch 8 step 239,cls loss=0.149099,rec_loss=0.017472,loss=0.166572,mAp=83.5057\n",
      "val epoch 8 step 259,cls loss=0.149420,rec_loss=0.017445,loss=0.166865,mAp=83.4578\n",
      "val epoch 8 step 279,cls loss=0.153359,rec_loss=0.017464,loss=0.170823,mAp=83.4744\n",
      "val epoch 8 step 299,cls loss=0.156411,rec_loss=0.017376,loss=0.173787,mAp=83.2511\n",
      "val epoch 8 step 319,cls loss=0.158775,rec_loss=0.017481,loss=0.176256,mAp=83.0219\n",
      "val epoch 8 step 339,cls loss=0.162754,rec_loss=0.017475,loss=0.180229,mAp=82.8247\n",
      "val epoch 8 step 359,cls loss=0.166487,rec_loss=0.017400,loss=0.183887,mAp=82.6302\n",
      "epoch 8 done!\n",
      "train epoch 9 step 49,cls loss=0.018409,rec_loss=0.016508,loss=0.034917,mAp=98.4958\n",
      "train epoch 9 step 99,cls loss=0.016551,rec_loss=0.016451,loss=0.033002,mAp=99.0187\n",
      "train epoch 9 step 149,cls loss=0.014352,rec_loss=0.016822,loss=0.031173,mAp=99.3458\n",
      "train epoch 9 step 199,cls loss=0.015365,rec_loss=0.017115,loss=0.032480,mAp=99.1281\n",
      "train epoch 9 step 249,cls loss=0.015023,rec_loss=0.016902,loss=0.031925,mAp=99.1308\n",
      "train epoch 9 step 299,cls loss=0.014764,rec_loss=0.016844,loss=0.031608,mAp=99.0674\n",
      "train epoch 9 step 349,cls loss=0.015511,rec_loss=0.016737,loss=0.032249,mAp=98.9161\n",
      "train epoch 9 step 399,cls loss=0.015983,rec_loss=0.016718,loss=0.032701,mAp=98.8245\n",
      "train epoch 9 step 449,cls loss=0.016094,rec_loss=0.016875,loss=0.032968,mAp=98.8829\n",
      "train epoch 9 step 499,cls loss=0.016523,rec_loss=0.016866,loss=0.033389,mAp=98.8863\n",
      "train epoch 9 step 549,cls loss=0.017157,rec_loss=0.016924,loss=0.034081,mAp=98.8284\n",
      "train epoch 9 step 599,cls loss=0.016892,rec_loss=0.016936,loss=0.033828,mAp=98.8670\n",
      "train epoch 9 step 649,cls loss=0.016449,rec_loss=0.016884,loss=0.033333,mAp=98.8881\n",
      "train epoch 9 step 699,cls loss=0.016516,rec_loss=0.016852,loss=0.033368,mAp=98.9170\n",
      "train epoch 9 step 749,cls loss=0.016759,rec_loss=0.016843,loss=0.033603,mAp=98.8281\n",
      "train epoch 9 step 799,cls loss=0.017476,rec_loss=0.016815,loss=0.034291,mAp=98.8023\n",
      "train epoch 9 step 849,cls loss=0.017795,rec_loss=0.016778,loss=0.034573,mAp=98.8255\n",
      "train epoch 9 step 899,cls loss=0.017570,rec_loss=0.016805,loss=0.034375,mAp=98.8491\n",
      "train epoch 9 step 949,cls loss=0.017641,rec_loss=0.016764,loss=0.034405,mAp=98.8360\n",
      "train epoch 9 step 999,cls loss=0.017673,rec_loss=0.016813,loss=0.034486,mAp=98.8240\n",
      "train epoch 9 step 1049,cls loss=0.018094,rec_loss=0.016795,loss=0.034889,mAp=98.7538\n",
      "train epoch 9 step 1099,cls loss=0.018444,rec_loss=0.016867,loss=0.035311,mAp=98.7430\n",
      "train epoch 9 step 1149,cls loss=0.019006,rec_loss=0.016883,loss=0.035888,mAp=98.6665\n",
      "train epoch 9 step 1199,cls loss=0.018930,rec_loss=0.016908,loss=0.035838,mAp=98.6683\n",
      "train epoch 9 step 1249,cls loss=0.018950,rec_loss=0.016916,loss=0.035865,mAp=98.6849\n",
      "train epoch 9 step 1299,cls loss=0.019048,rec_loss=0.016922,loss=0.035970,mAp=98.6714\n",
      "train epoch 9 step 1349,cls loss=0.018918,rec_loss=0.016918,loss=0.035835,mAp=98.6696\n",
      "train epoch 9 step 1399,cls loss=0.019333,rec_loss=0.016924,loss=0.036257,mAp=98.6401\n",
      "train epoch 9 step 1449,cls loss=0.019318,rec_loss=0.016964,loss=0.036282,mAp=98.6186\n",
      "train epoch 9 step 1499,cls loss=0.019710,rec_loss=0.016976,loss=0.036685,mAp=98.5791\n",
      "train epoch 9 step 1549,cls loss=0.019501,rec_loss=0.016992,loss=0.036493,mAp=98.6007\n",
      "train epoch 9 step 1599,cls loss=0.019414,rec_loss=0.016985,loss=0.036399,mAp=98.6088\n",
      "train epoch 9 step 1649,cls loss=0.019267,rec_loss=0.016980,loss=0.036247,mAp=98.6244\n",
      "train epoch 9 step 1699,cls loss=0.019393,rec_loss=0.016967,loss=0.036360,mAp=98.6305\n",
      "train epoch 9 step 1749,cls loss=0.019690,rec_loss=0.017020,loss=0.036710,mAp=98.5855\n",
      "train epoch 9 step 1799,cls loss=0.019951,rec_loss=0.017023,loss=0.036975,mAp=98.5861\n",
      "train epoch 9 step 1849,cls loss=0.020035,rec_loss=0.017033,loss=0.037068,mAp=98.5680\n",
      "train epoch 9 step 1899,cls loss=0.020155,rec_loss=0.017051,loss=0.037206,mAp=98.5622\n",
      "train epoch 9 step 1949,cls loss=0.020465,rec_loss=0.017045,loss=0.037510,mAp=98.5228\n",
      "train epoch 9 step 1999,cls loss=0.020610,rec_loss=0.017039,loss=0.037649,mAp=98.5108\n",
      "train epoch 9 step 2049,cls loss=0.020752,rec_loss=0.017044,loss=0.037796,mAp=98.4709\n",
      "train epoch 9 step 2099,cls loss=0.020689,rec_loss=0.017052,loss=0.037741,mAp=98.4875\n",
      "train epoch 9 step 2149,cls loss=0.020721,rec_loss=0.017070,loss=0.037791,mAp=98.4965\n",
      "train epoch 9 step 2199,cls loss=0.020912,rec_loss=0.017098,loss=0.038010,mAp=98.4700\n",
      "train epoch 9 step 2249,cls loss=0.021384,rec_loss=0.017101,loss=0.038485,mAp=98.4076\n",
      "train epoch 9 step 2299,cls loss=0.021750,rec_loss=0.017104,loss=0.038854,mAp=98.3632\n",
      "train epoch 9 step 2349,cls loss=0.021929,rec_loss=0.017137,loss=0.039067,mAp=98.3357\n",
      "train epoch 9 step 2399,cls loss=0.021858,rec_loss=0.017158,loss=0.039016,mAp=98.3454\n",
      "train epoch 9 step 2449,cls loss=0.021933,rec_loss=0.017140,loss=0.039074,mAp=98.3596\n",
      "train epoch 9 step 2499,cls loss=0.021986,rec_loss=0.017140,loss=0.039126,mAp=98.3584\n",
      "train epoch 9 step 2549,cls loss=0.021907,rec_loss=0.017152,loss=0.039059,mAp=98.3642\n",
      "train epoch 9 step 2599,cls loss=0.021902,rec_loss=0.017144,loss=0.039045,mAp=98.3732\n",
      "val epoch 9 step 19,cls loss=0.219759,rec_loss=0.019723,loss=0.239482,mAp=81.8682\n",
      "val epoch 9 step 39,cls loss=0.176514,rec_loss=0.019513,loss=0.196027,mAp=83.8314\n",
      "val epoch 9 step 59,cls loss=0.179552,rec_loss=0.019322,loss=0.198874,mAp=83.2401\n",
      "val epoch 9 step 79,cls loss=0.187285,rec_loss=0.019618,loss=0.206903,mAp=83.1258\n",
      "val epoch 9 step 99,cls loss=0.170201,rec_loss=0.019307,loss=0.189507,mAp=83.7106\n",
      "val epoch 9 step 119,cls loss=0.163384,rec_loss=0.018811,loss=0.182195,mAp=84.2718\n",
      "val epoch 9 step 139,cls loss=0.158500,rec_loss=0.018535,loss=0.177035,mAp=84.1177\n",
      "val epoch 9 step 159,cls loss=0.156950,rec_loss=0.018270,loss=0.175221,mAp=83.9746\n",
      "val epoch 9 step 179,cls loss=0.152994,rec_loss=0.018074,loss=0.171068,mAp=84.1235\n",
      "val epoch 9 step 199,cls loss=0.153714,rec_loss=0.017987,loss=0.171702,mAp=83.7893\n",
      "val epoch 9 step 219,cls loss=0.153143,rec_loss=0.018037,loss=0.171181,mAp=83.9251\n",
      "val epoch 9 step 239,cls loss=0.149705,rec_loss=0.017968,loss=0.167673,mAp=84.2079\n",
      "val epoch 9 step 259,cls loss=0.148472,rec_loss=0.017948,loss=0.166420,mAp=84.4740\n",
      "val epoch 9 step 279,cls loss=0.154138,rec_loss=0.017974,loss=0.172112,mAp=84.2860\n",
      "val epoch 9 step 299,cls loss=0.157962,rec_loss=0.017880,loss=0.175842,mAp=84.0923\n",
      "val epoch 9 step 319,cls loss=0.160869,rec_loss=0.018018,loss=0.178887,mAp=83.9126\n",
      "val epoch 9 step 339,cls loss=0.164695,rec_loss=0.017969,loss=0.182664,mAp=83.7562\n",
      "val epoch 9 step 359,cls loss=0.169778,rec_loss=0.017907,loss=0.187685,mAp=83.3913\n",
      "epoch 9 done!\n",
      "train epoch 10 step 49,cls loss=0.014209,rec_loss=0.015763,loss=0.029972,mAp=99.3750\n",
      "train epoch 10 step 99,cls loss=0.014643,rec_loss=0.015631,loss=0.030273,mAp=99.3139\n",
      "train epoch 10 step 149,cls loss=0.015446,rec_loss=0.015945,loss=0.031391,mAp=99.1676\n",
      "train epoch 10 step 199,cls loss=0.016514,rec_loss=0.016212,loss=0.032726,mAp=99.0413\n",
      "train epoch 10 step 249,cls loss=0.017054,rec_loss=0.016418,loss=0.033472,mAp=98.9247\n",
      "train epoch 10 step 299,cls loss=0.017397,rec_loss=0.016411,loss=0.033808,mAp=98.9234\n",
      "train epoch 10 step 349,cls loss=0.016719,rec_loss=0.016361,loss=0.033080,mAp=99.0296\n",
      "train epoch 10 step 399,cls loss=0.016079,rec_loss=0.016244,loss=0.032323,mAp=99.1321\n",
      "train epoch 10 step 449,cls loss=0.015410,rec_loss=0.016260,loss=0.031669,mAp=99.2193\n",
      "train epoch 10 step 499,cls loss=0.014724,rec_loss=0.016203,loss=0.030927,mAp=99.2307\n",
      "train epoch 10 step 549,cls loss=0.014599,rec_loss=0.016215,loss=0.030815,mAp=99.1958\n",
      "train epoch 10 step 599,cls loss=0.014758,rec_loss=0.016257,loss=0.031015,mAp=99.1749\n",
      "train epoch 10 step 649,cls loss=0.014886,rec_loss=0.016252,loss=0.031138,mAp=99.1608\n",
      "train epoch 10 step 699,cls loss=0.014809,rec_loss=0.016169,loss=0.030978,mAp=99.1255\n",
      "train epoch 10 step 749,cls loss=0.014508,rec_loss=0.016135,loss=0.030644,mAp=99.1282\n",
      "train epoch 10 step 799,cls loss=0.014878,rec_loss=0.016269,loss=0.031148,mAp=99.0832\n",
      "train epoch 10 step 849,cls loss=0.015304,rec_loss=0.016318,loss=0.031622,mAp=99.0377\n",
      "train epoch 10 step 899,cls loss=0.015807,rec_loss=0.016429,loss=0.032236,mAp=98.9632\n",
      "train epoch 10 step 949,cls loss=0.015734,rec_loss=0.016433,loss=0.032167,mAp=98.9381\n",
      "train epoch 10 step 999,cls loss=0.015816,rec_loss=0.016432,loss=0.032248,mAp=98.9328\n",
      "train epoch 10 step 1049,cls loss=0.015788,rec_loss=0.016380,loss=0.032168,mAp=98.9102\n",
      "train epoch 10 step 1099,cls loss=0.016043,rec_loss=0.016360,loss=0.032403,mAp=98.8481\n",
      "train epoch 10 step 1149,cls loss=0.016115,rec_loss=0.016363,loss=0.032479,mAp=98.8500\n",
      "train epoch 10 step 1199,cls loss=0.016752,rec_loss=0.016392,loss=0.033144,mAp=98.7507\n",
      "train epoch 10 step 1249,cls loss=0.017556,rec_loss=0.016467,loss=0.034023,mAp=98.6395\n",
      "train epoch 10 step 1299,cls loss=0.017642,rec_loss=0.016498,loss=0.034140,mAp=98.6486\n",
      "train epoch 10 step 1349,cls loss=0.018293,rec_loss=0.016526,loss=0.034819,mAp=98.5705\n",
      "train epoch 10 step 1399,cls loss=0.018428,rec_loss=0.016531,loss=0.034960,mAp=98.5804\n",
      "train epoch 10 step 1449,cls loss=0.018877,rec_loss=0.016571,loss=0.035448,mAp=98.5230\n",
      "train epoch 10 step 1499,cls loss=0.019404,rec_loss=0.016597,loss=0.036001,mAp=98.4707\n",
      "train epoch 10 step 1549,cls loss=0.019583,rec_loss=0.016606,loss=0.036189,mAp=98.4703\n",
      "train epoch 10 step 1599,cls loss=0.019419,rec_loss=0.016625,loss=0.036043,mAp=98.4942\n",
      "train epoch 10 step 1649,cls loss=0.019327,rec_loss=0.016624,loss=0.035951,mAp=98.5247\n",
      "train epoch 10 step 1699,cls loss=0.019329,rec_loss=0.016642,loss=0.035971,mAp=98.5325\n",
      "train epoch 10 step 1749,cls loss=0.019247,rec_loss=0.016682,loss=0.035929,mAp=98.5495\n",
      "train epoch 10 step 1799,cls loss=0.019507,rec_loss=0.016716,loss=0.036223,mAp=98.5435\n",
      "train epoch 10 step 1849,cls loss=0.019589,rec_loss=0.016699,loss=0.036288,mAp=98.5296\n",
      "train epoch 10 step 1899,cls loss=0.019410,rec_loss=0.016690,loss=0.036100,mAp=98.5595\n",
      "train epoch 10 step 1949,cls loss=0.019340,rec_loss=0.016648,loss=0.035989,mAp=98.5431\n",
      "train epoch 10 step 1999,cls loss=0.019432,rec_loss=0.016639,loss=0.036071,mAp=98.5337\n",
      "train epoch 10 step 2049,cls loss=0.019603,rec_loss=0.016664,loss=0.036267,mAp=98.5143\n",
      "train epoch 10 step 2099,cls loss=0.019576,rec_loss=0.016651,loss=0.036227,mAp=98.5266\n",
      "train epoch 10 step 2149,cls loss=0.019537,rec_loss=0.016629,loss=0.036166,mAp=98.5138\n",
      "train epoch 10 step 2199,cls loss=0.019562,rec_loss=0.016633,loss=0.036195,mAp=98.5220\n",
      "train epoch 10 step 2249,cls loss=0.019518,rec_loss=0.016628,loss=0.036145,mAp=98.5327\n",
      "train epoch 10 step 2299,cls loss=0.019420,rec_loss=0.016630,loss=0.036050,mAp=98.5473\n",
      "train epoch 10 step 2349,cls loss=0.019393,rec_loss=0.016620,loss=0.036013,mAp=98.5387\n",
      "train epoch 10 step 2399,cls loss=0.019262,rec_loss=0.016604,loss=0.035866,mAp=98.5570\n",
      "train epoch 10 step 2449,cls loss=0.019070,rec_loss=0.016599,loss=0.035669,mAp=98.5791\n",
      "train epoch 10 step 2499,cls loss=0.019018,rec_loss=0.016610,loss=0.035627,mAp=98.5746\n",
      "train epoch 10 step 2549,cls loss=0.018999,rec_loss=0.016591,loss=0.035590,mAp=98.5742\n",
      "train epoch 10 step 2599,cls loss=0.018890,rec_loss=0.016561,loss=0.035451,mAp=98.5771\n",
      "val epoch 10 step 19,cls loss=0.238694,rec_loss=0.021063,loss=0.259757,mAp=82.3837\n",
      "val epoch 10 step 39,cls loss=0.213322,rec_loss=0.020653,loss=0.233976,mAp=82.6165\n",
      "val epoch 10 step 59,cls loss=0.213533,rec_loss=0.020381,loss=0.233915,mAp=81.6625\n",
      "val epoch 10 step 79,cls loss=0.216978,rec_loss=0.020698,loss=0.237676,mAp=82.0071\n",
      "val epoch 10 step 99,cls loss=0.197020,rec_loss=0.020361,loss=0.217381,mAp=83.3296\n",
      "val epoch 10 step 119,cls loss=0.179356,rec_loss=0.019968,loss=0.199325,mAp=84.8505\n",
      "val epoch 10 step 139,cls loss=0.169138,rec_loss=0.019637,loss=0.188775,mAp=84.7647\n",
      "val epoch 10 step 159,cls loss=0.164984,rec_loss=0.019338,loss=0.184322,mAp=84.5682\n",
      "val epoch 10 step 179,cls loss=0.161484,rec_loss=0.019139,loss=0.180623,mAp=84.5370\n",
      "val epoch 10 step 199,cls loss=0.159755,rec_loss=0.019031,loss=0.178786,mAp=84.4458\n",
      "val epoch 10 step 219,cls loss=0.158320,rec_loss=0.019110,loss=0.177429,mAp=84.7613\n",
      "val epoch 10 step 239,cls loss=0.153612,rec_loss=0.019058,loss=0.172670,mAp=85.1331\n",
      "val epoch 10 step 259,cls loss=0.156433,rec_loss=0.019075,loss=0.175507,mAp=85.1116\n",
      "val epoch 10 step 279,cls loss=0.161465,rec_loss=0.019151,loss=0.180616,mAp=85.1097\n",
      "val epoch 10 step 299,cls loss=0.164157,rec_loss=0.019077,loss=0.183234,mAp=85.0264\n",
      "val epoch 10 step 319,cls loss=0.168124,rec_loss=0.019204,loss=0.187328,mAp=84.7495\n",
      "val epoch 10 step 339,cls loss=0.170353,rec_loss=0.019192,loss=0.189545,mAp=84.7448\n",
      "val epoch 10 step 359,cls loss=0.175615,rec_loss=0.019094,loss=0.194709,mAp=84.2817\n",
      "epoch 10 done!\n",
      "train epoch 11 step 49,cls loss=0.014798,rec_loss=0.014389,loss=0.029187,mAp=98.4167\n",
      "train epoch 11 step 99,cls loss=0.011359,rec_loss=0.015151,loss=0.026510,mAp=98.9375\n",
      "train epoch 11 step 149,cls loss=0.010666,rec_loss=0.015679,loss=0.026345,mAp=99.2917\n",
      "train epoch 11 step 199,cls loss=0.013814,rec_loss=0.015547,loss=0.029361,mAp=99.1604\n",
      "train epoch 11 step 249,cls loss=0.013075,rec_loss=0.015677,loss=0.028752,mAp=99.2567\n",
      "train epoch 11 step 299,cls loss=0.013466,rec_loss=0.015608,loss=0.029074,mAp=99.1306\n",
      "train epoch 11 step 349,cls loss=0.014120,rec_loss=0.015609,loss=0.029729,mAp=99.0607\n",
      "train epoch 11 step 399,cls loss=0.014481,rec_loss=0.015814,loss=0.030294,mAp=99.0479\n",
      "train epoch 11 step 449,cls loss=0.014767,rec_loss=0.015971,loss=0.030739,mAp=99.1028\n",
      "train epoch 11 step 499,cls loss=0.014560,rec_loss=0.015973,loss=0.030532,mAp=99.1592\n",
      "train epoch 11 step 549,cls loss=0.014445,rec_loss=0.016007,loss=0.030452,mAp=99.1917\n",
      "train epoch 11 step 599,cls loss=0.014511,rec_loss=0.016051,loss=0.030562,mAp=99.1528\n",
      "train epoch 11 step 649,cls loss=0.015100,rec_loss=0.016022,loss=0.031122,mAp=99.1250\n",
      "train epoch 11 step 699,cls loss=0.015162,rec_loss=0.016148,loss=0.031310,mAp=99.0417\n",
      "train epoch 11 step 749,cls loss=0.015335,rec_loss=0.016168,loss=0.031502,mAp=99.0509\n",
      "train epoch 11 step 799,cls loss=0.015051,rec_loss=0.016180,loss=0.031231,mAp=99.0581\n",
      "train epoch 11 step 849,cls loss=0.015001,rec_loss=0.016164,loss=0.031165,mAp=99.1062\n",
      "train epoch 11 step 899,cls loss=0.014801,rec_loss=0.016168,loss=0.030969,mAp=99.1290\n",
      "train epoch 11 step 949,cls loss=0.014957,rec_loss=0.016173,loss=0.031129,mAp=99.1406\n",
      "train epoch 11 step 999,cls loss=0.015523,rec_loss=0.016193,loss=0.031716,mAp=99.0324\n",
      "train epoch 11 step 1049,cls loss=0.015877,rec_loss=0.016184,loss=0.032061,mAp=98.9854\n",
      "train epoch 11 step 1099,cls loss=0.016043,rec_loss=0.016178,loss=0.032222,mAp=98.9697\n",
      "train epoch 11 step 1149,cls loss=0.016281,rec_loss=0.016193,loss=0.032474,mAp=98.9469\n",
      "train epoch 11 step 1199,cls loss=0.016098,rec_loss=0.016182,loss=0.032280,mAp=98.9630\n",
      "train epoch 11 step 1249,cls loss=0.015884,rec_loss=0.016155,loss=0.032039,mAp=98.9628\n",
      "train epoch 11 step 1299,cls loss=0.016203,rec_loss=0.016109,loss=0.032312,mAp=98.9259\n",
      "train epoch 11 step 1349,cls loss=0.016254,rec_loss=0.016156,loss=0.032410,mAp=98.9286\n",
      "train epoch 11 step 1399,cls loss=0.016363,rec_loss=0.016165,loss=0.032528,mAp=98.9297\n",
      "train epoch 11 step 1449,cls loss=0.016365,rec_loss=0.016177,loss=0.032542,mAp=98.9455\n",
      "train epoch 11 step 1499,cls loss=0.016542,rec_loss=0.016178,loss=0.032721,mAp=98.9479\n",
      "train epoch 11 step 1549,cls loss=0.016489,rec_loss=0.016191,loss=0.032679,mAp=98.9635\n",
      "train epoch 11 step 1599,cls loss=0.016260,rec_loss=0.016184,loss=0.032444,mAp=98.9803\n",
      "train epoch 11 step 1649,cls loss=0.016594,rec_loss=0.016193,loss=0.032787,mAp=98.9576\n",
      "train epoch 11 step 1699,cls loss=0.016573,rec_loss=0.016209,loss=0.032783,mAp=98.9459\n",
      "train epoch 11 step 1749,cls loss=0.016640,rec_loss=0.016254,loss=0.032895,mAp=98.9285\n",
      "train epoch 11 step 1799,cls loss=0.016881,rec_loss=0.016288,loss=0.033168,mAp=98.9038\n",
      "train epoch 11 step 1849,cls loss=0.016912,rec_loss=0.016325,loss=0.033237,mAp=98.8918\n",
      "train epoch 11 step 1899,cls loss=0.017137,rec_loss=0.016318,loss=0.033455,mAp=98.8512\n",
      "train epoch 11 step 1949,cls loss=0.017188,rec_loss=0.016320,loss=0.033508,mAp=98.8540\n",
      "train epoch 11 step 1999,cls loss=0.017277,rec_loss=0.016376,loss=0.033653,mAp=98.8698\n",
      "train epoch 11 step 2049,cls loss=0.017608,rec_loss=0.016409,loss=0.034017,mAp=98.8124\n",
      "train epoch 11 step 2099,cls loss=0.017945,rec_loss=0.016439,loss=0.034384,mAp=98.7544\n",
      "train epoch 11 step 2149,cls loss=0.017807,rec_loss=0.016442,loss=0.034250,mAp=98.7736\n",
      "train epoch 11 step 2199,cls loss=0.017782,rec_loss=0.016446,loss=0.034227,mAp=98.7901\n",
      "train epoch 11 step 2249,cls loss=0.017873,rec_loss=0.016465,loss=0.034338,mAp=98.7715\n",
      "train epoch 11 step 2299,cls loss=0.017789,rec_loss=0.016468,loss=0.034257,mAp=98.7873\n",
      "train epoch 11 step 2349,cls loss=0.017821,rec_loss=0.016475,loss=0.034296,mAp=98.7836\n",
      "train epoch 11 step 2399,cls loss=0.017918,rec_loss=0.016488,loss=0.034406,mAp=98.7794\n",
      "train epoch 11 step 2449,cls loss=0.017919,rec_loss=0.016497,loss=0.034416,mAp=98.7771\n",
      "train epoch 11 step 2499,cls loss=0.017958,rec_loss=0.016508,loss=0.034466,mAp=98.7636\n",
      "train epoch 11 step 2549,cls loss=0.017992,rec_loss=0.016512,loss=0.034504,mAp=98.7380\n",
      "train epoch 11 step 2599,cls loss=0.017994,rec_loss=0.016516,loss=0.034510,mAp=98.7526\n",
      "val epoch 11 step 19,cls loss=0.249980,rec_loss=0.019328,loss=0.269308,mAp=82.4308\n",
      "val epoch 11 step 39,cls loss=0.220885,rec_loss=0.018938,loss=0.239823,mAp=81.9569\n",
      "val epoch 11 step 59,cls loss=0.215745,rec_loss=0.018614,loss=0.234358,mAp=81.3147\n",
      "val epoch 11 step 79,cls loss=0.207737,rec_loss=0.018753,loss=0.226490,mAp=81.9757\n",
      "val epoch 11 step 99,cls loss=0.191478,rec_loss=0.018418,loss=0.209896,mAp=82.7763\n",
      "val epoch 11 step 119,cls loss=0.178000,rec_loss=0.017926,loss=0.195926,mAp=83.8153\n",
      "val epoch 11 step 139,cls loss=0.169082,rec_loss=0.017673,loss=0.186755,mAp=83.5383\n",
      "val epoch 11 step 159,cls loss=0.167661,rec_loss=0.017386,loss=0.185047,mAp=82.9630\n",
      "val epoch 11 step 179,cls loss=0.165918,rec_loss=0.017220,loss=0.183138,mAp=83.4964\n",
      "val epoch 11 step 199,cls loss=0.164351,rec_loss=0.017107,loss=0.181459,mAp=83.2101\n",
      "val epoch 11 step 219,cls loss=0.166172,rec_loss=0.017181,loss=0.183353,mAp=83.2878\n",
      "val epoch 11 step 239,cls loss=0.163632,rec_loss=0.017133,loss=0.180766,mAp=83.7618\n",
      "val epoch 11 step 259,cls loss=0.163546,rec_loss=0.017133,loss=0.180679,mAp=83.8306\n",
      "val epoch 11 step 279,cls loss=0.168121,rec_loss=0.017179,loss=0.185300,mAp=83.9016\n",
      "val epoch 11 step 299,cls loss=0.170701,rec_loss=0.017075,loss=0.187776,mAp=83.9574\n",
      "val epoch 11 step 319,cls loss=0.174730,rec_loss=0.017192,loss=0.191922,mAp=83.7118\n",
      "val epoch 11 step 339,cls loss=0.177365,rec_loss=0.017167,loss=0.194532,mAp=83.5673\n",
      "val epoch 11 step 359,cls loss=0.181958,rec_loss=0.017100,loss=0.199059,mAp=83.1650\n",
      "epoch 11 done!\n",
      "train epoch 12 step 49,cls loss=0.022596,rec_loss=0.016961,loss=0.039557,mAp=98.8917\n",
      "train epoch 12 step 99,cls loss=0.018591,rec_loss=0.016796,loss=0.035387,mAp=98.9181\n",
      "train epoch 12 step 149,cls loss=0.018348,rec_loss=0.016565,loss=0.034914,mAp=99.0287\n",
      "train epoch 12 step 199,cls loss=0.017483,rec_loss=0.016666,loss=0.034150,mAp=98.9111\n",
      "train epoch 12 step 249,cls loss=0.017701,rec_loss=0.016598,loss=0.034299,mAp=98.8206\n",
      "train epoch 12 step 299,cls loss=0.018861,rec_loss=0.016606,loss=0.035467,mAp=98.3983\n",
      "train epoch 12 step 349,cls loss=0.018404,rec_loss=0.016513,loss=0.034916,mAp=98.4414\n",
      "train epoch 12 step 399,cls loss=0.018799,rec_loss=0.016528,loss=0.035327,mAp=98.4192\n",
      "train epoch 12 step 449,cls loss=0.017865,rec_loss=0.016566,loss=0.034431,mAp=98.5393\n",
      "train epoch 12 step 499,cls loss=0.017080,rec_loss=0.016479,loss=0.033559,mAp=98.6229\n",
      "train epoch 12 step 549,cls loss=0.016583,rec_loss=0.016424,loss=0.033008,mAp=98.6685\n",
      "train epoch 12 step 599,cls loss=0.015642,rec_loss=0.016322,loss=0.031964,mAp=98.7586\n",
      "train epoch 12 step 649,cls loss=0.015444,rec_loss=0.016245,loss=0.031689,mAp=98.7835\n",
      "train epoch 12 step 699,cls loss=0.015353,rec_loss=0.016164,loss=0.031516,mAp=98.8329\n",
      "train epoch 12 step 749,cls loss=0.015252,rec_loss=0.016126,loss=0.031378,mAp=98.8518\n",
      "train epoch 12 step 799,cls loss=0.015881,rec_loss=0.016106,loss=0.031988,mAp=98.7587\n",
      "train epoch 12 step 849,cls loss=0.016650,rec_loss=0.016118,loss=0.032768,mAp=98.7238\n",
      "train epoch 12 step 899,cls loss=0.017047,rec_loss=0.016160,loss=0.033207,mAp=98.7021\n",
      "train epoch 12 step 949,cls loss=0.017486,rec_loss=0.016173,loss=0.033658,mAp=98.6389\n",
      "train epoch 12 step 999,cls loss=0.017554,rec_loss=0.016230,loss=0.033784,mAp=98.6757\n",
      "train epoch 12 step 1049,cls loss=0.017720,rec_loss=0.016218,loss=0.033938,mAp=98.6822\n",
      "train epoch 12 step 1099,cls loss=0.018248,rec_loss=0.016188,loss=0.034436,mAp=98.6955\n",
      "train epoch 12 step 1149,cls loss=0.017792,rec_loss=0.016150,loss=0.033942,mAp=98.7377\n",
      "train epoch 12 step 1199,cls loss=0.017868,rec_loss=0.016163,loss=0.034032,mAp=98.7550\n",
      "train epoch 12 step 1249,cls loss=0.017678,rec_loss=0.016185,loss=0.033862,mAp=98.7548\n",
      "train epoch 12 step 1299,cls loss=0.017723,rec_loss=0.016210,loss=0.033933,mAp=98.7616\n",
      "train epoch 12 step 1349,cls loss=0.017489,rec_loss=0.016191,loss=0.033680,mAp=98.7572\n",
      "train epoch 12 step 1399,cls loss=0.017287,rec_loss=0.016138,loss=0.033426,mAp=98.8016\n",
      "train epoch 12 step 1449,cls loss=0.016969,rec_loss=0.016124,loss=0.033092,mAp=98.8386\n",
      "train epoch 12 step 1499,cls loss=0.016860,rec_loss=0.016121,loss=0.032981,mAp=98.8672\n",
      "train epoch 12 step 1549,cls loss=0.017299,rec_loss=0.016147,loss=0.033446,mAp=98.8625\n",
      "train epoch 12 step 1599,cls loss=0.017087,rec_loss=0.016146,loss=0.033233,mAp=98.8759\n",
      "train epoch 12 step 1649,cls loss=0.017148,rec_loss=0.016138,loss=0.033286,mAp=98.8923\n",
      "train epoch 12 step 1699,cls loss=0.017174,rec_loss=0.016150,loss=0.033324,mAp=98.8948\n",
      "train epoch 12 step 1749,cls loss=0.017305,rec_loss=0.016135,loss=0.033439,mAp=98.8775\n",
      "train epoch 12 step 1799,cls loss=0.017359,rec_loss=0.016117,loss=0.033476,mAp=98.8377\n",
      "train epoch 12 step 1849,cls loss=0.017662,rec_loss=0.016160,loss=0.033822,mAp=98.7986\n",
      "train epoch 12 step 1899,cls loss=0.017735,rec_loss=0.016161,loss=0.033896,mAp=98.7875\n",
      "train epoch 12 step 1949,cls loss=0.017786,rec_loss=0.016174,loss=0.033960,mAp=98.7833\n",
      "train epoch 12 step 1999,cls loss=0.017694,rec_loss=0.016169,loss=0.033863,mAp=98.8002\n",
      "train epoch 12 step 2049,cls loss=0.017590,rec_loss=0.016165,loss=0.033755,mAp=98.8071\n",
      "train epoch 12 step 2099,cls loss=0.018038,rec_loss=0.016192,loss=0.034230,mAp=98.7393\n",
      "train epoch 12 step 2149,cls loss=0.018155,rec_loss=0.016241,loss=0.034396,mAp=98.7434\n",
      "train epoch 12 step 2199,cls loss=0.018041,rec_loss=0.016254,loss=0.034294,mAp=98.7492\n",
      "train epoch 12 step 2249,cls loss=0.017915,rec_loss=0.016260,loss=0.034175,mAp=98.7526\n",
      "train epoch 12 step 2299,cls loss=0.017840,rec_loss=0.016254,loss=0.034094,mAp=98.7520\n",
      "train epoch 12 step 2349,cls loss=0.017760,rec_loss=0.016264,loss=0.034024,mAp=98.7670\n",
      "train epoch 12 step 2399,cls loss=0.017718,rec_loss=0.016254,loss=0.033973,mAp=98.7760\n",
      "train epoch 12 step 2449,cls loss=0.017705,rec_loss=0.016257,loss=0.033962,mAp=98.7746\n",
      "train epoch 12 step 2499,cls loss=0.017646,rec_loss=0.016262,loss=0.033909,mAp=98.7820\n",
      "train epoch 12 step 2549,cls loss=0.017501,rec_loss=0.016246,loss=0.033747,mAp=98.8034\n",
      "train epoch 12 step 2599,cls loss=0.017409,rec_loss=0.016251,loss=0.033660,mAp=98.7984\n",
      "val epoch 12 step 19,cls loss=0.226468,rec_loss=0.018781,loss=0.245249,mAp=86.0492\n",
      "val epoch 12 step 39,cls loss=0.211922,rec_loss=0.018399,loss=0.230321,mAp=82.8306\n",
      "val epoch 12 step 59,cls loss=0.231835,rec_loss=0.018005,loss=0.249839,mAp=80.3212\n",
      "val epoch 12 step 79,cls loss=0.229818,rec_loss=0.018109,loss=0.247928,mAp=80.9468\n",
      "val epoch 12 step 99,cls loss=0.206639,rec_loss=0.017717,loss=0.224356,mAp=82.5485\n",
      "val epoch 12 step 119,cls loss=0.189885,rec_loss=0.017241,loss=0.207126,mAp=83.3152\n",
      "val epoch 12 step 139,cls loss=0.186533,rec_loss=0.017015,loss=0.203548,mAp=82.5118\n",
      "val epoch 12 step 159,cls loss=0.182407,rec_loss=0.016730,loss=0.199137,mAp=82.5252\n",
      "val epoch 12 step 179,cls loss=0.180050,rec_loss=0.016540,loss=0.196590,mAp=82.8162\n",
      "val epoch 12 step 199,cls loss=0.178722,rec_loss=0.016409,loss=0.195132,mAp=82.9006\n",
      "val epoch 12 step 219,cls loss=0.177405,rec_loss=0.016477,loss=0.193883,mAp=83.0850\n",
      "val epoch 12 step 239,cls loss=0.174490,rec_loss=0.016415,loss=0.190905,mAp=83.2457\n",
      "val epoch 12 step 259,cls loss=0.174824,rec_loss=0.016422,loss=0.191246,mAp=83.3406\n",
      "val epoch 12 step 279,cls loss=0.179497,rec_loss=0.016457,loss=0.195954,mAp=83.4134\n",
      "val epoch 12 step 299,cls loss=0.181711,rec_loss=0.016352,loss=0.198062,mAp=83.3310\n",
      "val epoch 12 step 319,cls loss=0.186807,rec_loss=0.016478,loss=0.203285,mAp=82.9557\n",
      "val epoch 12 step 339,cls loss=0.189324,rec_loss=0.016443,loss=0.205768,mAp=82.7903\n",
      "val epoch 12 step 359,cls loss=0.195992,rec_loss=0.016391,loss=0.212383,mAp=82.4845\n",
      "epoch 12 done!\n",
      "train epoch 13 step 49,cls loss=0.008577,rec_loss=0.015493,loss=0.024070,mAp=99.7500\n",
      "train epoch 13 step 99,cls loss=0.008340,rec_loss=0.015191,loss=0.023531,mAp=99.5417\n",
      "train epoch 13 step 149,cls loss=0.008319,rec_loss=0.015086,loss=0.023405,mAp=99.6111\n",
      "train epoch 13 step 199,cls loss=0.009144,rec_loss=0.014971,loss=0.024114,mAp=99.6250\n",
      "train epoch 13 step 249,cls loss=0.010915,rec_loss=0.015252,loss=0.026168,mAp=99.0000\n",
      "train epoch 13 step 299,cls loss=0.012664,rec_loss=0.015437,loss=0.028101,mAp=99.0764\n",
      "train epoch 13 step 349,cls loss=0.012581,rec_loss=0.015536,loss=0.028117,mAp=99.1298\n",
      "train epoch 13 step 399,cls loss=0.011652,rec_loss=0.015535,loss=0.027187,mAp=99.2073\n",
      "train epoch 13 step 449,cls loss=0.011637,rec_loss=0.015470,loss=0.027107,mAp=99.2074\n",
      "train epoch 13 step 499,cls loss=0.011983,rec_loss=0.015616,loss=0.027599,mAp=99.1117\n",
      "train epoch 13 step 549,cls loss=0.012119,rec_loss=0.015578,loss=0.027697,mAp=99.0864\n",
      "train epoch 13 step 599,cls loss=0.012734,rec_loss=0.015604,loss=0.028338,mAp=99.0583\n",
      "train epoch 13 step 649,cls loss=0.012927,rec_loss=0.015663,loss=0.028589,mAp=99.1083\n",
      "train epoch 13 step 699,cls loss=0.013416,rec_loss=0.015664,loss=0.029080,mAp=99.1036\n",
      "train epoch 13 step 749,cls loss=0.013595,rec_loss=0.015645,loss=0.029240,mAp=99.1272\n",
      "train epoch 13 step 799,cls loss=0.013564,rec_loss=0.015688,loss=0.029252,mAp=99.1740\n",
      "train epoch 13 step 849,cls loss=0.014027,rec_loss=0.015670,loss=0.029697,mAp=99.1354\n",
      "train epoch 13 step 899,cls loss=0.014095,rec_loss=0.015665,loss=0.029760,mAp=99.1047\n",
      "train epoch 13 step 949,cls loss=0.013751,rec_loss=0.015716,loss=0.029467,mAp=99.1160\n",
      "train epoch 13 step 999,cls loss=0.013743,rec_loss=0.015739,loss=0.029482,mAp=99.0748\n",
      "train epoch 13 step 1049,cls loss=0.014045,rec_loss=0.015808,loss=0.029853,mAp=99.0543\n",
      "train epoch 13 step 1099,cls loss=0.013940,rec_loss=0.015802,loss=0.029742,mAp=99.0897\n",
      "train epoch 13 step 1149,cls loss=0.013767,rec_loss=0.015834,loss=0.029601,mAp=99.1112\n",
      "train epoch 13 step 1199,cls loss=0.013778,rec_loss=0.015809,loss=0.029586,mAp=99.1310\n",
      "train epoch 13 step 1249,cls loss=0.013622,rec_loss=0.015837,loss=0.029459,mAp=99.1474\n",
      "train epoch 13 step 1299,cls loss=0.013641,rec_loss=0.015841,loss=0.029482,mAp=99.1482\n",
      "train epoch 13 step 1349,cls loss=0.013914,rec_loss=0.015863,loss=0.029777,mAp=99.0454\n",
      "train epoch 13 step 1399,cls loss=0.013784,rec_loss=0.015878,loss=0.029661,mAp=99.0676\n",
      "train epoch 13 step 1449,cls loss=0.014012,rec_loss=0.015904,loss=0.029916,mAp=98.9935\n",
      "train epoch 13 step 1499,cls loss=0.013923,rec_loss=0.015976,loss=0.029899,mAp=98.9978\n",
      "train epoch 13 step 1549,cls loss=0.013900,rec_loss=0.016004,loss=0.029905,mAp=98.9993\n",
      "train epoch 13 step 1599,cls loss=0.013721,rec_loss=0.015990,loss=0.029711,mAp=99.0279\n",
      "train epoch 13 step 1649,cls loss=0.013862,rec_loss=0.015999,loss=0.029861,mAp=99.0258\n",
      "train epoch 13 step 1699,cls loss=0.014016,rec_loss=0.015991,loss=0.030007,mAp=99.0066\n",
      "train epoch 13 step 1749,cls loss=0.014124,rec_loss=0.015976,loss=0.030100,mAp=99.0147\n",
      "train epoch 13 step 1799,cls loss=0.014275,rec_loss=0.016000,loss=0.030275,mAp=98.9856\n",
      "train epoch 13 step 1849,cls loss=0.014250,rec_loss=0.015987,loss=0.030237,mAp=98.9894\n",
      "train epoch 13 step 1899,cls loss=0.014267,rec_loss=0.015984,loss=0.030252,mAp=99.0094\n",
      "train epoch 13 step 1949,cls loss=0.014345,rec_loss=0.015982,loss=0.030328,mAp=98.9862\n",
      "train epoch 13 step 1999,cls loss=0.014510,rec_loss=0.015967,loss=0.030477,mAp=98.9925\n",
      "train epoch 13 step 2049,cls loss=0.014457,rec_loss=0.015955,loss=0.030412,mAp=98.9924\n",
      "train epoch 13 step 2099,cls loss=0.014596,rec_loss=0.015960,loss=0.030556,mAp=98.9807\n",
      "train epoch 13 step 2149,cls loss=0.014548,rec_loss=0.015958,loss=0.030506,mAp=98.9637\n",
      "train epoch 13 step 2199,cls loss=0.014716,rec_loss=0.015951,loss=0.030666,mAp=98.9655\n",
      "train epoch 13 step 2249,cls loss=0.014977,rec_loss=0.016001,loss=0.030978,mAp=98.9348\n",
      "train epoch 13 step 2299,cls loss=0.015359,rec_loss=0.016021,loss=0.031379,mAp=98.8978\n",
      "train epoch 13 step 2349,cls loss=0.015400,rec_loss=0.016034,loss=0.031434,mAp=98.9044\n",
      "train epoch 13 step 2399,cls loss=0.015423,rec_loss=0.016051,loss=0.031474,mAp=98.9180\n",
      "train epoch 13 step 2449,cls loss=0.015483,rec_loss=0.016064,loss=0.031547,mAp=98.8959\n",
      "train epoch 13 step 2499,cls loss=0.015337,rec_loss=0.016062,loss=0.031399,mAp=98.9063\n",
      "train epoch 13 step 2549,cls loss=0.015338,rec_loss=0.016052,loss=0.031390,mAp=98.8918\n",
      "train epoch 13 step 2599,cls loss=0.015360,rec_loss=0.016039,loss=0.031399,mAp=98.8907\n",
      "val epoch 13 step 19,cls loss=0.242957,rec_loss=0.020434,loss=0.263391,mAp=84.4956\n",
      "val epoch 13 step 39,cls loss=0.222295,rec_loss=0.020057,loss=0.242352,mAp=82.6544\n",
      "val epoch 13 step 59,cls loss=0.224574,rec_loss=0.019496,loss=0.244070,mAp=81.8527\n",
      "val epoch 13 step 79,cls loss=0.217349,rec_loss=0.019599,loss=0.236948,mAp=82.1225\n",
      "val epoch 13 step 99,cls loss=0.202171,rec_loss=0.019320,loss=0.221491,mAp=82.2408\n",
      "val epoch 13 step 119,cls loss=0.191140,rec_loss=0.018844,loss=0.209984,mAp=83.4096\n",
      "val epoch 13 step 139,cls loss=0.185024,rec_loss=0.018604,loss=0.203628,mAp=82.6000\n",
      "val epoch 13 step 159,cls loss=0.182247,rec_loss=0.018328,loss=0.200574,mAp=82.6885\n",
      "val epoch 13 step 179,cls loss=0.177823,rec_loss=0.018114,loss=0.195937,mAp=82.7359\n",
      "val epoch 13 step 199,cls loss=0.176665,rec_loss=0.017956,loss=0.194621,mAp=82.5267\n",
      "val epoch 13 step 219,cls loss=0.181687,rec_loss=0.018034,loss=0.199721,mAp=82.5493\n",
      "val epoch 13 step 239,cls loss=0.180055,rec_loss=0.017973,loss=0.198028,mAp=82.9714\n",
      "val epoch 13 step 259,cls loss=0.180654,rec_loss=0.017960,loss=0.198615,mAp=83.0928\n",
      "val epoch 13 step 279,cls loss=0.185026,rec_loss=0.017973,loss=0.203000,mAp=83.0959\n",
      "val epoch 13 step 299,cls loss=0.187849,rec_loss=0.017870,loss=0.205719,mAp=83.3067\n",
      "val epoch 13 step 319,cls loss=0.190688,rec_loss=0.018007,loss=0.208695,mAp=82.9557\n",
      "val epoch 13 step 339,cls loss=0.193611,rec_loss=0.017996,loss=0.211607,mAp=82.7660\n",
      "val epoch 13 step 359,cls loss=0.198500,rec_loss=0.017911,loss=0.216411,mAp=82.4911\n",
      "epoch 13 done!\n",
      "train epoch 14 step 49,cls loss=0.019610,rec_loss=0.017064,loss=0.036674,mAp=98.4583\n",
      "train epoch 14 step 99,cls loss=0.014040,rec_loss=0.016842,loss=0.030882,mAp=99.1458\n",
      "train epoch 14 step 149,cls loss=0.011414,rec_loss=0.016248,loss=0.027662,mAp=99.4306\n",
      "train epoch 14 step 199,cls loss=0.011019,rec_loss=0.016124,loss=0.027143,mAp=99.5104\n",
      "train epoch 14 step 249,cls loss=0.010933,rec_loss=0.015797,loss=0.026731,mAp=99.4867\n",
      "train epoch 14 step 299,cls loss=0.011240,rec_loss=0.015609,loss=0.026848,mAp=99.4333\n",
      "train epoch 14 step 349,cls loss=0.011792,rec_loss=0.015552,loss=0.027344,mAp=99.4190\n",
      "train epoch 14 step 399,cls loss=0.011554,rec_loss=0.015592,loss=0.027147,mAp=99.3875\n",
      "train epoch 14 step 449,cls loss=0.012221,rec_loss=0.015574,loss=0.027794,mAp=99.2704\n",
      "train epoch 14 step 499,cls loss=0.012457,rec_loss=0.015521,loss=0.027979,mAp=99.2808\n",
      "train epoch 14 step 549,cls loss=0.012617,rec_loss=0.015531,loss=0.028148,mAp=99.2439\n",
      "train epoch 14 step 599,cls loss=0.011952,rec_loss=0.015512,loss=0.027464,mAp=99.2757\n",
      "train epoch 14 step 649,cls loss=0.011733,rec_loss=0.015360,loss=0.027093,mAp=99.2962\n",
      "train epoch 14 step 699,cls loss=0.011706,rec_loss=0.015347,loss=0.027053,mAp=99.2628\n",
      "train epoch 14 step 749,cls loss=0.011736,rec_loss=0.015285,loss=0.027021,mAp=99.2897\n",
      "train epoch 14 step 799,cls loss=0.011383,rec_loss=0.015290,loss=0.026673,mAp=99.3341\n",
      "train epoch 14 step 849,cls loss=0.011106,rec_loss=0.015286,loss=0.026392,mAp=99.3733\n",
      "train epoch 14 step 899,cls loss=0.011187,rec_loss=0.015231,loss=0.026418,mAp=99.3248\n",
      "train epoch 14 step 949,cls loss=0.011877,rec_loss=0.015268,loss=0.027146,mAp=99.2550\n",
      "train epoch 14 step 999,cls loss=0.012209,rec_loss=0.015301,loss=0.027509,mAp=99.2493\n",
      "train epoch 14 step 1049,cls loss=0.012775,rec_loss=0.015413,loss=0.028188,mAp=99.1906\n",
      "train epoch 14 step 1099,cls loss=0.012766,rec_loss=0.015422,loss=0.028189,mAp=99.1833\n",
      "train epoch 14 step 1149,cls loss=0.012591,rec_loss=0.015453,loss=0.028044,mAp=99.1934\n",
      "train epoch 14 step 1199,cls loss=0.012545,rec_loss=0.015488,loss=0.028033,mAp=99.2149\n",
      "train epoch 14 step 1249,cls loss=0.012977,rec_loss=0.015487,loss=0.028464,mAp=99.1750\n",
      "train epoch 14 step 1299,cls loss=0.013568,rec_loss=0.015493,loss=0.029061,mAp=99.0918\n",
      "train epoch 14 step 1349,cls loss=0.013512,rec_loss=0.015509,loss=0.029021,mAp=99.1100\n",
      "train epoch 14 step 1399,cls loss=0.013526,rec_loss=0.015498,loss=0.029024,mAp=99.0957\n",
      "train epoch 14 step 1449,cls loss=0.013308,rec_loss=0.015475,loss=0.028783,mAp=99.1182\n",
      "train epoch 14 step 1499,cls loss=0.013388,rec_loss=0.015500,loss=0.028888,mAp=99.1190\n",
      "train epoch 14 step 1549,cls loss=0.013621,rec_loss=0.015520,loss=0.029140,mAp=99.0923\n",
      "train epoch 14 step 1599,cls loss=0.014126,rec_loss=0.015588,loss=0.029714,mAp=99.0344\n",
      "train epoch 14 step 1649,cls loss=0.014073,rec_loss=0.015584,loss=0.029657,mAp=99.0435\n",
      "train epoch 14 step 1699,cls loss=0.013948,rec_loss=0.015592,loss=0.029539,mAp=99.0521\n",
      "train epoch 14 step 1749,cls loss=0.014011,rec_loss=0.015605,loss=0.029616,mAp=99.0446\n",
      "train epoch 14 step 1799,cls loss=0.014171,rec_loss=0.015625,loss=0.029795,mAp=99.0230\n",
      "train epoch 14 step 1849,cls loss=0.014446,rec_loss=0.015639,loss=0.030085,mAp=98.9834\n",
      "train epoch 14 step 1899,cls loss=0.014392,rec_loss=0.015616,loss=0.030008,mAp=98.9992\n",
      "train epoch 14 step 1949,cls loss=0.014398,rec_loss=0.015625,loss=0.030023,mAp=98.9928\n",
      "train epoch 14 step 1999,cls loss=0.014205,rec_loss=0.015647,loss=0.029852,mAp=99.0118\n",
      "train epoch 14 step 2049,cls loss=0.014356,rec_loss=0.015646,loss=0.030002,mAp=99.0105\n",
      "train epoch 14 step 2099,cls loss=0.014507,rec_loss=0.015654,loss=0.030161,mAp=98.9973\n",
      "train epoch 14 step 2149,cls loss=0.014495,rec_loss=0.015679,loss=0.030173,mAp=99.0055\n",
      "train epoch 14 step 2199,cls loss=0.014567,rec_loss=0.015664,loss=0.030231,mAp=99.0001\n",
      "train epoch 14 step 2249,cls loss=0.014640,rec_loss=0.015665,loss=0.030306,mAp=98.9973\n",
      "train epoch 14 step 2299,cls loss=0.014777,rec_loss=0.015672,loss=0.030450,mAp=98.9856\n",
      "train epoch 14 step 2349,cls loss=0.014753,rec_loss=0.015679,loss=0.030431,mAp=98.9815\n",
      "train epoch 14 step 2399,cls loss=0.014692,rec_loss=0.015695,loss=0.030388,mAp=98.9775\n",
      "train epoch 14 step 2449,cls loss=0.014914,rec_loss=0.015695,loss=0.030609,mAp=98.9545\n",
      "train epoch 14 step 2499,cls loss=0.014988,rec_loss=0.015686,loss=0.030674,mAp=98.9491\n",
      "train epoch 14 step 2549,cls loss=0.015035,rec_loss=0.015702,loss=0.030737,mAp=98.9529\n",
      "train epoch 14 step 2599,cls loss=0.015178,rec_loss=0.015687,loss=0.030866,mAp=98.9562\n",
      "val epoch 14 step 19,cls loss=0.252445,rec_loss=0.018732,loss=0.271177,mAp=81.5239\n",
      "val epoch 14 step 39,cls loss=0.224017,rec_loss=0.018378,loss=0.242395,mAp=80.9474\n",
      "val epoch 14 step 59,cls loss=0.215595,rec_loss=0.017998,loss=0.233593,mAp=81.0500\n",
      "val epoch 14 step 79,cls loss=0.218465,rec_loss=0.018060,loss=0.236525,mAp=80.9611\n",
      "val epoch 14 step 99,cls loss=0.197368,rec_loss=0.017666,loss=0.215035,mAp=82.3216\n",
      "val epoch 14 step 119,cls loss=0.182786,rec_loss=0.017223,loss=0.200009,mAp=83.2919\n",
      "val epoch 14 step 139,cls loss=0.173626,rec_loss=0.016981,loss=0.190607,mAp=83.5795\n",
      "val epoch 14 step 159,cls loss=0.172645,rec_loss=0.016733,loss=0.189378,mAp=83.4384\n",
      "val epoch 14 step 179,cls loss=0.168129,rec_loss=0.016568,loss=0.184697,mAp=83.9425\n",
      "val epoch 14 step 199,cls loss=0.170639,rec_loss=0.016459,loss=0.187097,mAp=83.7765\n",
      "val epoch 14 step 219,cls loss=0.171411,rec_loss=0.016555,loss=0.187965,mAp=83.9204\n",
      "val epoch 14 step 239,cls loss=0.169650,rec_loss=0.016508,loss=0.186158,mAp=84.0510\n",
      "val epoch 14 step 259,cls loss=0.168588,rec_loss=0.016524,loss=0.185112,mAp=84.2659\n",
      "val epoch 14 step 279,cls loss=0.172278,rec_loss=0.016538,loss=0.188815,mAp=84.4311\n",
      "val epoch 14 step 299,cls loss=0.175783,rec_loss=0.016470,loss=0.192252,mAp=84.3919\n",
      "val epoch 14 step 319,cls loss=0.181410,rec_loss=0.016603,loss=0.198013,mAp=84.0294\n",
      "val epoch 14 step 339,cls loss=0.184353,rec_loss=0.016577,loss=0.200930,mAp=83.9344\n",
      "val epoch 14 step 359,cls loss=0.190889,rec_loss=0.016524,loss=0.207412,mAp=83.5228\n",
      "epoch 14 done!\n",
      "train epoch 15 step 49,cls loss=0.009135,rec_loss=0.015821,loss=0.024956,mAp=100.0000\n",
      "train epoch 15 step 99,cls loss=0.009746,rec_loss=0.015464,loss=0.025210,mAp=99.8750\n",
      "train epoch 15 step 149,cls loss=0.009691,rec_loss=0.015169,loss=0.024859,mAp=99.7500\n",
      "train epoch 15 step 199,cls loss=0.010476,rec_loss=0.015326,loss=0.025802,mAp=99.6250\n",
      "train epoch 15 step 249,cls loss=0.014491,rec_loss=0.015531,loss=0.030021,mAp=99.2617\n",
      "train epoch 15 step 299,cls loss=0.016013,rec_loss=0.015682,loss=0.031694,mAp=98.8759\n",
      "train epoch 15 step 349,cls loss=0.016136,rec_loss=0.015710,loss=0.031846,mAp=98.8460\n",
      "train epoch 15 step 399,cls loss=0.017752,rec_loss=0.015713,loss=0.033465,mAp=98.7684\n",
      "train epoch 15 step 449,cls loss=0.017339,rec_loss=0.015657,loss=0.032996,mAp=98.7034\n",
      "train epoch 15 step 499,cls loss=0.016729,rec_loss=0.015653,loss=0.032382,mAp=98.7747\n",
      "train epoch 15 step 549,cls loss=0.016327,rec_loss=0.015582,loss=0.031910,mAp=98.7990\n",
      "train epoch 15 step 599,cls loss=0.015841,rec_loss=0.015507,loss=0.031348,mAp=98.8241\n",
      "train epoch 15 step 649,cls loss=0.015506,rec_loss=0.015433,loss=0.030940,mAp=98.8600\n",
      "train epoch 15 step 699,cls loss=0.015141,rec_loss=0.015436,loss=0.030577,mAp=98.8700\n",
      "train epoch 15 step 749,cls loss=0.014301,rec_loss=0.015386,loss=0.029687,mAp=98.9454\n",
      "train epoch 15 step 799,cls loss=0.013910,rec_loss=0.015359,loss=0.029269,mAp=98.9655\n",
      "train epoch 15 step 849,cls loss=0.013512,rec_loss=0.015364,loss=0.028876,mAp=98.9920\n",
      "train epoch 15 step 899,cls loss=0.013336,rec_loss=0.015325,loss=0.028661,mAp=99.0156\n",
      "train epoch 15 step 949,cls loss=0.013453,rec_loss=0.015309,loss=0.028762,mAp=99.0367\n",
      "train epoch 15 step 999,cls loss=0.013700,rec_loss=0.015335,loss=0.029035,mAp=99.0300\n",
      "train epoch 15 step 1049,cls loss=0.013360,rec_loss=0.015330,loss=0.028690,mAp=99.0722\n",
      "train epoch 15 step 1099,cls loss=0.013168,rec_loss=0.015343,loss=0.028511,mAp=99.1106\n",
      "train epoch 15 step 1149,cls loss=0.012902,rec_loss=0.015359,loss=0.028261,mAp=99.1340\n",
      "train epoch 15 step 1199,cls loss=0.013057,rec_loss=0.015369,loss=0.028427,mAp=99.1354\n",
      "train epoch 15 step 1249,cls loss=0.013028,rec_loss=0.015375,loss=0.028403,mAp=99.1366\n",
      "train epoch 15 step 1299,cls loss=0.012828,rec_loss=0.015414,loss=0.028243,mAp=99.1682\n",
      "train epoch 15 step 1349,cls loss=0.012635,rec_loss=0.015377,loss=0.028012,mAp=99.1805\n",
      "train epoch 15 step 1399,cls loss=0.012583,rec_loss=0.015433,loss=0.028016,mAp=99.1875\n",
      "train epoch 15 step 1449,cls loss=0.012753,rec_loss=0.015480,loss=0.028232,mAp=99.1638\n",
      "train epoch 15 step 1499,cls loss=0.012788,rec_loss=0.015507,loss=0.028294,mAp=99.1611\n",
      "train epoch 15 step 1549,cls loss=0.012805,rec_loss=0.015492,loss=0.028297,mAp=99.1640\n",
      "train epoch 15 step 1599,cls loss=0.013387,rec_loss=0.015518,loss=0.028905,mAp=99.1317\n",
      "train epoch 15 step 1649,cls loss=0.013973,rec_loss=0.015573,loss=0.029546,mAp=99.0281\n",
      "train epoch 15 step 1699,cls loss=0.014311,rec_loss=0.015633,loss=0.029944,mAp=98.9857\n",
      "train epoch 15 step 1749,cls loss=0.014352,rec_loss=0.015668,loss=0.030020,mAp=98.9560\n",
      "train epoch 15 step 1799,cls loss=0.014399,rec_loss=0.015656,loss=0.030055,mAp=98.9533\n",
      "train epoch 15 step 1849,cls loss=0.014510,rec_loss=0.015698,loss=0.030208,mAp=98.9321\n",
      "train epoch 15 step 1899,cls loss=0.014307,rec_loss=0.015699,loss=0.030006,mAp=98.9515\n",
      "train epoch 15 step 1949,cls loss=0.014156,rec_loss=0.015677,loss=0.029833,mAp=98.9677\n",
      "train epoch 15 step 1999,cls loss=0.014242,rec_loss=0.015662,loss=0.029904,mAp=98.9674\n",
      "train epoch 15 step 2049,cls loss=0.014397,rec_loss=0.015630,loss=0.030027,mAp=98.9456\n",
      "train epoch 15 step 2099,cls loss=0.014570,rec_loss=0.015641,loss=0.030211,mAp=98.9404\n",
      "train epoch 15 step 2149,cls loss=0.014548,rec_loss=0.015638,loss=0.030186,mAp=98.9457\n",
      "train epoch 15 step 2199,cls loss=0.014506,rec_loss=0.015631,loss=0.030136,mAp=98.9549\n",
      "train epoch 15 step 2249,cls loss=0.014379,rec_loss=0.015626,loss=0.030005,mAp=98.9707\n",
      "train epoch 15 step 2299,cls loss=0.014283,rec_loss=0.015605,loss=0.029887,mAp=98.9885\n",
      "train epoch 15 step 2349,cls loss=0.014116,rec_loss=0.015597,loss=0.029713,mAp=99.0048\n",
      "train epoch 15 step 2399,cls loss=0.013889,rec_loss=0.015579,loss=0.029469,mAp=99.0255\n",
      "train epoch 15 step 2449,cls loss=0.013924,rec_loss=0.015574,loss=0.029498,mAp=99.0239\n",
      "train epoch 15 step 2499,cls loss=0.013983,rec_loss=0.015559,loss=0.029542,mAp=99.0142\n",
      "train epoch 15 step 2549,cls loss=0.013947,rec_loss=0.015562,loss=0.029508,mAp=99.0072\n",
      "train epoch 15 step 2599,cls loss=0.013861,rec_loss=0.015549,loss=0.029410,mAp=99.0151\n",
      "val epoch 15 step 19,cls loss=0.257735,rec_loss=0.018232,loss=0.275966,mAp=81.1022\n",
      "val epoch 15 step 39,cls loss=0.223488,rec_loss=0.017898,loss=0.241386,mAp=81.9475\n",
      "val epoch 15 step 59,cls loss=0.228932,rec_loss=0.017523,loss=0.246455,mAp=80.9685\n",
      "val epoch 15 step 79,cls loss=0.243882,rec_loss=0.017536,loss=0.261418,mAp=80.2725\n",
      "val epoch 15 step 99,cls loss=0.223163,rec_loss=0.017204,loss=0.240367,mAp=80.8367\n",
      "val epoch 15 step 119,cls loss=0.206640,rec_loss=0.016704,loss=0.223344,mAp=81.6347\n",
      "val epoch 15 step 139,cls loss=0.199495,rec_loss=0.016491,loss=0.215986,mAp=81.5129\n",
      "val epoch 15 step 159,cls loss=0.195549,rec_loss=0.016281,loss=0.211830,mAp=81.8301\n",
      "val epoch 15 step 179,cls loss=0.186980,rec_loss=0.016112,loss=0.203092,mAp=82.3272\n",
      "val epoch 15 step 199,cls loss=0.186816,rec_loss=0.015952,loss=0.202768,mAp=82.1421\n",
      "val epoch 15 step 219,cls loss=0.190335,rec_loss=0.016009,loss=0.206344,mAp=82.2636\n",
      "val epoch 15 step 239,cls loss=0.188364,rec_loss=0.015949,loss=0.204313,mAp=82.2146\n",
      "val epoch 15 step 259,cls loss=0.188935,rec_loss=0.015948,loss=0.204882,mAp=82.3162\n",
      "val epoch 15 step 279,cls loss=0.193767,rec_loss=0.015963,loss=0.209731,mAp=82.3329\n",
      "val epoch 15 step 299,cls loss=0.193666,rec_loss=0.015876,loss=0.209542,mAp=82.3350\n",
      "val epoch 15 step 319,cls loss=0.197941,rec_loss=0.016010,loss=0.213951,mAp=81.8546\n",
      "val epoch 15 step 339,cls loss=0.200731,rec_loss=0.015987,loss=0.216718,mAp=81.8144\n",
      "val epoch 15 step 359,cls loss=0.207078,rec_loss=0.015932,loss=0.223010,mAp=81.5194\n",
      "epoch 15 done!\n",
      "train epoch 16 step 49,cls loss=0.008794,rec_loss=0.015111,loss=0.023905,mAp=99.4167\n",
      "train epoch 16 step 99,cls loss=0.009240,rec_loss=0.014949,loss=0.024188,mAp=98.8750\n",
      "train epoch 16 step 149,cls loss=0.014223,rec_loss=0.015023,loss=0.029246,mAp=98.6806\n",
      "train epoch 16 step 199,cls loss=0.016135,rec_loss=0.015044,loss=0.031180,mAp=98.3403\n",
      "train epoch 16 step 249,cls loss=0.017734,rec_loss=0.015211,loss=0.032944,mAp=98.4861\n",
      "train epoch 16 step 299,cls loss=0.016997,rec_loss=0.015220,loss=0.032217,mAp=98.4954\n",
      "train epoch 16 step 349,cls loss=0.016007,rec_loss=0.015228,loss=0.031235,mAp=98.5952\n",
      "train epoch 16 step 399,cls loss=0.015737,rec_loss=0.015117,loss=0.030854,mAp=98.6979\n",
      "train epoch 16 step 449,cls loss=0.015004,rec_loss=0.015221,loss=0.030225,mAp=98.7315\n",
      "train epoch 16 step 499,cls loss=0.015151,rec_loss=0.015300,loss=0.030451,mAp=98.7042\n",
      "train epoch 16 step 549,cls loss=0.014790,rec_loss=0.015325,loss=0.030116,mAp=98.7197\n",
      "train epoch 16 step 599,cls loss=0.014229,rec_loss=0.015284,loss=0.029513,mAp=98.7951\n",
      "train epoch 16 step 649,cls loss=0.014707,rec_loss=0.015266,loss=0.029972,mAp=98.7731\n",
      "train epoch 16 step 699,cls loss=0.014414,rec_loss=0.015253,loss=0.029667,mAp=98.7833\n",
      "train epoch 16 step 749,cls loss=0.013901,rec_loss=0.015206,loss=0.029108,mAp=98.8367\n",
      "train epoch 16 step 799,cls loss=0.013314,rec_loss=0.015141,loss=0.028456,mAp=98.9094\n",
      "train epoch 16 step 849,cls loss=0.013281,rec_loss=0.015116,loss=0.028397,mAp=98.9123\n",
      "train epoch 16 step 899,cls loss=0.013144,rec_loss=0.015151,loss=0.028295,mAp=98.9345\n",
      "train epoch 16 step 949,cls loss=0.012860,rec_loss=0.015145,loss=0.028004,mAp=98.9906\n",
      "train epoch 16 step 999,cls loss=0.012740,rec_loss=0.015134,loss=0.027874,mAp=99.0148\n",
      "train epoch 16 step 1049,cls loss=0.013449,rec_loss=0.015269,loss=0.028718,mAp=98.9321\n",
      "train epoch 16 step 1099,cls loss=0.013461,rec_loss=0.015295,loss=0.028756,mAp=98.9504\n",
      "train epoch 16 step 1149,cls loss=0.013574,rec_loss=0.015281,loss=0.028855,mAp=98.9127\n",
      "train epoch 16 step 1199,cls loss=0.013661,rec_loss=0.015275,loss=0.028936,mAp=98.9076\n",
      "train epoch 16 step 1249,cls loss=0.013691,rec_loss=0.015279,loss=0.028970,mAp=98.9197\n",
      "train epoch 16 step 1299,cls loss=0.013593,rec_loss=0.015241,loss=0.028834,mAp=98.9452\n",
      "train epoch 16 step 1349,cls loss=0.013784,rec_loss=0.015305,loss=0.029089,mAp=98.9621\n",
      "train epoch 16 step 1399,cls loss=0.014189,rec_loss=0.015318,loss=0.029507,mAp=98.9799\n",
      "train epoch 16 step 1449,cls loss=0.014296,rec_loss=0.015338,loss=0.029634,mAp=98.9800\n",
      "train epoch 16 step 1499,cls loss=0.014142,rec_loss=0.015351,loss=0.029492,mAp=98.9848\n",
      "train epoch 16 step 1549,cls loss=0.013940,rec_loss=0.015351,loss=0.029290,mAp=99.0095\n",
      "train epoch 16 step 1599,cls loss=0.013878,rec_loss=0.015336,loss=0.029215,mAp=99.0326\n",
      "train epoch 16 step 1649,cls loss=0.013686,rec_loss=0.015308,loss=0.028993,mAp=99.0544\n",
      "train epoch 16 step 1699,cls loss=0.013897,rec_loss=0.015321,loss=0.029218,mAp=99.0344\n",
      "train epoch 16 step 1749,cls loss=0.014202,rec_loss=0.015328,loss=0.029530,mAp=99.0001\n",
      "train epoch 16 step 1799,cls loss=0.014397,rec_loss=0.015345,loss=0.029742,mAp=98.9998\n",
      "train epoch 16 step 1849,cls loss=0.014346,rec_loss=0.015369,loss=0.029715,mAp=99.0077\n",
      "train epoch 16 step 1899,cls loss=0.014230,rec_loss=0.015360,loss=0.029590,mAp=99.0207\n",
      "train epoch 16 step 1949,cls loss=0.014338,rec_loss=0.015366,loss=0.029704,mAp=98.9924\n",
      "train epoch 16 step 1999,cls loss=0.014349,rec_loss=0.015376,loss=0.029725,mAp=98.9763\n",
      "train epoch 16 step 2049,cls loss=0.014301,rec_loss=0.015407,loss=0.029707,mAp=98.9647\n",
      "train epoch 16 step 2099,cls loss=0.014149,rec_loss=0.015394,loss=0.029543,mAp=98.9794\n",
      "train epoch 16 step 2149,cls loss=0.014049,rec_loss=0.015376,loss=0.029425,mAp=98.9828\n",
      "train epoch 16 step 2199,cls loss=0.013850,rec_loss=0.015379,loss=0.029230,mAp=99.0059\n",
      "train epoch 16 step 2249,cls loss=0.013839,rec_loss=0.015384,loss=0.029223,mAp=99.0255\n",
      "train epoch 16 step 2299,cls loss=0.013854,rec_loss=0.015361,loss=0.029215,mAp=99.0286\n",
      "train epoch 16 step 2349,cls loss=0.014073,rec_loss=0.015362,loss=0.029434,mAp=99.0114\n",
      "train epoch 16 step 2399,cls loss=0.014128,rec_loss=0.015346,loss=0.029474,mAp=99.0193\n",
      "train epoch 16 step 2449,cls loss=0.014001,rec_loss=0.015331,loss=0.029332,mAp=99.0223\n",
      "train epoch 16 step 2499,cls loss=0.013956,rec_loss=0.015316,loss=0.029271,mAp=99.0219\n",
      "train epoch 16 step 2549,cls loss=0.013970,rec_loss=0.015323,loss=0.029293,mAp=99.0231\n",
      "train epoch 16 step 2599,cls loss=0.014008,rec_loss=0.015344,loss=0.029352,mAp=99.0322\n",
      "val epoch 16 step 19,cls loss=0.239378,rec_loss=0.019117,loss=0.258495,mAp=84.5455\n",
      "val epoch 16 step 39,cls loss=0.212563,rec_loss=0.018662,loss=0.231225,mAp=82.7464\n",
      "val epoch 16 step 59,cls loss=0.208115,rec_loss=0.018455,loss=0.226570,mAp=82.6681\n",
      "val epoch 16 step 79,cls loss=0.215127,rec_loss=0.018431,loss=0.233558,mAp=82.6593\n",
      "val epoch 16 step 99,cls loss=0.197849,rec_loss=0.018168,loss=0.216018,mAp=83.3749\n",
      "val epoch 16 step 119,cls loss=0.185637,rec_loss=0.017718,loss=0.203355,mAp=83.7038\n",
      "val epoch 16 step 139,cls loss=0.177421,rec_loss=0.017543,loss=0.194964,mAp=83.5690\n",
      "val epoch 16 step 159,cls loss=0.179231,rec_loss=0.017292,loss=0.196523,mAp=83.2609\n",
      "val epoch 16 step 179,cls loss=0.173228,rec_loss=0.017120,loss=0.190348,mAp=83.5828\n",
      "val epoch 16 step 199,cls loss=0.171360,rec_loss=0.016945,loss=0.188305,mAp=83.5478\n",
      "val epoch 16 step 219,cls loss=0.174714,rec_loss=0.017041,loss=0.191755,mAp=83.4754\n",
      "val epoch 16 step 239,cls loss=0.174120,rec_loss=0.016993,loss=0.191113,mAp=83.5298\n",
      "val epoch 16 step 259,cls loss=0.175503,rec_loss=0.017005,loss=0.192508,mAp=83.4046\n",
      "val epoch 16 step 279,cls loss=0.179514,rec_loss=0.016996,loss=0.196510,mAp=83.3724\n",
      "val epoch 16 step 299,cls loss=0.183399,rec_loss=0.016907,loss=0.200306,mAp=83.2538\n",
      "val epoch 16 step 319,cls loss=0.187882,rec_loss=0.017027,loss=0.204909,mAp=83.0321\n",
      "val epoch 16 step 339,cls loss=0.190940,rec_loss=0.016975,loss=0.207914,mAp=83.1904\n",
      "val epoch 16 step 359,cls loss=0.195685,rec_loss=0.016918,loss=0.212603,mAp=82.9077\n",
      "epoch 16 done!\n",
      "train epoch 17 step 49,cls loss=0.007039,rec_loss=0.014289,loss=0.021328,mAp=99.0833\n",
      "train epoch 17 step 99,cls loss=0.006062,rec_loss=0.013901,loss=0.019963,mAp=99.5417\n",
      "train epoch 17 step 149,cls loss=0.010675,rec_loss=0.014117,loss=0.024792,mAp=99.3056\n",
      "train epoch 17 step 199,cls loss=0.011846,rec_loss=0.014232,loss=0.026078,mAp=99.1007\n",
      "train epoch 17 step 249,cls loss=0.010450,rec_loss=0.014519,loss=0.024969,mAp=99.2306\n",
      "train epoch 17 step 299,cls loss=0.009556,rec_loss=0.014550,loss=0.024106,mAp=99.3171\n",
      "train epoch 17 step 349,cls loss=0.009167,rec_loss=0.014625,loss=0.023791,mAp=99.3611\n",
      "train epoch 17 step 399,cls loss=0.009129,rec_loss=0.014606,loss=0.023735,mAp=99.3559\n",
      "train epoch 17 step 449,cls loss=0.008446,rec_loss=0.014664,loss=0.023109,mAp=99.4275\n",
      "train epoch 17 step 499,cls loss=0.008402,rec_loss=0.014639,loss=0.023040,mAp=99.4431\n",
      "train epoch 17 step 549,cls loss=0.009134,rec_loss=0.014616,loss=0.023750,mAp=99.3990\n",
      "train epoch 17 step 599,cls loss=0.009161,rec_loss=0.014742,loss=0.023904,mAp=99.4421\n",
      "train epoch 17 step 649,cls loss=0.009225,rec_loss=0.014772,loss=0.023997,mAp=99.4113\n",
      "train epoch 17 step 699,cls loss=0.008966,rec_loss=0.014762,loss=0.023728,mAp=99.4355\n",
      "train epoch 17 step 749,cls loss=0.008864,rec_loss=0.014780,loss=0.023644,mAp=99.4593\n",
      "train epoch 17 step 799,cls loss=0.008895,rec_loss=0.014756,loss=0.023652,mAp=99.4618\n",
      "train epoch 17 step 849,cls loss=0.009798,rec_loss=0.014818,loss=0.024616,mAp=99.4346\n",
      "train epoch 17 step 899,cls loss=0.009697,rec_loss=0.014842,loss=0.024539,mAp=99.4198\n",
      "train epoch 17 step 949,cls loss=0.009791,rec_loss=0.014814,loss=0.024605,mAp=99.4364\n",
      "train epoch 17 step 999,cls loss=0.009667,rec_loss=0.014850,loss=0.024516,mAp=99.4458\n",
      "train epoch 17 step 1049,cls loss=0.009886,rec_loss=0.014876,loss=0.024762,mAp=99.4464\n",
      "train epoch 17 step 1099,cls loss=0.009924,rec_loss=0.014854,loss=0.024778,mAp=99.4261\n",
      "train epoch 17 step 1149,cls loss=0.010157,rec_loss=0.014895,loss=0.025052,mAp=99.3750\n",
      "train epoch 17 step 1199,cls loss=0.010536,rec_loss=0.014961,loss=0.025497,mAp=99.3090\n",
      "train epoch 17 step 1249,cls loss=0.010667,rec_loss=0.014981,loss=0.025648,mAp=99.2767\n",
      "train epoch 17 step 1299,cls loss=0.010761,rec_loss=0.015045,loss=0.025806,mAp=99.2516\n",
      "train epoch 17 step 1349,cls loss=0.010627,rec_loss=0.015035,loss=0.025663,mAp=99.2670\n",
      "train epoch 17 step 1399,cls loss=0.010723,rec_loss=0.015029,loss=0.025752,mAp=99.2718\n",
      "train epoch 17 step 1449,cls loss=0.010721,rec_loss=0.015005,loss=0.025726,mAp=99.2625\n",
      "train epoch 17 step 1499,cls loss=0.010732,rec_loss=0.015001,loss=0.025733,mAp=99.2481\n",
      "train epoch 17 step 1549,cls loss=0.010733,rec_loss=0.015016,loss=0.025749,mAp=99.2469\n",
      "train epoch 17 step 1599,cls loss=0.010625,rec_loss=0.015042,loss=0.025667,mAp=99.2300\n",
      "train epoch 17 step 1649,cls loss=0.010667,rec_loss=0.015036,loss=0.025703,mAp=99.2433\n",
      "train epoch 17 step 1699,cls loss=0.010707,rec_loss=0.015054,loss=0.025761,mAp=99.2464\n",
      "train epoch 17 step 1749,cls loss=0.010859,rec_loss=0.015093,loss=0.025952,mAp=99.2244\n",
      "train epoch 17 step 1799,cls loss=0.011018,rec_loss=0.015138,loss=0.026157,mAp=99.1985\n",
      "train epoch 17 step 1849,cls loss=0.011367,rec_loss=0.015161,loss=0.026528,mAp=99.1530\n",
      "train epoch 17 step 1899,cls loss=0.011444,rec_loss=0.015186,loss=0.026630,mAp=99.1621\n",
      "train epoch 17 step 1949,cls loss=0.011479,rec_loss=0.015166,loss=0.026645,mAp=99.1473\n",
      "train epoch 17 step 1999,cls loss=0.011480,rec_loss=0.015172,loss=0.026653,mAp=99.1399\n",
      "train epoch 17 step 2049,cls loss=0.011691,rec_loss=0.015203,loss=0.026894,mAp=99.1324\n",
      "train epoch 17 step 2099,cls loss=0.011615,rec_loss=0.015204,loss=0.026818,mAp=99.1471\n",
      "train epoch 17 step 2149,cls loss=0.011727,rec_loss=0.015229,loss=0.026955,mAp=99.1485\n",
      "train epoch 17 step 2199,cls loss=0.011623,rec_loss=0.015223,loss=0.026846,mAp=99.1622\n",
      "train epoch 17 step 2249,cls loss=0.011625,rec_loss=0.015204,loss=0.026829,mAp=99.1697\n",
      "train epoch 17 step 2299,cls loss=0.011774,rec_loss=0.015207,loss=0.026981,mAp=99.1524\n",
      "train epoch 17 step 2349,cls loss=0.011816,rec_loss=0.015206,loss=0.027023,mAp=99.1456\n",
      "train epoch 17 step 2399,cls loss=0.011881,rec_loss=0.015219,loss=0.027100,mAp=99.1438\n",
      "train epoch 17 step 2449,cls loss=0.011934,rec_loss=0.015227,loss=0.027161,mAp=99.1421\n",
      "train epoch 17 step 2499,cls loss=0.011885,rec_loss=0.015226,loss=0.027111,mAp=99.1492\n",
      "train epoch 17 step 2549,cls loss=0.011748,rec_loss=0.015214,loss=0.026962,mAp=99.1659\n",
      "train epoch 17 step 2599,cls loss=0.011608,rec_loss=0.015205,loss=0.026813,mAp=99.1819\n",
      "val epoch 17 step 19,cls loss=0.292385,rec_loss=0.018494,loss=0.310879,mAp=80.9760\n",
      "val epoch 17 step 39,cls loss=0.272003,rec_loss=0.018194,loss=0.290198,mAp=80.6990\n",
      "val epoch 17 step 59,cls loss=0.257537,rec_loss=0.017877,loss=0.275413,mAp=81.6167\n",
      "val epoch 17 step 79,cls loss=0.261823,rec_loss=0.017928,loss=0.279751,mAp=81.5771\n",
      "val epoch 17 step 99,cls loss=0.236965,rec_loss=0.017606,loss=0.254571,mAp=82.7176\n",
      "val epoch 17 step 119,cls loss=0.218365,rec_loss=0.017118,loss=0.235484,mAp=83.7617\n",
      "val epoch 17 step 139,cls loss=0.209338,rec_loss=0.016917,loss=0.226254,mAp=83.6321\n",
      "val epoch 17 step 159,cls loss=0.204027,rec_loss=0.016561,loss=0.220588,mAp=83.6084\n",
      "val epoch 17 step 179,cls loss=0.197419,rec_loss=0.016404,loss=0.213823,mAp=83.9695\n",
      "val epoch 17 step 199,cls loss=0.196630,rec_loss=0.016315,loss=0.212944,mAp=83.6757\n",
      "val epoch 17 step 219,cls loss=0.202703,rec_loss=0.016388,loss=0.219091,mAp=83.6778\n",
      "val epoch 17 step 239,cls loss=0.199025,rec_loss=0.016368,loss=0.215393,mAp=83.9395\n",
      "val epoch 17 step 259,cls loss=0.202114,rec_loss=0.016372,loss=0.218485,mAp=83.8340\n",
      "val epoch 17 step 279,cls loss=0.207503,rec_loss=0.016360,loss=0.223863,mAp=83.8538\n",
      "val epoch 17 step 299,cls loss=0.209163,rec_loss=0.016255,loss=0.225419,mAp=83.8767\n",
      "val epoch 17 step 319,cls loss=0.212674,rec_loss=0.016407,loss=0.229081,mAp=83.6107\n",
      "val epoch 17 step 339,cls loss=0.214873,rec_loss=0.016386,loss=0.231259,mAp=83.3931\n",
      "val epoch 17 step 359,cls loss=0.219827,rec_loss=0.016318,loss=0.236146,mAp=83.0020\n",
      "epoch 17 done!\n",
      "train epoch 18 step 49,cls loss=0.010758,rec_loss=0.014359,loss=0.025117,mAp=99.1000\n",
      "train epoch 18 step 99,cls loss=0.010055,rec_loss=0.014207,loss=0.024262,mAp=99.2792\n",
      "train epoch 18 step 149,cls loss=0.008289,rec_loss=0.014110,loss=0.022398,mAp=99.4083\n",
      "train epoch 18 step 199,cls loss=0.008474,rec_loss=0.014156,loss=0.022631,mAp=99.4104\n",
      "train epoch 18 step 249,cls loss=0.007967,rec_loss=0.014073,loss=0.022040,mAp=99.5283\n",
      "train epoch 18 step 299,cls loss=0.008447,rec_loss=0.014268,loss=0.022716,mAp=99.5583\n",
      "train epoch 18 step 349,cls loss=0.009022,rec_loss=0.014434,loss=0.023456,mAp=99.5857\n",
      "train epoch 18 step 399,cls loss=0.009115,rec_loss=0.014359,loss=0.023474,mAp=99.5594\n",
      "train epoch 18 step 449,cls loss=0.009434,rec_loss=0.014365,loss=0.023799,mAp=99.5824\n",
      "train epoch 18 step 499,cls loss=0.009325,rec_loss=0.014446,loss=0.023771,mAp=99.5825\n",
      "train epoch 18 step 549,cls loss=0.009205,rec_loss=0.014444,loss=0.023650,mAp=99.6053\n",
      "train epoch 18 step 599,cls loss=0.008784,rec_loss=0.014462,loss=0.023246,mAp=99.6174\n",
      "train epoch 18 step 649,cls loss=0.008462,rec_loss=0.014478,loss=0.022940,mAp=99.6468\n",
      "train epoch 18 step 699,cls loss=0.008605,rec_loss=0.014504,loss=0.023110,mAp=99.6274\n",
      "train epoch 18 step 749,cls loss=0.008678,rec_loss=0.014542,loss=0.023220,mAp=99.6494\n",
      "train epoch 18 step 799,cls loss=0.008445,rec_loss=0.014558,loss=0.023002,mAp=99.6557\n",
      "train epoch 18 step 849,cls loss=0.008153,rec_loss=0.014541,loss=0.022695,mAp=99.6760\n",
      "train epoch 18 step 899,cls loss=0.007994,rec_loss=0.014580,loss=0.022574,mAp=99.6940\n",
      "train epoch 18 step 949,cls loss=0.008968,rec_loss=0.014583,loss=0.023551,mAp=99.6364\n",
      "train epoch 18 step 999,cls loss=0.009274,rec_loss=0.014615,loss=0.023889,mAp=99.5921\n",
      "train epoch 18 step 1049,cls loss=0.009206,rec_loss=0.014650,loss=0.023856,mAp=99.5996\n",
      "train epoch 18 step 1099,cls loss=0.009314,rec_loss=0.014692,loss=0.024006,mAp=99.5496\n",
      "train epoch 18 step 1149,cls loss=0.009922,rec_loss=0.014764,loss=0.024686,mAp=99.4658\n",
      "train epoch 18 step 1199,cls loss=0.010250,rec_loss=0.014843,loss=0.025093,mAp=99.4238\n",
      "train epoch 18 step 1249,cls loss=0.010261,rec_loss=0.014903,loss=0.025164,mAp=99.4035\n",
      "train epoch 18 step 1299,cls loss=0.010904,rec_loss=0.014927,loss=0.025830,mAp=99.3643\n",
      "train epoch 18 step 1349,cls loss=0.010978,rec_loss=0.014970,loss=0.025948,mAp=99.3277\n",
      "train epoch 18 step 1399,cls loss=0.010827,rec_loss=0.014990,loss=0.025817,mAp=99.3106\n",
      "train epoch 18 step 1449,cls loss=0.011037,rec_loss=0.015008,loss=0.026045,mAp=99.3042\n",
      "train epoch 18 step 1499,cls loss=0.011397,rec_loss=0.015015,loss=0.026411,mAp=99.2663\n",
      "train epoch 18 step 1549,cls loss=0.011391,rec_loss=0.015033,loss=0.026424,mAp=99.2650\n",
      "train epoch 18 step 1599,cls loss=0.011483,rec_loss=0.015069,loss=0.026552,mAp=99.2645\n",
      "train epoch 18 step 1649,cls loss=0.011496,rec_loss=0.015079,loss=0.026575,mAp=99.2716\n",
      "train epoch 18 step 1699,cls loss=0.011501,rec_loss=0.015066,loss=0.026567,mAp=99.2600\n",
      "train epoch 18 step 1749,cls loss=0.011540,rec_loss=0.015056,loss=0.026596,mAp=99.2644\n",
      "train epoch 18 step 1799,cls loss=0.011733,rec_loss=0.015094,loss=0.026827,mAp=99.2224\n",
      "train epoch 18 step 1849,cls loss=0.011873,rec_loss=0.015104,loss=0.026976,mAp=99.1983\n",
      "train epoch 18 step 1899,cls loss=0.012012,rec_loss=0.015147,loss=0.027159,mAp=99.1964\n",
      "train epoch 18 step 1949,cls loss=0.012022,rec_loss=0.015154,loss=0.027176,mAp=99.1978\n",
      "train epoch 18 step 1999,cls loss=0.012450,rec_loss=0.015180,loss=0.027630,mAp=99.1440\n",
      "train epoch 18 step 2049,cls loss=0.012707,rec_loss=0.015245,loss=0.027952,mAp=99.1297\n",
      "train epoch 18 step 2099,cls loss=0.012704,rec_loss=0.015263,loss=0.027967,mAp=99.1167\n",
      "train epoch 18 step 2149,cls loss=0.012921,rec_loss=0.015257,loss=0.028178,mAp=99.0894\n",
      "train epoch 18 step 2199,cls loss=0.013101,rec_loss=0.015289,loss=0.028390,mAp=99.0683\n",
      "train epoch 18 step 2249,cls loss=0.013283,rec_loss=0.015293,loss=0.028575,mAp=99.0520\n",
      "train epoch 18 step 2299,cls loss=0.013207,rec_loss=0.015299,loss=0.028506,mAp=99.0563\n",
      "train epoch 18 step 2349,cls loss=0.013177,rec_loss=0.015303,loss=0.028480,mAp=99.0551\n",
      "train epoch 18 step 2399,cls loss=0.013298,rec_loss=0.015315,loss=0.028614,mAp=99.0378\n",
      "train epoch 18 step 2449,cls loss=0.013610,rec_loss=0.015307,loss=0.028917,mAp=98.9894\n",
      "train epoch 18 step 2499,cls loss=0.013937,rec_loss=0.015318,loss=0.029255,mAp=98.9475\n",
      "train epoch 18 step 2549,cls loss=0.013973,rec_loss=0.015295,loss=0.029268,mAp=98.9428\n",
      "train epoch 18 step 2599,cls loss=0.014069,rec_loss=0.015307,loss=0.029376,mAp=98.9351\n",
      "val epoch 18 step 19,cls loss=0.274161,rec_loss=0.019396,loss=0.293557,mAp=79.5414\n",
      "val epoch 18 step 39,cls loss=0.250626,rec_loss=0.018994,loss=0.269619,mAp=80.6349\n",
      "val epoch 18 step 59,cls loss=0.246704,rec_loss=0.018661,loss=0.265365,mAp=80.3385\n",
      "val epoch 18 step 79,cls loss=0.249317,rec_loss=0.018730,loss=0.268047,mAp=80.7618\n",
      "val epoch 18 step 99,cls loss=0.223778,rec_loss=0.018363,loss=0.242141,mAp=82.1439\n",
      "val epoch 18 step 119,cls loss=0.208599,rec_loss=0.018024,loss=0.226623,mAp=82.9657\n",
      "val epoch 18 step 139,cls loss=0.196161,rec_loss=0.017885,loss=0.214046,mAp=82.6127\n",
      "val epoch 18 step 159,cls loss=0.191234,rec_loss=0.017626,loss=0.208860,mAp=82.8526\n",
      "val epoch 18 step 179,cls loss=0.185420,rec_loss=0.017471,loss=0.202892,mAp=82.8196\n",
      "val epoch 18 step 199,cls loss=0.184719,rec_loss=0.017405,loss=0.202125,mAp=82.4307\n",
      "val epoch 18 step 219,cls loss=0.185478,rec_loss=0.017493,loss=0.202970,mAp=82.5800\n",
      "val epoch 18 step 239,cls loss=0.182787,rec_loss=0.017436,loss=0.200223,mAp=82.7470\n",
      "val epoch 18 step 259,cls loss=0.182284,rec_loss=0.017445,loss=0.199729,mAp=82.7950\n",
      "val epoch 18 step 279,cls loss=0.188565,rec_loss=0.017418,loss=0.205984,mAp=82.5549\n",
      "val epoch 18 step 299,cls loss=0.190180,rec_loss=0.017306,loss=0.207486,mAp=82.6556\n",
      "val epoch 18 step 319,cls loss=0.193857,rec_loss=0.017456,loss=0.211313,mAp=82.3692\n",
      "val epoch 18 step 339,cls loss=0.196501,rec_loss=0.017441,loss=0.213942,mAp=82.2112\n",
      "val epoch 18 step 359,cls loss=0.198916,rec_loss=0.017385,loss=0.216301,mAp=81.9423\n",
      "epoch 18 done!\n",
      "train epoch 19 step 49,cls loss=0.006296,rec_loss=0.015590,loss=0.021886,mAp=100.0000\n",
      "train epoch 19 step 99,cls loss=0.007960,rec_loss=0.014660,loss=0.022620,mAp=99.9097\n",
      "train epoch 19 step 149,cls loss=0.007790,rec_loss=0.014530,loss=0.022320,mAp=99.8565\n",
      "train epoch 19 step 199,cls loss=0.007605,rec_loss=0.014493,loss=0.022098,mAp=99.6424\n",
      "train epoch 19 step 249,cls loss=0.008529,rec_loss=0.014521,loss=0.023050,mAp=99.5639\n",
      "train epoch 19 step 299,cls loss=0.009266,rec_loss=0.014708,loss=0.023974,mAp=99.4699\n",
      "train epoch 19 step 349,cls loss=0.009764,rec_loss=0.014819,loss=0.024583,mAp=99.3849\n",
      "train epoch 19 step 399,cls loss=0.009432,rec_loss=0.014782,loss=0.024214,mAp=99.4118\n",
      "train epoch 19 step 449,cls loss=0.008922,rec_loss=0.014615,loss=0.023537,mAp=99.4772\n",
      "train epoch 19 step 499,cls loss=0.008326,rec_loss=0.014532,loss=0.022858,mAp=99.5294\n",
      "train epoch 19 step 549,cls loss=0.008328,rec_loss=0.014432,loss=0.022761,mAp=99.4753\n",
      "train epoch 19 step 599,cls loss=0.009157,rec_loss=0.014419,loss=0.023576,mAp=99.4075\n",
      "train epoch 19 step 649,cls loss=0.009554,rec_loss=0.014515,loss=0.024069,mAp=99.2993\n",
      "train epoch 19 step 699,cls loss=0.009182,rec_loss=0.014498,loss=0.023680,mAp=99.3255\n",
      "train epoch 19 step 749,cls loss=0.009342,rec_loss=0.014460,loss=0.023802,mAp=99.3177\n",
      "train epoch 19 step 799,cls loss=0.009078,rec_loss=0.014458,loss=0.023536,mAp=99.3551\n",
      "train epoch 19 step 849,cls loss=0.008908,rec_loss=0.014482,loss=0.023391,mAp=99.3563\n",
      "train epoch 19 step 899,cls loss=0.008719,rec_loss=0.014511,loss=0.023230,mAp=99.3782\n",
      "train epoch 19 step 949,cls loss=0.008645,rec_loss=0.014515,loss=0.023160,mAp=99.3670\n",
      "train epoch 19 step 999,cls loss=0.008524,rec_loss=0.014507,loss=0.023031,mAp=99.3820\n",
      "train epoch 19 step 1049,cls loss=0.008366,rec_loss=0.014441,loss=0.022807,mAp=99.3726\n",
      "train epoch 19 step 1099,cls loss=0.008553,rec_loss=0.014398,loss=0.022951,mAp=99.3973\n",
      "train epoch 19 step 1149,cls loss=0.008845,rec_loss=0.014433,loss=0.023278,mAp=99.3794\n",
      "train epoch 19 step 1199,cls loss=0.008697,rec_loss=0.014435,loss=0.023132,mAp=99.4053\n",
      "train epoch 19 step 1249,cls loss=0.008665,rec_loss=0.014437,loss=0.023103,mAp=99.3907\n",
      "train epoch 19 step 1299,cls loss=0.008619,rec_loss=0.014416,loss=0.023035,mAp=99.4013\n",
      "train epoch 19 step 1349,cls loss=0.008819,rec_loss=0.014431,loss=0.023250,mAp=99.4087\n",
      "train epoch 19 step 1399,cls loss=0.009264,rec_loss=0.014443,loss=0.023707,mAp=99.3435\n",
      "train epoch 19 step 1449,cls loss=0.009454,rec_loss=0.014441,loss=0.023895,mAp=99.3503\n",
      "train epoch 19 step 1499,cls loss=0.009643,rec_loss=0.014451,loss=0.024094,mAp=99.3420\n",
      "train epoch 19 step 1549,cls loss=0.009719,rec_loss=0.014476,loss=0.024195,mAp=99.3261\n",
      "train epoch 19 step 1599,cls loss=0.009733,rec_loss=0.014480,loss=0.024213,mAp=99.3263\n",
      "train epoch 19 step 1649,cls loss=0.009691,rec_loss=0.014477,loss=0.024168,mAp=99.3266\n",
      "train epoch 19 step 1699,cls loss=0.009752,rec_loss=0.014508,loss=0.024260,mAp=99.3280\n",
      "train epoch 19 step 1749,cls loss=0.009717,rec_loss=0.014517,loss=0.024233,mAp=99.3460\n",
      "train epoch 19 step 1799,cls loss=0.009553,rec_loss=0.014524,loss=0.024077,mAp=99.3572\n",
      "train epoch 19 step 1849,cls loss=0.009602,rec_loss=0.014533,loss=0.024135,mAp=99.3746\n",
      "train epoch 19 step 1899,cls loss=0.009467,rec_loss=0.014532,loss=0.023998,mAp=99.3845\n",
      "train epoch 19 step 1949,cls loss=0.009476,rec_loss=0.014551,loss=0.024027,mAp=99.3874\n",
      "train epoch 19 step 1999,cls loss=0.009603,rec_loss=0.014536,loss=0.024139,mAp=99.3677\n",
      "train epoch 19 step 2049,cls loss=0.010158,rec_loss=0.014552,loss=0.024709,mAp=99.3232\n",
      "train epoch 19 step 2099,cls loss=0.010352,rec_loss=0.014607,loss=0.024959,mAp=99.3097\n",
      "train epoch 19 step 2149,cls loss=0.010408,rec_loss=0.014634,loss=0.025041,mAp=99.2947\n",
      "train epoch 19 step 2199,cls loss=0.010479,rec_loss=0.014625,loss=0.025104,mAp=99.2849\n",
      "train epoch 19 step 2249,cls loss=0.010403,rec_loss=0.014646,loss=0.025049,mAp=99.3008\n",
      "train epoch 19 step 2299,cls loss=0.010437,rec_loss=0.014649,loss=0.025087,mAp=99.2943\n",
      "train epoch 19 step 2349,cls loss=0.010614,rec_loss=0.014673,loss=0.025287,mAp=99.2738\n",
      "train epoch 19 step 2399,cls loss=0.010594,rec_loss=0.014675,loss=0.025269,mAp=99.2672\n",
      "train epoch 19 step 2449,cls loss=0.010487,rec_loss=0.014669,loss=0.025156,mAp=99.2822\n",
      "train epoch 19 step 2499,cls loss=0.010388,rec_loss=0.014658,loss=0.025046,mAp=99.2916\n",
      "train epoch 19 step 2549,cls loss=0.010444,rec_loss=0.014653,loss=0.025097,mAp=99.2605\n",
      "train epoch 19 step 2599,cls loss=0.010607,rec_loss=0.014670,loss=0.025277,mAp=99.2587\n",
      "val epoch 19 step 19,cls loss=0.249903,rec_loss=0.018632,loss=0.268535,mAp=79.8924\n",
      "val epoch 19 step 39,cls loss=0.214517,rec_loss=0.018317,loss=0.232834,mAp=82.4577\n",
      "val epoch 19 step 59,cls loss=0.218824,rec_loss=0.017919,loss=0.236742,mAp=82.1144\n",
      "val epoch 19 step 79,cls loss=0.228133,rec_loss=0.018016,loss=0.246149,mAp=80.9381\n",
      "val epoch 19 step 99,cls loss=0.206704,rec_loss=0.017733,loss=0.224437,mAp=81.9466\n",
      "val epoch 19 step 119,cls loss=0.194615,rec_loss=0.017351,loss=0.211967,mAp=82.1803\n",
      "val epoch 19 step 139,cls loss=0.185403,rec_loss=0.017115,loss=0.202519,mAp=82.4915\n",
      "val epoch 19 step 159,cls loss=0.181715,rec_loss=0.016804,loss=0.198519,mAp=82.1302\n",
      "val epoch 19 step 179,cls loss=0.176453,rec_loss=0.016626,loss=0.193078,mAp=82.1982\n",
      "val epoch 19 step 199,cls loss=0.175042,rec_loss=0.016490,loss=0.191532,mAp=81.8268\n",
      "val epoch 19 step 219,cls loss=0.177008,rec_loss=0.016571,loss=0.193580,mAp=81.9449\n",
      "val epoch 19 step 239,cls loss=0.174243,rec_loss=0.016526,loss=0.190769,mAp=82.3442\n",
      "val epoch 19 step 259,cls loss=0.173251,rec_loss=0.016515,loss=0.189766,mAp=82.4885\n",
      "val epoch 19 step 279,cls loss=0.178937,rec_loss=0.016526,loss=0.195463,mAp=82.2832\n",
      "val epoch 19 step 299,cls loss=0.182062,rec_loss=0.016438,loss=0.198500,mAp=82.3679\n",
      "val epoch 19 step 319,cls loss=0.186621,rec_loss=0.016597,loss=0.203219,mAp=82.0052\n",
      "val epoch 19 step 339,cls loss=0.189460,rec_loss=0.016541,loss=0.206001,mAp=82.0444\n",
      "val epoch 19 step 359,cls loss=0.193512,rec_loss=0.016488,loss=0.210000,mAp=81.6258\n",
      "epoch 19 done!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print '开始训练'\n",
    "print 'red==train;green==val...initial plt'\n",
    "fig=plt.figure()\n",
    "ax_loss=fig.add_subplot(1,2,1)\n",
    "plt.title('loss')\n",
    "plt.xlabel(\"epoch\") \n",
    "ax_acc=fig.add_subplot(1,2,2)\n",
    "plt.title('acc')\n",
    "plt.xlabel(\"epoch\")  \n",
    "ax_loss.grid(True) #添加网格\n",
    "ax_acc.grid(True) #添加网格\n",
    "for t in range(epoch_num):\n",
    "    model.train()\n",
    "    mAP_train = []\n",
    "    cls_loss_train= []\n",
    "    rec_loss_train = []\n",
    "    loss_train = []\n",
    "    for i, (img,img_re,label) in enumerate(train_loader):\n",
    "        img = img.cuda()\n",
    "        label = label.cuda()\n",
    "        img_re= img_re.cuda()\n",
    "        label = Variable(label)\n",
    "        input_img = Variable(img)\n",
    "        img_re = Variable(img_re)\n",
    "        \n",
    "        score,fake_im = model(input_img)\n",
    "        model.zero_grad()\n",
    "        mAP_train.append(compute_mAP(label.data, score.data))#计算准确率\n",
    "        cls_loss = cls_criterion(score,label)\n",
    "        rec_loss=rec_criterion(fake_im,img_re)\n",
    "        loss=torch.mean(cls_loss)+torch.mean(rec_loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_train.append(loss.data.cpu()[0])\n",
    "        cls_loss_train.append(cls_loss.data.cpu()[0])\n",
    "        rec_loss_train.append(rec_loss.data.cpu()[0])\n",
    "        if (i+1)%50 == 0:\n",
    "            print_str=\"train epoch %d step %d,cls loss=%.6f,rec_loss=%.6f,loss=%.6f,mAp=%.4f\"\n",
    "            print print_str%(t, i,np.mean(cls_loss_train),\n",
    "                             np.mean(rec_loss_train),np.mean(loss_train),100 * np.mean(mAP_train))\n",
    "    #绘图\n",
    "    ax_loss.scatter(t,np.mean(loss_train),c='r')\n",
    "    ax_acc.scatter(t,100 * np.mean(mAP_train),c='r')\n",
    "    #测试\n",
    "    model.eval()\n",
    "    mAP_val = []\n",
    "    cls_loss_val= []\n",
    "    rec_loss_val = []\n",
    "    loss_val= []\n",
    "    for i, (img, img_re,label) in enumerate(val_loader):\n",
    "        img = img.cuda()\n",
    "        label = label.cuda()\n",
    "        img_re = img_re.cuda()\n",
    "        label = Variable(label)\n",
    "        img_re = Variable(img_re)\n",
    "        input_img = Variable(img)\n",
    "        score,fake_im = model(input_img)\n",
    "        mAP_val.append(compute_mAP(label.data, score.data))  # 计算准确率\n",
    "        cls_loss = cls_criterion(score,label)\n",
    "        rec_loss=rec_criterion(fake_im,img_re)\n",
    "        loss=torch.mean(cls_loss)+torch.mean(rec_loss)\n",
    "        loss_val.append(loss.data.cpu()[0])\n",
    "        cls_loss_val.append(cls_loss.data.cpu()[0])\n",
    "        rec_loss_val.append(rec_loss.data.cpu()[0])\n",
    "        if (i+1)%20 == 0:\n",
    "            print_str=\"val epoch %d step %d,cls loss=%.6f,rec_loss=%.6f,loss=%.6f,mAp=%.4f\"\n",
    "            print print_str%(t, i,np.mean(cls_loss_val),\n",
    "                             np.mean(rec_loss_val),np.mean(loss_val),100 * np.mean(mAP_val))\n",
    "    #绘图\n",
    "    ax_loss.scatter(t,np.mean(loss_val),c='g')\n",
    "    ax_acc.scatter(t,100 * np.mean(mAP_val),c='g')\n",
    "    print('epoch %d done!'%t)\n",
    "    model.save_v2(save_dir=ex_dir,epoch=t,train_stat=mAP_train,val_stat=mAP_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
