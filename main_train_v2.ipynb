{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这次我准备利用10582的数据集进行多标签分类的训练了\n",
      "加载数据集完成\n"
     ]
    }
   ],
   "source": [
    "print '这次我准备利用10582的数据集进行多标签分类的训练了'\n",
    "import lib.Experiment as ex\n",
    "from datasets.VOC_dataset_aug import VOC_dataset_aug\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score\n",
    "from torch.autograd.variable import Variable\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import networks.VGG16_224 as vgg_model\n",
    "from lib.loss import MultiLabelClsV1\n",
    "import os\n",
    "import networks.test as ts\n",
    "def compute_mAP(labels,outputs):\n",
    "    y_true = labels.cpu().numpy()\n",
    "    y_pred = outputs.cpu().numpy()\n",
    "    AP = []\n",
    "    for i in range(y_true.shape[0]):\n",
    "        AP.append(average_precision_score(y_true[i],y_pred[i]))\n",
    "    return np.mean(AP)\n",
    "ex_dir='./data/training_luo_v2_10582aug'\n",
    "batch_size=4\n",
    "workers=4\n",
    "epoch_num=20\n",
    "ex.check_dir(ex_dir)\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "voc_train=VOC_dataset_aug(train='train', transform=transforms.Compose([transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "    ]), label_transform=None )\n",
    "voc_val=VOC_dataset_aug(train='val', transform=transforms.Compose([transforms.Resize((224, 224)),transforms.ToTensor()]),\n",
    "                    label_transform=None  )\n",
    "train_loader = torch.utils.data.DataLoader(voc_train,\n",
    "    batch_size=batch_size, shuffle=True,\n",
    "    num_workers=workers, pin_memory=True)\n",
    "val_loader = torch.utils.data.DataLoader(voc_val,\n",
    "    batch_size=batch_size, shuffle=False,\n",
    "    num_workers=workers, pin_memory=True)\n",
    "print '加载数据集完成'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载模型完成\n"
     ]
    }
   ],
   "source": [
    "model=vgg_model.get_VGG16_224_model()\n",
    "model.cuda()\n",
    "criterion = nn.MultiLabelSoftMarginLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "print '加载模型完成'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练\n",
      "red==train;green==val...initial plt\n",
      "train epoch 0 step 49, mean loss=0.098900, mean mAp=84.6907\n",
      "train epoch 0 step 99, mean loss=0.100159, mean mAp=84.2910\n",
      "train epoch 0 step 149, mean loss=0.102356, mean mAp=83.5408\n",
      "train epoch 0 step 199, mean loss=0.104954, mean mAp=83.7196\n",
      "train epoch 0 step 249, mean loss=0.106091, mean mAp=83.6039\n",
      "train epoch 0 step 299, mean loss=0.106139, mean mAp=83.8866\n",
      "train epoch 0 step 349, mean loss=0.106500, mean mAp=83.6988\n",
      "train epoch 0 step 399, mean loss=0.108354, mean mAp=83.1519\n",
      "train epoch 0 step 449, mean loss=0.107237, mean mAp=83.3493\n",
      "train epoch 0 step 499, mean loss=0.107644, mean mAp=83.6531\n",
      "train epoch 0 step 549, mean loss=0.107550, mean mAp=83.8060\n",
      "train epoch 0 step 599, mean loss=0.107828, mean mAp=83.9837\n",
      "train epoch 0 step 649, mean loss=0.108061, mean mAp=83.9034\n",
      "train epoch 0 step 699, mean loss=0.108151, mean mAp=83.9624\n",
      "train epoch 0 step 749, mean loss=0.108701, mean mAp=83.8023\n",
      "train epoch 0 step 799, mean loss=0.108563, mean mAp=83.8328\n",
      "train epoch 0 step 849, mean loss=0.108167, mean mAp=83.8217\n",
      "train epoch 0 step 899, mean loss=0.108639, mean mAp=83.7814\n",
      "train epoch 0 step 949, mean loss=0.108427, mean mAp=83.9872\n",
      "train epoch 0 step 999, mean loss=0.108204, mean mAp=83.9165\n",
      "train epoch 0 step 1049, mean loss=0.107834, mean mAp=83.9485\n",
      "train epoch 0 step 1099, mean loss=0.107102, mean mAp=84.1107\n",
      "train epoch 0 step 1149, mean loss=0.107633, mean mAp=84.0176\n",
      "train epoch 0 step 1199, mean loss=0.107316, mean mAp=84.1093\n",
      "train epoch 0 step 1249, mean loss=0.107434, mean mAp=84.0591\n",
      "train epoch 0 step 1299, mean loss=0.107705, mean mAp=83.9725\n",
      "train epoch 0 step 1349, mean loss=0.107065, mean mAp=84.0395\n",
      "train epoch 0 step 1399, mean loss=0.106936, mean mAp=84.0958\n",
      "train epoch 0 step 1449, mean loss=0.107080, mean mAp=84.1400\n",
      "train epoch 0 step 1499, mean loss=0.107111, mean mAp=84.1984\n",
      "train epoch 0 step 1549, mean loss=0.107402, mean mAp=84.0686\n",
      "train epoch 0 step 1599, mean loss=0.106966, mean mAp=84.1801\n",
      "train epoch 0 step 1649, mean loss=0.107214, mean mAp=84.1730\n",
      "train epoch 0 step 1699, mean loss=0.106949, mean mAp=84.2292\n",
      "train epoch 0 step 1749, mean loss=0.106771, mean mAp=84.2937\n",
      "train epoch 0 step 1799, mean loss=0.106651, mean mAp=84.2953\n",
      "train epoch 0 step 1849, mean loss=0.106316, mean mAp=84.3828\n",
      "train epoch 0 step 1899, mean loss=0.106357, mean mAp=84.3948\n",
      "train epoch 0 step 1949, mean loss=0.106208, mean mAp=84.4317\n",
      "train epoch 0 step 1999, mean loss=0.105989, mean mAp=84.4893\n",
      "train epoch 0 step 2049, mean loss=0.106067, mean mAp=84.4848\n",
      "train epoch 0 step 2099, mean loss=0.105712, mean mAp=84.5556\n",
      "train epoch 0 step 2149, mean loss=0.105680, mean mAp=84.5786\n",
      "train epoch 0 step 2199, mean loss=0.105626, mean mAp=84.6183\n",
      "train epoch 0 step 2249, mean loss=0.105902, mean mAp=84.5860\n",
      "train epoch 0 step 2299, mean loss=0.105867, mean mAp=84.6113\n",
      "train epoch 0 step 2349, mean loss=0.105783, mean mAp=84.6662\n",
      "train epoch 0 step 2399, mean loss=0.105839, mean mAp=84.6429\n",
      "train epoch 0 step 2449, mean loss=0.105806, mean mAp=84.6767\n",
      "train epoch 0 step 2499, mean loss=0.105627, mean mAp=84.7225\n",
      "train epoch 0 step 2549, mean loss=0.105807, mean mAp=84.6871\n",
      "train epoch 0 step 2599, mean loss=0.105587, mean mAp=84.7503\n",
      "val epoch 0 step 19, loss=0.1347  mAp=84.1194\n",
      "val epoch 0 step 39, loss=0.1357  mAp=80.4949\n",
      "val epoch 0 step 59, loss=0.1349  mAp=80.5468\n",
      "val epoch 0 step 79, loss=0.1324  mAp=81.0351\n",
      "val epoch 0 step 99, loss=0.1252  mAp=81.5216\n",
      "val epoch 0 step 119, loss=0.1196  mAp=82.2318\n",
      "val epoch 0 step 139, loss=0.1161  mAp=82.1833\n",
      "val epoch 0 step 159, loss=0.1172  mAp=81.5641\n",
      "val epoch 0 step 179, loss=0.1150  mAp=81.4677\n",
      "val epoch 0 step 199, loss=0.1146  mAp=81.0837\n",
      "val epoch 0 step 219, loss=0.1139  mAp=81.1676\n",
      "val epoch 0 step 239, loss=0.1117  mAp=81.3441\n",
      "val epoch 0 step 259, loss=0.1120  mAp=81.5208\n",
      "val epoch 0 step 279, loss=0.1135  mAp=81.7658\n",
      "val epoch 0 step 299, loss=0.1137  mAp=81.8347\n",
      "val epoch 0 step 319, loss=0.1158  mAp=81.5461\n",
      "val epoch 0 step 339, loss=0.1165  mAp=81.5799\n",
      "val epoch 0 step 359, loss=0.1189  mAp=81.4049\n",
      "epoch 0 done!\n",
      "train epoch 1 step 49, mean loss=0.082159, mean mAp=87.6217\n",
      "train epoch 1 step 99, mean loss=0.082570, mean mAp=88.5376\n",
      "train epoch 1 step 149, mean loss=0.086177, mean mAp=88.2135\n",
      "train epoch 1 step 199, mean loss=0.085999, mean mAp=88.3441\n",
      "train epoch 1 step 249, mean loss=0.085477, mean mAp=88.8983\n",
      "train epoch 1 step 299, mean loss=0.085114, mean mAp=88.8318\n",
      "train epoch 1 step 349, mean loss=0.084309, mean mAp=88.7942\n",
      "train epoch 1 step 399, mean loss=0.085854, mean mAp=88.8896\n",
      "train epoch 1 step 449, mean loss=0.084186, mean mAp=89.0070\n",
      "train epoch 1 step 499, mean loss=0.083789, mean mAp=89.1435\n",
      "train epoch 1 step 549, mean loss=0.084590, mean mAp=88.8226\n",
      "train epoch 1 step 599, mean loss=0.085392, mean mAp=88.8904\n",
      "train epoch 1 step 649, mean loss=0.085244, mean mAp=89.0220\n",
      "train epoch 1 step 699, mean loss=0.086244, mean mAp=88.8349\n",
      "train epoch 1 step 749, mean loss=0.085555, mean mAp=88.9239\n",
      "train epoch 1 step 799, mean loss=0.085491, mean mAp=89.0552\n",
      "train epoch 1 step 849, mean loss=0.085085, mean mAp=89.1388\n",
      "train epoch 1 step 899, mean loss=0.086004, mean mAp=88.8225\n",
      "train epoch 1 step 949, mean loss=0.086319, mean mAp=88.8476\n",
      "train epoch 1 step 999, mean loss=0.085911, mean mAp=88.9008\n",
      "train epoch 1 step 1049, mean loss=0.085501, mean mAp=89.0262\n",
      "train epoch 1 step 1099, mean loss=0.085480, mean mAp=89.0720\n",
      "train epoch 1 step 1149, mean loss=0.086206, mean mAp=88.9716\n",
      "train epoch 1 step 1199, mean loss=0.086291, mean mAp=88.8651\n",
      "train epoch 1 step 1249, mean loss=0.086693, mean mAp=88.8330\n",
      "train epoch 1 step 1299, mean loss=0.086472, mean mAp=88.8323\n",
      "train epoch 1 step 1349, mean loss=0.086977, mean mAp=88.7092\n",
      "train epoch 1 step 1399, mean loss=0.087057, mean mAp=88.6621\n",
      "train epoch 1 step 1449, mean loss=0.087211, mean mAp=88.6278\n",
      "train epoch 1 step 1499, mean loss=0.087005, mean mAp=88.6569\n",
      "train epoch 1 step 1549, mean loss=0.087410, mean mAp=88.5949\n",
      "train epoch 1 step 1599, mean loss=0.087418, mean mAp=88.6060\n",
      "train epoch 1 step 1649, mean loss=0.087075, mean mAp=88.6188\n",
      "train epoch 1 step 1699, mean loss=0.087246, mean mAp=88.5762\n",
      "train epoch 1 step 1749, mean loss=0.087678, mean mAp=88.4911\n",
      "train epoch 1 step 1799, mean loss=0.087539, mean mAp=88.5565\n",
      "train epoch 1 step 1849, mean loss=0.087388, mean mAp=88.5991\n",
      "train epoch 1 step 1899, mean loss=0.087733, mean mAp=88.5117\n",
      "train epoch 1 step 1949, mean loss=0.087753, mean mAp=88.5102\n",
      "train epoch 1 step 1999, mean loss=0.087844, mean mAp=88.4898\n",
      "train epoch 1 step 2049, mean loss=0.087981, mean mAp=88.4909\n",
      "train epoch 1 step 2099, mean loss=0.087988, mean mAp=88.4068\n",
      "train epoch 1 step 2149, mean loss=0.088458, mean mAp=88.3275\n",
      "train epoch 1 step 2199, mean loss=0.088254, mean mAp=88.3862\n",
      "train epoch 1 step 2249, mean loss=0.088477, mean mAp=88.3207\n",
      "train epoch 1 step 2299, mean loss=0.088505, mean mAp=88.3373\n",
      "train epoch 1 step 2349, mean loss=0.088511, mean mAp=88.3218\n",
      "train epoch 1 step 2399, mean loss=0.088481, mean mAp=88.3606\n",
      "train epoch 1 step 2449, mean loss=0.088570, mean mAp=88.3397\n",
      "train epoch 1 step 2499, mean loss=0.088614, mean mAp=88.3475\n",
      "train epoch 1 step 2549, mean loss=0.088689, mean mAp=88.3106\n",
      "train epoch 1 step 2599, mean loss=0.088980, mean mAp=88.2729\n",
      "val epoch 1 step 19, loss=0.1612  mAp=77.8516\n",
      "val epoch 1 step 39, loss=0.1462  mAp=77.6769\n",
      "val epoch 1 step 59, loss=0.1448  mAp=77.9106\n",
      "val epoch 1 step 79, loss=0.1421  mAp=78.8965\n",
      "val epoch 1 step 99, loss=0.1341  mAp=80.2911\n",
      "val epoch 1 step 119, loss=0.1254  mAp=81.6666\n",
      "val epoch 1 step 139, loss=0.1209  mAp=81.9871\n",
      "val epoch 1 step 159, loss=0.1210  mAp=81.7108\n",
      "val epoch 1 step 179, loss=0.1181  mAp=81.8390\n",
      "val epoch 1 step 199, loss=0.1161  mAp=82.0772\n",
      "val epoch 1 step 219, loss=0.1155  mAp=82.2532\n",
      "val epoch 1 step 239, loss=0.1133  mAp=82.5821\n",
      "val epoch 1 step 259, loss=0.1141  mAp=82.2039\n",
      "val epoch 1 step 279, loss=0.1166  mAp=82.2736\n",
      "val epoch 1 step 299, loss=0.1173  mAp=82.4433\n",
      "val epoch 1 step 319, loss=0.1191  mAp=82.2357\n",
      "val epoch 1 step 339, loss=0.1196  mAp=82.2943\n",
      "val epoch 1 step 359, loss=0.1223  mAp=82.1420\n",
      "epoch 1 done!\n",
      "train epoch 2 step 49, mean loss=0.070175, mean mAp=91.1149\n",
      "train epoch 2 step 99, mean loss=0.065504, mean mAp=92.5876\n",
      "train epoch 2 step 149, mean loss=0.069261, mean mAp=91.8677\n",
      "train epoch 2 step 199, mean loss=0.066285, mean mAp=92.5480\n",
      "train epoch 2 step 249, mean loss=0.065857, mean mAp=92.7421\n",
      "train epoch 2 step 299, mean loss=0.067947, mean mAp=92.4972\n",
      "train epoch 2 step 349, mean loss=0.068221, mean mAp=92.3582\n",
      "train epoch 2 step 399, mean loss=0.067985, mean mAp=92.4601\n",
      "train epoch 2 step 449, mean loss=0.068963, mean mAp=92.1705\n",
      "train epoch 2 step 499, mean loss=0.070295, mean mAp=92.0396\n",
      "train epoch 2 step 549, mean loss=0.071960, mean mAp=91.6827\n",
      "train epoch 2 step 599, mean loss=0.072571, mean mAp=91.4783\n",
      "train epoch 2 step 649, mean loss=0.072129, mean mAp=91.5721\n",
      "train epoch 2 step 699, mean loss=0.072593, mean mAp=91.4423\n",
      "train epoch 2 step 749, mean loss=0.072484, mean mAp=91.4996\n",
      "train epoch 2 step 799, mean loss=0.072127, mean mAp=91.6143\n",
      "train epoch 2 step 849, mean loss=0.071646, mean mAp=91.6778\n",
      "train epoch 2 step 899, mean loss=0.071645, mean mAp=91.4891\n",
      "train epoch 2 step 949, mean loss=0.071324, mean mAp=91.5395\n",
      "train epoch 2 step 999, mean loss=0.070920, mean mAp=91.5722\n",
      "train epoch 2 step 1049, mean loss=0.071010, mean mAp=91.5933\n",
      "train epoch 2 step 1099, mean loss=0.071487, mean mAp=91.5364\n",
      "train epoch 2 step 1149, mean loss=0.072200, mean mAp=91.4632\n",
      "train epoch 2 step 1199, mean loss=0.072498, mean mAp=91.3937\n",
      "train epoch 2 step 1249, mean loss=0.072657, mean mAp=91.3585\n",
      "train epoch 2 step 1299, mean loss=0.072927, mean mAp=91.3690\n",
      "train epoch 2 step 1349, mean loss=0.072907, mean mAp=91.3506\n",
      "train epoch 2 step 1399, mean loss=0.073331, mean mAp=91.2834\n",
      "train epoch 2 step 1449, mean loss=0.074046, mean mAp=91.1791\n",
      "train epoch 2 step 1499, mean loss=0.074040, mean mAp=91.1577\n",
      "train epoch 2 step 1549, mean loss=0.074053, mean mAp=91.1734\n",
      "train epoch 2 step 1599, mean loss=0.074030, mean mAp=91.1867\n",
      "train epoch 2 step 1649, mean loss=0.074294, mean mAp=91.1702\n",
      "train epoch 2 step 1699, mean loss=0.074648, mean mAp=91.1494\n",
      "train epoch 2 step 1749, mean loss=0.075053, mean mAp=91.1277\n",
      "train epoch 2 step 1799, mean loss=0.075069, mean mAp=91.0999\n",
      "train epoch 2 step 1849, mean loss=0.075162, mean mAp=91.0329\n",
      "train epoch 2 step 1899, mean loss=0.075189, mean mAp=91.0203\n",
      "train epoch 2 step 1949, mean loss=0.075016, mean mAp=91.0906\n",
      "train epoch 2 step 1999, mean loss=0.074818, mean mAp=91.1183\n",
      "train epoch 2 step 2049, mean loss=0.074733, mean mAp=91.1395\n",
      "train epoch 2 step 2099, mean loss=0.074588, mean mAp=91.1855\n",
      "train epoch 2 step 2149, mean loss=0.074684, mean mAp=91.1485\n",
      "train epoch 2 step 2199, mean loss=0.074509, mean mAp=91.1531\n",
      "train epoch 2 step 2249, mean loss=0.074777, mean mAp=91.1040\n",
      "train epoch 2 step 2299, mean loss=0.074988, mean mAp=91.0405\n",
      "train epoch 2 step 2349, mean loss=0.075123, mean mAp=91.0511\n",
      "train epoch 2 step 2399, mean loss=0.075275, mean mAp=91.0401\n",
      "train epoch 2 step 2449, mean loss=0.075427, mean mAp=91.0189\n",
      "train epoch 2 step 2499, mean loss=0.075575, mean mAp=90.9672\n",
      "train epoch 2 step 2549, mean loss=0.075267, mean mAp=91.0206\n",
      "train epoch 2 step 2599, mean loss=0.075353, mean mAp=91.0062\n",
      "val epoch 2 step 19, loss=0.1232  mAp=84.1106\n",
      "val epoch 2 step 39, loss=0.1196  mAp=83.8482\n",
      "val epoch 2 step 59, loss=0.1227  mAp=83.1767\n",
      "val epoch 2 step 79, loss=0.1207  mAp=83.8810\n",
      "val epoch 2 step 99, loss=0.1129  mAp=84.4117\n",
      "val epoch 2 step 119, loss=0.1078  mAp=85.4781\n",
      "val epoch 2 step 139, loss=0.1053  mAp=85.3777\n",
      "val epoch 2 step 159, loss=0.1054  mAp=85.3589\n",
      "val epoch 2 step 179, loss=0.1014  mAp=85.4104\n",
      "val epoch 2 step 199, loss=0.1004  mAp=85.3221\n",
      "val epoch 2 step 219, loss=0.1004  mAp=85.6428\n",
      "val epoch 2 step 239, loss=0.0980  mAp=86.1150\n",
      "val epoch 2 step 259, loss=0.0988  mAp=86.0227\n",
      "val epoch 2 step 279, loss=0.1003  mAp=86.2535\n",
      "val epoch 2 step 299, loss=0.1013  mAp=86.2122\n",
      "val epoch 2 step 319, loss=0.1033  mAp=85.9463\n",
      "val epoch 2 step 339, loss=0.1045  mAp=85.8343\n",
      "val epoch 2 step 359, loss=0.1071  mAp=85.6263\n",
      "epoch 2 done!\n",
      "train epoch 3 step 49, mean loss=0.057780, mean mAp=93.9608\n",
      "train epoch 3 step 99, mean loss=0.062473, mean mAp=92.9463\n",
      "train epoch 3 step 149, mean loss=0.059264, mean mAp=93.7413\n",
      "train epoch 3 step 199, mean loss=0.056990, mean mAp=94.0777\n",
      "train epoch 3 step 249, mean loss=0.055651, mean mAp=94.4672\n",
      "train epoch 3 step 299, mean loss=0.056644, mean mAp=94.5320\n",
      "train epoch 3 step 349, mean loss=0.055956, mean mAp=94.6010\n",
      "train epoch 3 step 399, mean loss=0.057823, mean mAp=94.3283\n",
      "train epoch 3 step 449, mean loss=0.059327, mean mAp=94.2811\n",
      "train epoch 3 step 499, mean loss=0.059748, mean mAp=94.2176\n",
      "train epoch 3 step 549, mean loss=0.059961, mean mAp=94.1052\n",
      "train epoch 3 step 599, mean loss=0.059645, mean mAp=94.1401\n",
      "train epoch 3 step 649, mean loss=0.058699, mean mAp=94.2701\n",
      "train epoch 3 step 699, mean loss=0.059374, mean mAp=94.1394\n",
      "train epoch 3 step 749, mean loss=0.058923, mean mAp=94.2027\n",
      "train epoch 3 step 799, mean loss=0.059463, mean mAp=94.0721\n",
      "train epoch 3 step 849, mean loss=0.060141, mean mAp=93.9833\n",
      "train epoch 3 step 899, mean loss=0.059756, mean mAp=93.9640\n",
      "train epoch 3 step 949, mean loss=0.059688, mean mAp=93.9885\n",
      "train epoch 3 step 999, mean loss=0.059641, mean mAp=93.8853\n",
      "train epoch 3 step 1049, mean loss=0.059903, mean mAp=93.8390\n",
      "train epoch 3 step 1099, mean loss=0.060221, mean mAp=93.7498\n",
      "train epoch 3 step 1149, mean loss=0.061141, mean mAp=93.5648\n",
      "train epoch 3 step 1199, mean loss=0.061300, mean mAp=93.5061\n",
      "train epoch 3 step 1249, mean loss=0.061042, mean mAp=93.4961\n",
      "train epoch 3 step 1299, mean loss=0.061165, mean mAp=93.5407\n",
      "train epoch 3 step 1349, mean loss=0.061301, mean mAp=93.4858\n",
      "train epoch 3 step 1399, mean loss=0.061429, mean mAp=93.4978\n",
      "train epoch 3 step 1449, mean loss=0.061689, mean mAp=93.4812\n",
      "train epoch 3 step 1499, mean loss=0.061550, mean mAp=93.4960\n",
      "train epoch 3 step 1549, mean loss=0.061714, mean mAp=93.4830\n",
      "train epoch 3 step 1599, mean loss=0.061707, mean mAp=93.4971\n",
      "train epoch 3 step 1649, mean loss=0.061862, mean mAp=93.4389\n",
      "train epoch 3 step 1699, mean loss=0.061861, mean mAp=93.3949\n",
      "train epoch 3 step 1749, mean loss=0.062226, mean mAp=93.3334\n",
      "train epoch 3 step 1799, mean loss=0.062215, mean mAp=93.3261\n",
      "train epoch 3 step 1849, mean loss=0.062454, mean mAp=93.3228\n",
      "train epoch 3 step 1899, mean loss=0.062718, mean mAp=93.3159\n",
      "train epoch 3 step 1949, mean loss=0.062856, mean mAp=93.2697\n",
      "train epoch 3 step 1999, mean loss=0.063084, mean mAp=93.2637\n",
      "train epoch 3 step 2049, mean loss=0.063131, mean mAp=93.2372\n",
      "train epoch 3 step 2099, mean loss=0.063321, mean mAp=93.2163\n",
      "train epoch 3 step 2149, mean loss=0.063355, mean mAp=93.2221\n",
      "train epoch 3 step 2199, mean loss=0.063687, mean mAp=93.1771\n",
      "train epoch 3 step 2249, mean loss=0.063706, mean mAp=93.1339\n",
      "train epoch 3 step 2299, mean loss=0.063326, mean mAp=93.1751\n",
      "train epoch 3 step 2349, mean loss=0.063539, mean mAp=93.1279\n",
      "train epoch 3 step 2399, mean loss=0.063630, mean mAp=93.1176\n",
      "train epoch 3 step 2449, mean loss=0.063439, mean mAp=93.1618\n",
      "train epoch 3 step 2499, mean loss=0.063478, mean mAp=93.1303\n",
      "train epoch 3 step 2549, mean loss=0.063612, mean mAp=93.1195\n",
      "train epoch 3 step 2599, mean loss=0.063384, mean mAp=93.1629\n",
      "val epoch 3 step 19, loss=0.1304  mAp=86.1826\n",
      "val epoch 3 step 39, loss=0.1246  mAp=85.1851\n",
      "val epoch 3 step 59, loss=0.1251  mAp=84.8321\n",
      "val epoch 3 step 79, loss=0.1310  mAp=84.5226\n",
      "val epoch 3 step 99, loss=0.1236  mAp=85.3498\n",
      "val epoch 3 step 119, loss=0.1167  mAp=86.6017\n",
      "val epoch 3 step 139, loss=0.1138  mAp=86.4807\n",
      "val epoch 3 step 159, loss=0.1122  mAp=86.3752\n",
      "val epoch 3 step 179, loss=0.1085  mAp=86.2872\n",
      "val epoch 3 step 199, loss=0.1069  mAp=86.3239\n",
      "val epoch 3 step 219, loss=0.1082  mAp=86.2358\n",
      "val epoch 3 step 239, loss=0.1061  mAp=86.3308\n",
      "val epoch 3 step 259, loss=0.1050  mAp=86.5766\n",
      "val epoch 3 step 279, loss=0.1072  mAp=86.6029\n",
      "val epoch 3 step 299, loss=0.1070  mAp=86.7015\n",
      "val epoch 3 step 319, loss=0.1084  mAp=86.4057\n",
      "val epoch 3 step 339, loss=0.1097  mAp=86.3994\n",
      "val epoch 3 step 359, loss=0.1123  mAp=86.2463\n",
      "epoch 3 done!\n",
      "train epoch 4 step 49, mean loss=0.046635, mean mAp=96.0290\n",
      "train epoch 4 step 99, mean loss=0.050298, mean mAp=95.5565\n",
      "train epoch 4 step 149, mean loss=0.052408, mean mAp=95.6505\n",
      "train epoch 4 step 199, mean loss=0.049548, mean mAp=95.5974\n",
      "train epoch 4 step 249, mean loss=0.049891, mean mAp=95.5745\n",
      "train epoch 4 step 299, mean loss=0.048656, mean mAp=95.5301\n",
      "train epoch 4 step 349, mean loss=0.049193, mean mAp=95.3601\n",
      "train epoch 4 step 399, mean loss=0.049721, mean mAp=95.5027\n",
      "train epoch 4 step 449, mean loss=0.050413, mean mAp=95.4420\n",
      "train epoch 4 step 499, mean loss=0.050518, mean mAp=95.3459\n",
      "train epoch 4 step 549, mean loss=0.050285, mean mAp=95.3463\n",
      "train epoch 4 step 599, mean loss=0.049919, mean mAp=95.3864\n",
      "train epoch 4 step 649, mean loss=0.051652, mean mAp=95.1559\n",
      "train epoch 4 step 699, mean loss=0.051099, mean mAp=95.1788\n",
      "train epoch 4 step 749, mean loss=0.050934, mean mAp=95.1843\n",
      "train epoch 4 step 799, mean loss=0.051086, mean mAp=95.1046\n",
      "train epoch 4 step 849, mean loss=0.051664, mean mAp=95.0313\n",
      "train epoch 4 step 899, mean loss=0.051886, mean mAp=95.0125\n",
      "train epoch 4 step 949, mean loss=0.051740, mean mAp=94.9920\n",
      "train epoch 4 step 999, mean loss=0.051955, mean mAp=95.0017\n",
      "train epoch 4 step 1049, mean loss=0.052283, mean mAp=94.8513\n",
      "train epoch 4 step 1099, mean loss=0.051472, mean mAp=94.9929\n",
      "train epoch 4 step 1149, mean loss=0.051603, mean mAp=95.0029\n",
      "train epoch 4 step 1199, mean loss=0.051731, mean mAp=94.9823\n",
      "train epoch 4 step 1249, mean loss=0.052076, mean mAp=94.9466\n",
      "train epoch 4 step 1299, mean loss=0.052265, mean mAp=94.8919\n",
      "train epoch 4 step 1349, mean loss=0.052607, mean mAp=94.8248\n",
      "train epoch 4 step 1399, mean loss=0.052627, mean mAp=94.8374\n",
      "train epoch 4 step 1449, mean loss=0.052460, mean mAp=94.8977\n",
      "train epoch 4 step 1499, mean loss=0.052648, mean mAp=94.8726\n",
      "train epoch 4 step 1549, mean loss=0.052895, mean mAp=94.8638\n",
      "train epoch 4 step 1599, mean loss=0.052869, mean mAp=94.8511\n",
      "train epoch 4 step 1649, mean loss=0.052803, mean mAp=94.8625\n",
      "train epoch 4 step 1699, mean loss=0.052543, mean mAp=94.9150\n",
      "train epoch 4 step 1749, mean loss=0.052543, mean mAp=94.8887\n",
      "train epoch 4 step 1799, mean loss=0.052669, mean mAp=94.8843\n",
      "train epoch 4 step 1849, mean loss=0.052923, mean mAp=94.8732\n",
      "train epoch 4 step 1899, mean loss=0.052984, mean mAp=94.8624\n",
      "train epoch 4 step 1949, mean loss=0.052760, mean mAp=94.8771\n",
      "train epoch 4 step 1999, mean loss=0.052710, mean mAp=94.9103\n",
      "train epoch 4 step 2049, mean loss=0.052853, mean mAp=94.8647\n",
      "train epoch 4 step 2099, mean loss=0.052880, mean mAp=94.8500\n",
      "train epoch 4 step 2149, mean loss=0.052824, mean mAp=94.8755\n",
      "train epoch 4 step 2199, mean loss=0.053138, mean mAp=94.8143\n",
      "train epoch 4 step 2249, mean loss=0.053261, mean mAp=94.7582\n",
      "train epoch 4 step 2299, mean loss=0.053542, mean mAp=94.7195\n",
      "train epoch 4 step 2349, mean loss=0.053396, mean mAp=94.7536\n",
      "train epoch 4 step 2399, mean loss=0.053270, mean mAp=94.7656\n",
      "train epoch 4 step 2449, mean loss=0.053084, mean mAp=94.8051\n",
      "train epoch 4 step 2499, mean loss=0.053075, mean mAp=94.8176\n",
      "train epoch 4 step 2549, mean loss=0.053197, mean mAp=94.8203\n",
      "train epoch 4 step 2599, mean loss=0.053253, mean mAp=94.7865\n",
      "val epoch 4 step 19, loss=0.1349  mAp=85.3095\n",
      "val epoch 4 step 39, loss=0.1260  mAp=85.1253\n",
      "val epoch 4 step 59, loss=0.1218  mAp=86.1095\n",
      "val epoch 4 step 79, loss=0.1225  mAp=85.5348\n",
      "val epoch 4 step 99, loss=0.1151  mAp=86.2923\n",
      "val epoch 4 step 119, loss=0.1078  mAp=87.2991\n",
      "val epoch 4 step 139, loss=0.1053  mAp=86.9080\n",
      "val epoch 4 step 159, loss=0.1059  mAp=86.6108\n",
      "val epoch 4 step 179, loss=0.1016  mAp=86.8442\n",
      "val epoch 4 step 199, loss=0.1030  mAp=86.5198\n",
      "val epoch 4 step 219, loss=0.1021  mAp=86.6375\n",
      "val epoch 4 step 239, loss=0.1009  mAp=87.0553\n",
      "val epoch 4 step 259, loss=0.1002  mAp=87.0974\n",
      "val epoch 4 step 279, loss=0.1032  mAp=87.3056\n",
      "val epoch 4 step 299, loss=0.1044  mAp=87.3890\n",
      "val epoch 4 step 319, loss=0.1064  mAp=87.1113\n",
      "val epoch 4 step 339, loss=0.1091  mAp=86.6877\n",
      "val epoch 4 step 359, loss=0.1115  mAp=86.3989\n",
      "epoch 4 done!\n",
      "train epoch 5 step 49, mean loss=0.034109, mean mAp=96.7012\n",
      "train epoch 5 step 99, mean loss=0.038647, mean mAp=96.7406\n",
      "train epoch 5 step 149, mean loss=0.037753, mean mAp=96.7039\n",
      "train epoch 5 step 199, mean loss=0.036758, mean mAp=96.9522\n",
      "train epoch 5 step 249, mean loss=0.037522, mean mAp=96.9146\n",
      "train epoch 5 step 299, mean loss=0.037461, mean mAp=96.9247\n",
      "train epoch 5 step 349, mean loss=0.038102, mean mAp=96.6905\n",
      "train epoch 5 step 399, mean loss=0.037535, mean mAp=96.7611\n",
      "train epoch 5 step 449, mean loss=0.040537, mean mAp=96.3487\n",
      "train epoch 5 step 499, mean loss=0.042034, mean mAp=95.9983\n",
      "train epoch 5 step 549, mean loss=0.041696, mean mAp=96.1750\n",
      "train epoch 5 step 599, mean loss=0.041795, mean mAp=96.1848\n",
      "train epoch 5 step 649, mean loss=0.041522, mean mAp=96.2829\n",
      "train epoch 5 step 699, mean loss=0.042165, mean mAp=96.2807\n",
      "train epoch 5 step 749, mean loss=0.043348, mean mAp=96.1596\n",
      "train epoch 5 step 799, mean loss=0.043679, mean mAp=96.1005\n",
      "train epoch 5 step 849, mean loss=0.043731, mean mAp=96.1105\n",
      "train epoch 5 step 899, mean loss=0.044002, mean mAp=96.0998\n",
      "train epoch 5 step 949, mean loss=0.044014, mean mAp=96.1041\n",
      "train epoch 5 step 999, mean loss=0.043978, mean mAp=96.0835\n",
      "train epoch 5 step 1049, mean loss=0.044060, mean mAp=96.1139\n",
      "train epoch 5 step 1099, mean loss=0.044556, mean mAp=96.0707\n",
      "train epoch 5 step 1149, mean loss=0.044479, mean mAp=96.0356\n",
      "train epoch 5 step 1199, mean loss=0.044354, mean mAp=96.0350\n",
      "train epoch 5 step 1249, mean loss=0.044453, mean mAp=96.0414\n",
      "train epoch 5 step 1299, mean loss=0.044247, mean mAp=96.0801\n",
      "train epoch 5 step 1349, mean loss=0.044382, mean mAp=96.0652\n",
      "train epoch 5 step 1399, mean loss=0.044803, mean mAp=96.0073\n",
      "train epoch 5 step 1449, mean loss=0.044558, mean mAp=96.0584\n",
      "train epoch 5 step 1499, mean loss=0.044930, mean mAp=96.0242\n",
      "train epoch 5 step 1549, mean loss=0.044997, mean mAp=96.0322\n",
      "train epoch 5 step 1599, mean loss=0.044819, mean mAp=96.0719\n",
      "train epoch 5 step 1649, mean loss=0.044640, mean mAp=96.0968\n",
      "train epoch 5 step 1699, mean loss=0.044867, mean mAp=96.0707\n",
      "train epoch 5 step 1749, mean loss=0.044813, mean mAp=96.0681\n",
      "train epoch 5 step 1799, mean loss=0.045058, mean mAp=96.0544\n",
      "train epoch 5 step 1849, mean loss=0.044957, mean mAp=96.0453\n",
      "train epoch 5 step 1899, mean loss=0.044860, mean mAp=96.0351\n",
      "train epoch 5 step 1949, mean loss=0.044776, mean mAp=96.0494\n",
      "train epoch 5 step 1999, mean loss=0.044842, mean mAp=96.0616\n",
      "train epoch 5 step 2049, mean loss=0.044869, mean mAp=96.0594\n",
      "train epoch 5 step 2099, mean loss=0.044986, mean mAp=96.0117\n",
      "train epoch 5 step 2149, mean loss=0.045306, mean mAp=95.9697\n",
      "train epoch 5 step 2199, mean loss=0.045444, mean mAp=95.9282\n",
      "train epoch 5 step 2249, mean loss=0.045733, mean mAp=95.9344\n",
      "train epoch 5 step 2299, mean loss=0.045768, mean mAp=95.9406\n",
      "train epoch 5 step 2349, mean loss=0.045712, mean mAp=95.9482\n",
      "train epoch 5 step 2399, mean loss=0.045827, mean mAp=95.9407\n",
      "train epoch 5 step 2449, mean loss=0.045988, mean mAp=95.9159\n",
      "train epoch 5 step 2499, mean loss=0.046060, mean mAp=95.9145\n",
      "train epoch 5 step 2549, mean loss=0.045951, mean mAp=95.9303\n",
      "train epoch 5 step 2599, mean loss=0.046171, mean mAp=95.9014\n",
      "val epoch 5 step 19, loss=0.1444  mAp=86.5189\n",
      "val epoch 5 step 39, loss=0.1406  mAp=84.9368\n",
      "val epoch 5 step 59, loss=0.1402  mAp=83.9387\n",
      "val epoch 5 step 79, loss=0.1358  mAp=84.4051\n",
      "val epoch 5 step 99, loss=0.1323  mAp=84.9498\n",
      "val epoch 5 step 119, loss=0.1226  mAp=86.2189\n",
      "val epoch 5 step 139, loss=0.1209  mAp=85.3685\n",
      "val epoch 5 step 159, loss=0.1205  mAp=85.1803\n",
      "val epoch 5 step 179, loss=0.1177  mAp=85.1513\n",
      "val epoch 5 step 199, loss=0.1158  mAp=85.2477\n",
      "val epoch 5 step 219, loss=0.1166  mAp=85.2674\n",
      "val epoch 5 step 239, loss=0.1143  mAp=85.4813\n",
      "val epoch 5 step 259, loss=0.1159  mAp=85.4342\n",
      "val epoch 5 step 279, loss=0.1169  mAp=85.6175\n",
      "val epoch 5 step 299, loss=0.1165  mAp=85.8653\n",
      "val epoch 5 step 319, loss=0.1184  mAp=85.5296\n",
      "val epoch 5 step 339, loss=0.1200  mAp=85.4390\n",
      "val epoch 5 step 359, loss=0.1231  mAp=85.0139\n",
      "epoch 5 done!\n",
      "train epoch 6 step 49, mean loss=0.042513, mean mAp=96.9082\n",
      "train epoch 6 step 99, mean loss=0.034469, mean mAp=97.2458\n",
      "train epoch 6 step 149, mean loss=0.031191, mean mAp=97.8884\n",
      "train epoch 6 step 199, mean loss=0.029166, mean mAp=98.2849\n",
      "train epoch 6 step 249, mean loss=0.028430, mean mAp=98.3923\n",
      "train epoch 6 step 299, mean loss=0.028627, mean mAp=98.3606\n",
      "train epoch 6 step 349, mean loss=0.031121, mean mAp=98.0781\n",
      "train epoch 6 step 399, mean loss=0.032344, mean mAp=97.8258\n",
      "train epoch 6 step 449, mean loss=0.033940, mean mAp=97.7483\n",
      "train epoch 6 step 499, mean loss=0.034640, mean mAp=97.5775\n",
      "train epoch 6 step 549, mean loss=0.035018, mean mAp=97.5527\n",
      "train epoch 6 step 599, mean loss=0.036280, mean mAp=97.3424\n",
      "train epoch 6 step 649, mean loss=0.036645, mean mAp=97.2299\n",
      "train epoch 6 step 699, mean loss=0.036726, mean mAp=97.1833\n",
      "train epoch 6 step 749, mean loss=0.035989, mean mAp=97.2710\n",
      "train epoch 6 step 799, mean loss=0.036290, mean mAp=97.2088\n",
      "train epoch 6 step 849, mean loss=0.036212, mean mAp=97.2404\n",
      "train epoch 6 step 899, mean loss=0.036133, mean mAp=97.2869\n",
      "train epoch 6 step 949, mean loss=0.035843, mean mAp=97.3360\n",
      "train epoch 6 step 999, mean loss=0.035396, mean mAp=97.3956\n",
      "train epoch 6 step 1049, mean loss=0.034909, mean mAp=97.4748\n",
      "train epoch 6 step 1099, mean loss=0.034997, mean mAp=97.4587\n",
      "train epoch 6 step 1149, mean loss=0.035755, mean mAp=97.3708\n",
      "train epoch 6 step 1199, mean loss=0.035545, mean mAp=97.3380\n",
      "train epoch 6 step 1249, mean loss=0.035363, mean mAp=97.3169\n",
      "train epoch 6 step 1299, mean loss=0.035381, mean mAp=97.3299\n",
      "train epoch 6 step 1349, mean loss=0.035439, mean mAp=97.3337\n",
      "train epoch 6 step 1399, mean loss=0.035793, mean mAp=97.2930\n",
      "train epoch 6 step 1449, mean loss=0.035518, mean mAp=97.3475\n",
      "train epoch 6 step 1499, mean loss=0.035667, mean mAp=97.3419\n",
      "train epoch 6 step 1549, mean loss=0.035775, mean mAp=97.3351\n",
      "train epoch 6 step 1599, mean loss=0.036110, mean mAp=97.3239\n",
      "train epoch 6 step 1649, mean loss=0.036312, mean mAp=97.2963\n",
      "train epoch 6 step 1699, mean loss=0.036104, mean mAp=97.3078\n",
      "train epoch 6 step 1749, mean loss=0.035989, mean mAp=97.3122\n",
      "train epoch 6 step 1799, mean loss=0.036151, mean mAp=97.3059\n",
      "train epoch 6 step 1849, mean loss=0.036133, mean mAp=97.2670\n",
      "train epoch 6 step 1899, mean loss=0.036293, mean mAp=97.2157\n",
      "train epoch 6 step 1949, mean loss=0.036386, mean mAp=97.1759\n",
      "train epoch 6 step 1999, mean loss=0.036429, mean mAp=97.1450\n",
      "train epoch 6 step 2049, mean loss=0.036497, mean mAp=97.1560\n",
      "train epoch 6 step 2099, mean loss=0.036557, mean mAp=97.1507\n",
      "train epoch 6 step 2149, mean loss=0.036571, mean mAp=97.1649\n",
      "train epoch 6 step 2199, mean loss=0.036767, mean mAp=97.1431\n",
      "train epoch 6 step 2249, mean loss=0.036813, mean mAp=97.1395\n",
      "train epoch 6 step 2299, mean loss=0.037141, mean mAp=97.0838\n",
      "train epoch 6 step 2349, mean loss=0.037245, mean mAp=97.0681\n",
      "train epoch 6 step 2399, mean loss=0.037287, mean mAp=97.0679\n",
      "train epoch 6 step 2449, mean loss=0.037374, mean mAp=97.0536\n",
      "train epoch 6 step 2499, mean loss=0.037377, mean mAp=97.0421\n",
      "train epoch 6 step 2549, mean loss=0.037406, mean mAp=97.0541\n",
      "train epoch 6 step 2599, mean loss=0.037375, mean mAp=97.0645\n",
      "val epoch 6 step 19, loss=0.1506  mAp=85.4703\n",
      "val epoch 6 step 39, loss=0.1439  mAp=84.1038\n",
      "val epoch 6 step 59, loss=0.1449  mAp=83.5403\n",
      "val epoch 6 step 79, loss=0.1461  mAp=83.5865\n",
      "val epoch 6 step 99, loss=0.1380  mAp=84.6936\n",
      "val epoch 6 step 119, loss=0.1268  mAp=85.9580\n",
      "val epoch 6 step 139, loss=0.1203  mAp=86.5899\n",
      "val epoch 6 step 159, loss=0.1205  mAp=86.1363\n",
      "val epoch 6 step 179, loss=0.1165  mAp=86.4707\n",
      "val epoch 6 step 199, loss=0.1151  mAp=86.5540\n",
      "val epoch 6 step 219, loss=0.1158  mAp=86.4581\n",
      "val epoch 6 step 239, loss=0.1150  mAp=86.6262\n",
      "val epoch 6 step 259, loss=0.1141  mAp=86.7899\n",
      "val epoch 6 step 279, loss=0.1166  mAp=86.9353\n",
      "val epoch 6 step 299, loss=0.1175  mAp=86.9535\n",
      "val epoch 6 step 319, loss=0.1206  mAp=86.6703\n",
      "val epoch 6 step 339, loss=0.1227  mAp=86.4315\n",
      "val epoch 6 step 359, loss=0.1257  mAp=86.1973\n",
      "epoch 6 done!\n",
      "train epoch 7 step 49, mean loss=0.028446, mean mAp=98.0208\n",
      "train epoch 7 step 99, mean loss=0.030861, mean mAp=97.9295\n",
      "train epoch 7 step 149, mean loss=0.032147, mean mAp=97.5335\n",
      "train epoch 7 step 199, mean loss=0.033092, mean mAp=97.5522\n",
      "train epoch 7 step 249, mean loss=0.031121, mean mAp=97.6035\n",
      "train epoch 7 step 299, mean loss=0.030527, mean mAp=97.7390\n",
      "train epoch 7 step 349, mean loss=0.029110, mean mAp=97.8906\n",
      "train epoch 7 step 399, mean loss=0.028258, mean mAp=97.9480\n",
      "train epoch 7 step 449, mean loss=0.028161, mean mAp=98.0334\n",
      "train epoch 7 step 499, mean loss=0.028527, mean mAp=98.0521\n",
      "train epoch 7 step 549, mean loss=0.029051, mean mAp=97.9309\n",
      "train epoch 7 step 599, mean loss=0.029677, mean mAp=97.8758\n",
      "train epoch 7 step 649, mean loss=0.030503, mean mAp=97.7449\n",
      "train epoch 7 step 699, mean loss=0.031180, mean mAp=97.6849\n",
      "train epoch 7 step 749, mean loss=0.031084, mean mAp=97.7173\n",
      "train epoch 7 step 799, mean loss=0.031498, mean mAp=97.6385\n",
      "train epoch 7 step 849, mean loss=0.031292, mean mAp=97.7270\n",
      "train epoch 7 step 899, mean loss=0.031205, mean mAp=97.7141\n",
      "train epoch 7 step 949, mean loss=0.031206, mean mAp=97.7574\n",
      "train epoch 7 step 999, mean loss=0.031329, mean mAp=97.7370\n",
      "train epoch 7 step 1049, mean loss=0.031685, mean mAp=97.6777\n",
      "train epoch 7 step 1099, mean loss=0.031579, mean mAp=97.6893\n",
      "train epoch 7 step 1149, mean loss=0.032491, mean mAp=97.5981\n",
      "train epoch 7 step 1199, mean loss=0.032685, mean mAp=97.5481\n",
      "train epoch 7 step 1249, mean loss=0.032729, mean mAp=97.5397\n",
      "train epoch 7 step 1299, mean loss=0.033122, mean mAp=97.4822\n",
      "train epoch 7 step 1349, mean loss=0.033010, mean mAp=97.4557\n",
      "train epoch 7 step 1399, mean loss=0.033180, mean mAp=97.4464\n",
      "train epoch 7 step 1449, mean loss=0.033072, mean mAp=97.4465\n",
      "train epoch 7 step 1499, mean loss=0.032982, mean mAp=97.4604\n",
      "train epoch 7 step 1549, mean loss=0.032961, mean mAp=97.4395\n",
      "train epoch 7 step 1599, mean loss=0.033109, mean mAp=97.3953\n",
      "train epoch 7 step 1649, mean loss=0.033050, mean mAp=97.4122\n",
      "train epoch 7 step 1699, mean loss=0.032896, mean mAp=97.4609\n",
      "train epoch 7 step 1749, mean loss=0.032901, mean mAp=97.4554\n",
      "train epoch 7 step 1799, mean loss=0.033008, mean mAp=97.4460\n",
      "train epoch 7 step 1849, mean loss=0.033122, mean mAp=97.4205\n",
      "train epoch 7 step 1899, mean loss=0.033252, mean mAp=97.3874\n",
      "train epoch 7 step 1949, mean loss=0.033409, mean mAp=97.3752\n",
      "train epoch 7 step 1999, mean loss=0.033260, mean mAp=97.3892\n",
      "train epoch 7 step 2049, mean loss=0.033120, mean mAp=97.4039\n",
      "train epoch 7 step 2099, mean loss=0.033183, mean mAp=97.4083\n",
      "train epoch 7 step 2149, mean loss=0.033312, mean mAp=97.3536\n",
      "train epoch 7 step 2199, mean loss=0.033399, mean mAp=97.3289\n",
      "train epoch 7 step 2249, mean loss=0.033255, mean mAp=97.3523\n",
      "train epoch 7 step 2299, mean loss=0.033330, mean mAp=97.3495\n",
      "train epoch 7 step 2349, mean loss=0.033209, mean mAp=97.3755\n",
      "train epoch 7 step 2399, mean loss=0.033340, mean mAp=97.3664\n",
      "train epoch 7 step 2449, mean loss=0.033388, mean mAp=97.3662\n",
      "train epoch 7 step 2499, mean loss=0.033237, mean mAp=97.3803\n",
      "train epoch 7 step 2549, mean loss=0.033209, mean mAp=97.3705\n",
      "train epoch 7 step 2599, mean loss=0.033312, mean mAp=97.3506\n",
      "val epoch 7 step 19, loss=0.1624  mAp=83.0457\n",
      "val epoch 7 step 39, loss=0.1462  mAp=84.5856\n",
      "val epoch 7 step 59, loss=0.1490  mAp=83.0255\n",
      "val epoch 7 step 79, loss=0.1503  mAp=83.0575\n",
      "val epoch 7 step 99, loss=0.1436  mAp=84.3453\n",
      "val epoch 7 step 119, loss=0.1326  mAp=85.6911\n",
      "val epoch 7 step 139, loss=0.1288  mAp=85.4851\n",
      "val epoch 7 step 159, loss=0.1275  mAp=85.4340\n",
      "val epoch 7 step 179, loss=0.1246  mAp=85.6629\n",
      "val epoch 7 step 199, loss=0.1224  mAp=85.6959\n",
      "val epoch 7 step 219, loss=0.1233  mAp=85.6447\n",
      "val epoch 7 step 239, loss=0.1213  mAp=85.8450\n",
      "val epoch 7 step 259, loss=0.1196  mAp=86.3033\n",
      "val epoch 7 step 279, loss=0.1231  mAp=86.3309\n",
      "val epoch 7 step 299, loss=0.1248  mAp=86.3371\n",
      "val epoch 7 step 319, loss=0.1283  mAp=85.9825\n",
      "val epoch 7 step 339, loss=0.1301  mAp=85.8356\n",
      "val epoch 7 step 359, loss=0.1341  mAp=85.3730\n",
      "epoch 7 done!\n",
      "train epoch 8 step 49, mean loss=0.028542, mean mAp=97.6010\n",
      "train epoch 8 step 99, mean loss=0.027836, mean mAp=97.8283\n",
      "train epoch 8 step 149, mean loss=0.028298, mean mAp=97.9557\n",
      "train epoch 8 step 199, mean loss=0.026768, mean mAp=97.9667\n",
      "train epoch 8 step 249, mean loss=0.025875, mean mAp=98.1527\n",
      "train epoch 8 step 299, mean loss=0.025395, mean mAp=98.2404\n",
      "train epoch 8 step 349, mean loss=0.024521, mean mAp=98.3549\n",
      "train epoch 8 step 399, mean loss=0.025204, mean mAp=98.3293\n",
      "train epoch 8 step 449, mean loss=0.024711, mean mAp=98.3253\n",
      "train epoch 8 step 499, mean loss=0.024231, mean mAp=98.4170\n",
      "train epoch 8 step 549, mean loss=0.025818, mean mAp=98.2046\n",
      "train epoch 8 step 599, mean loss=0.025204, mean mAp=98.2617\n",
      "train epoch 8 step 649, mean loss=0.025528, mean mAp=98.1765\n",
      "train epoch 8 step 699, mean loss=0.024867, mean mAp=98.2603\n",
      "train epoch 8 step 749, mean loss=0.024655, mean mAp=98.3229\n",
      "train epoch 8 step 799, mean loss=0.024314, mean mAp=98.3793\n",
      "train epoch 8 step 849, mean loss=0.024135, mean mAp=98.3643\n",
      "train epoch 8 step 899, mean loss=0.025247, mean mAp=98.2196\n",
      "train epoch 8 step 949, mean loss=0.025563, mean mAp=98.1798\n",
      "train epoch 8 step 999, mean loss=0.025492, mean mAp=98.2221\n",
      "train epoch 8 step 1049, mean loss=0.025429, mean mAp=98.1921\n",
      "train epoch 8 step 1099, mean loss=0.025450, mean mAp=98.1633\n",
      "train epoch 8 step 1149, mean loss=0.026079, mean mAp=98.0435\n",
      "train epoch 8 step 1199, mean loss=0.026171, mean mAp=98.0404\n",
      "train epoch 8 step 1249, mean loss=0.026429, mean mAp=98.0291\n",
      "train epoch 8 step 1299, mean loss=0.026270, mean mAp=98.0478\n",
      "train epoch 8 step 1349, mean loss=0.026198, mean mAp=98.0745\n",
      "train epoch 8 step 1399, mean loss=0.026215, mean mAp=98.1014\n",
      "train epoch 8 step 1449, mean loss=0.026473, mean mAp=98.0899\n",
      "train epoch 8 step 1499, mean loss=0.027161, mean mAp=97.9934\n",
      "train epoch 8 step 1549, mean loss=0.027472, mean mAp=97.9675\n",
      "train epoch 8 step 1599, mean loss=0.027284, mean mAp=97.9837\n",
      "train epoch 8 step 1649, mean loss=0.027424, mean mAp=97.9926\n",
      "train epoch 8 step 1699, mean loss=0.027683, mean mAp=97.9463\n",
      "train epoch 8 step 1749, mean loss=0.027738, mean mAp=97.9262\n",
      "train epoch 8 step 1799, mean loss=0.027592, mean mAp=97.9289\n",
      "train epoch 8 step 1849, mean loss=0.027564, mean mAp=97.9295\n",
      "train epoch 8 step 1899, mean loss=0.027566, mean mAp=97.9346\n",
      "train epoch 8 step 1949, mean loss=0.027401, mean mAp=97.9555\n",
      "train epoch 8 step 1999, mean loss=0.027443, mean mAp=97.9680\n",
      "train epoch 8 step 2049, mean loss=0.027539, mean mAp=97.9491\n",
      "train epoch 8 step 2099, mean loss=0.027840, mean mAp=97.9078\n",
      "train epoch 8 step 2149, mean loss=0.028080, mean mAp=97.8877\n",
      "train epoch 8 step 2199, mean loss=0.028054, mean mAp=97.9142\n",
      "train epoch 8 step 2249, mean loss=0.028125, mean mAp=97.9047\n",
      "train epoch 8 step 2299, mean loss=0.028122, mean mAp=97.8931\n",
      "train epoch 8 step 2349, mean loss=0.028271, mean mAp=97.8879\n",
      "train epoch 8 step 2399, mean loss=0.028385, mean mAp=97.8642\n",
      "train epoch 8 step 2449, mean loss=0.028430, mean mAp=97.8533\n",
      "train epoch 8 step 2499, mean loss=0.028497, mean mAp=97.8620\n",
      "train epoch 8 step 2549, mean loss=0.028504, mean mAp=97.8770\n",
      "train epoch 8 step 2599, mean loss=0.028498, mean mAp=97.8909\n",
      "val epoch 8 step 19, loss=0.1606  mAp=85.2253\n",
      "val epoch 8 step 39, loss=0.1981  mAp=82.2689\n",
      "val epoch 8 step 59, loss=0.2011  mAp=81.3507\n",
      "val epoch 8 step 79, loss=0.2089  mAp=81.1884\n",
      "val epoch 8 step 99, loss=0.1950  mAp=81.6611\n",
      "val epoch 8 step 119, loss=0.1844  mAp=82.8820\n",
      "val epoch 8 step 139, loss=0.1797  mAp=82.5337\n",
      "val epoch 8 step 159, loss=0.1773  mAp=82.7112\n",
      "val epoch 8 step 179, loss=0.1697  mAp=83.4236\n",
      "val epoch 8 step 199, loss=0.1669  mAp=83.5719\n",
      "val epoch 8 step 219, loss=0.1658  mAp=83.8982\n",
      "val epoch 8 step 239, loss=0.1631  mAp=84.2527\n",
      "val epoch 8 step 259, loss=0.1626  mAp=84.2900\n",
      "val epoch 8 step 279, loss=0.1651  mAp=84.4392\n",
      "val epoch 8 step 299, loss=0.1670  mAp=84.3847\n",
      "val epoch 8 step 319, loss=0.1700  mAp=84.0157\n",
      "val epoch 8 step 339, loss=0.1722  mAp=83.7053\n",
      "val epoch 8 step 359, loss=0.1747  mAp=83.5701\n",
      "epoch 8 done!\n",
      "train epoch 9 step 49, mean loss=0.025895, mean mAp=98.2917\n",
      "train epoch 9 step 99, mean loss=0.025736, mean mAp=98.0385\n",
      "train epoch 9 step 149, mean loss=0.024988, mean mAp=97.8590\n",
      "train epoch 9 step 199, mean loss=0.024036, mean mAp=98.0088\n",
      "train epoch 9 step 249, mean loss=0.023591, mean mAp=98.1354\n",
      "train epoch 9 step 299, mean loss=0.022972, mean mAp=98.2355\n",
      "train epoch 9 step 349, mean loss=0.022962, mean mAp=98.2281\n",
      "train epoch 9 step 399, mean loss=0.022979, mean mAp=98.2381\n",
      "train epoch 9 step 449, mean loss=0.023297, mean mAp=98.2709\n",
      "train epoch 9 step 499, mean loss=0.023739, mean mAp=98.3830\n",
      "train epoch 9 step 549, mean loss=0.023827, mean mAp=98.3086\n",
      "train epoch 9 step 599, mean loss=0.023670, mean mAp=98.3260\n",
      "train epoch 9 step 649, mean loss=0.023760, mean mAp=98.3201\n",
      "train epoch 9 step 699, mean loss=0.023327, mean mAp=98.3461\n",
      "train epoch 9 step 749, mean loss=0.022901, mean mAp=98.3758\n",
      "train epoch 9 step 799, mean loss=0.023527, mean mAp=98.2919\n",
      "train epoch 9 step 849, mean loss=0.023723, mean mAp=98.2647\n",
      "train epoch 9 step 899, mean loss=0.023838, mean mAp=98.2433\n",
      "train epoch 9 step 949, mean loss=0.023861, mean mAp=98.2532\n",
      "train epoch 9 step 999, mean loss=0.024072, mean mAp=98.2336\n",
      "train epoch 9 step 1049, mean loss=0.023891, mean mAp=98.2590\n",
      "train epoch 9 step 1099, mean loss=0.023781, mean mAp=98.2687\n",
      "train epoch 9 step 1149, mean loss=0.023474, mean mAp=98.3121\n",
      "train epoch 9 step 1199, mean loss=0.023385, mean mAp=98.3185\n",
      "train epoch 9 step 1249, mean loss=0.023732, mean mAp=98.2553\n",
      "train epoch 9 step 1299, mean loss=0.023892, mean mAp=98.2302\n",
      "train epoch 9 step 1349, mean loss=0.023748, mean mAp=98.2640\n",
      "train epoch 9 step 1399, mean loss=0.023680, mean mAp=98.2796\n",
      "train epoch 9 step 1449, mean loss=0.023787, mean mAp=98.2970\n",
      "train epoch 9 step 1499, mean loss=0.024009, mean mAp=98.2668\n",
      "train epoch 9 step 1549, mean loss=0.023970, mean mAp=98.2789\n",
      "train epoch 9 step 1599, mean loss=0.023938, mean mAp=98.2885\n",
      "train epoch 9 step 1649, mean loss=0.023863, mean mAp=98.3000\n",
      "train epoch 9 step 1699, mean loss=0.023958, mean mAp=98.2642\n",
      "train epoch 9 step 1749, mean loss=0.023922, mean mAp=98.2552\n",
      "train epoch 9 step 1799, mean loss=0.024052, mean mAp=98.2257\n",
      "train epoch 9 step 1849, mean loss=0.024319, mean mAp=98.2207\n",
      "train epoch 9 step 1899, mean loss=0.024209, mean mAp=98.2298\n",
      "train epoch 9 step 1949, mean loss=0.024299, mean mAp=98.2283\n",
      "train epoch 9 step 1999, mean loss=0.024497, mean mAp=98.2031\n",
      "train epoch 9 step 2049, mean loss=0.024596, mean mAp=98.1944\n",
      "train epoch 9 step 2099, mean loss=0.024594, mean mAp=98.1978\n",
      "train epoch 9 step 2149, mean loss=0.024579, mean mAp=98.2039\n",
      "train epoch 9 step 2199, mean loss=0.024516, mean mAp=98.2044\n",
      "train epoch 9 step 2249, mean loss=0.024492, mean mAp=98.2067\n",
      "train epoch 9 step 2299, mean loss=0.024643, mean mAp=98.1995\n",
      "train epoch 9 step 2349, mean loss=0.024873, mean mAp=98.1816\n",
      "train epoch 9 step 2399, mean loss=0.024802, mean mAp=98.1853\n",
      "train epoch 9 step 2449, mean loss=0.024849, mean mAp=98.1787\n",
      "train epoch 9 step 2499, mean loss=0.024819, mean mAp=98.1764\n",
      "train epoch 9 step 2549, mean loss=0.025063, mean mAp=98.1619\n",
      "train epoch 9 step 2599, mean loss=0.025193, mean mAp=98.1515\n",
      "val epoch 9 step 19, loss=0.1826  mAp=84.0426\n",
      "val epoch 9 step 39, loss=0.1726  mAp=83.3892\n",
      "val epoch 9 step 59, loss=0.1794  mAp=82.0823\n",
      "val epoch 9 step 79, loss=0.1755  mAp=82.6973\n",
      "val epoch 9 step 99, loss=0.1660  mAp=83.6460\n",
      "val epoch 9 step 119, loss=0.1588  mAp=84.2289\n",
      "val epoch 9 step 139, loss=0.1500  mAp=84.5736\n",
      "val epoch 9 step 159, loss=0.1450  mAp=84.8392\n",
      "val epoch 9 step 179, loss=0.1397  mAp=85.1553\n",
      "val epoch 9 step 199, loss=0.1360  mAp=85.5931\n",
      "val epoch 9 step 219, loss=0.1370  mAp=85.5134\n",
      "val epoch 9 step 239, loss=0.1329  mAp=86.0327\n",
      "val epoch 9 step 259, loss=0.1326  mAp=85.9194\n",
      "val epoch 9 step 279, loss=0.1365  mAp=85.8122\n",
      "val epoch 9 step 299, loss=0.1373  mAp=85.8683\n",
      "val epoch 9 step 319, loss=0.1399  mAp=85.4843\n",
      "val epoch 9 step 339, loss=0.1422  mAp=85.5250\n",
      "val epoch 9 step 359, loss=0.1459  mAp=85.2588\n",
      "epoch 9 done!\n",
      "train epoch 10 step 49, mean loss=0.016975, mean mAp=99.0625\n",
      "train epoch 10 step 99, mean loss=0.018167, mean mAp=98.8368\n",
      "train epoch 10 step 149, mean loss=0.019431, mean mAp=98.8317\n",
      "train epoch 10 step 199, mean loss=0.022373, mean mAp=98.4768\n",
      "train epoch 10 step 249, mean loss=0.021179, mean mAp=98.5981\n",
      "train epoch 10 step 299, mean loss=0.021030, mean mAp=98.5915\n",
      "train epoch 10 step 349, mean loss=0.020135, mean mAp=98.6617\n",
      "train epoch 10 step 399, mean loss=0.019266, mean mAp=98.7717\n",
      "train epoch 10 step 449, mean loss=0.018460, mean mAp=98.8869\n",
      "train epoch 10 step 499, mean loss=0.019514, mean mAp=98.8457\n",
      "train epoch 10 step 549, mean loss=0.019512, mean mAp=98.7506\n",
      "train epoch 10 step 599, mean loss=0.018944, mean mAp=98.8082\n",
      "train epoch 10 step 649, mean loss=0.019151, mean mAp=98.8028\n",
      "train epoch 10 step 699, mean loss=0.019398, mean mAp=98.7353\n",
      "train epoch 10 step 749, mean loss=0.019639, mean mAp=98.7474\n",
      "train epoch 10 step 799, mean loss=0.019769, mean mAp=98.7553\n",
      "train epoch 10 step 849, mean loss=0.019577, mean mAp=98.7942\n",
      "train epoch 10 step 899, mean loss=0.019929, mean mAp=98.7780\n",
      "train epoch 10 step 949, mean loss=0.020172, mean mAp=98.7467\n",
      "train epoch 10 step 999, mean loss=0.020201, mean mAp=98.7060\n",
      "train epoch 10 step 1049, mean loss=0.020322, mean mAp=98.6976\n",
      "train epoch 10 step 1099, mean loss=0.020429, mean mAp=98.7108\n",
      "train epoch 10 step 1149, mean loss=0.020230, mean mAp=98.7283\n",
      "train epoch 10 step 1199, mean loss=0.020235, mean mAp=98.7327\n",
      "train epoch 10 step 1249, mean loss=0.020954, mean mAp=98.6859\n",
      "train epoch 10 step 1299, mean loss=0.021156, mean mAp=98.6706\n",
      "train epoch 10 step 1349, mean loss=0.021482, mean mAp=98.6329\n",
      "train epoch 10 step 1399, mean loss=0.021337, mean mAp=98.6450\n",
      "train epoch 10 step 1449, mean loss=0.021257, mean mAp=98.6616\n",
      "train epoch 10 step 1499, mean loss=0.021143, mean mAp=98.6618\n",
      "train epoch 10 step 1549, mean loss=0.021200, mean mAp=98.6692\n",
      "train epoch 10 step 1599, mean loss=0.021206, mean mAp=98.6722\n",
      "train epoch 10 step 1649, mean loss=0.021157, mean mAp=98.6688\n",
      "train epoch 10 step 1699, mean loss=0.021268, mean mAp=98.6523\n",
      "train epoch 10 step 1749, mean loss=0.021273, mean mAp=98.6571\n",
      "train epoch 10 step 1799, mean loss=0.021799, mean mAp=98.5951\n",
      "train epoch 10 step 1849, mean loss=0.021913, mean mAp=98.5853\n",
      "train epoch 10 step 1899, mean loss=0.022335, mean mAp=98.5189\n",
      "train epoch 10 step 1949, mean loss=0.022677, mean mAp=98.4520\n",
      "train epoch 10 step 1999, mean loss=0.022828, mean mAp=98.4555\n",
      "train epoch 10 step 2049, mean loss=0.022793, mean mAp=98.4545\n",
      "train epoch 10 step 2099, mean loss=0.022710, mean mAp=98.4665\n",
      "train epoch 10 step 2149, mean loss=0.022766, mean mAp=98.4512\n",
      "train epoch 10 step 2199, mean loss=0.022835, mean mAp=98.4125\n",
      "train epoch 10 step 2249, mean loss=0.022914, mean mAp=98.4098\n",
      "train epoch 10 step 2299, mean loss=0.022887, mean mAp=98.4188\n",
      "train epoch 10 step 2349, mean loss=0.022886, mean mAp=98.4205\n",
      "train epoch 10 step 2399, mean loss=0.022911, mean mAp=98.4001\n",
      "train epoch 10 step 2449, mean loss=0.022967, mean mAp=98.3929\n",
      "train epoch 10 step 2499, mean loss=0.023010, mean mAp=98.3898\n",
      "train epoch 10 step 2549, mean loss=0.022948, mean mAp=98.3932\n",
      "train epoch 10 step 2599, mean loss=0.022875, mean mAp=98.4033\n",
      "val epoch 10 step 19, loss=0.1921  mAp=86.5337\n",
      "val epoch 10 step 39, loss=0.1675  mAp=85.2806\n",
      "val epoch 10 step 59, loss=0.1684  mAp=83.7915\n",
      "val epoch 10 step 79, loss=0.1695  mAp=84.3477\n",
      "val epoch 10 step 99, loss=0.1598  mAp=84.8725\n",
      "val epoch 10 step 119, loss=0.1550  mAp=85.5428\n",
      "val epoch 10 step 139, loss=0.1505  mAp=85.1264\n",
      "val epoch 10 step 159, loss=0.1497  mAp=84.9052\n",
      "val epoch 10 step 179, loss=0.1467  mAp=85.2049\n",
      "val epoch 10 step 199, loss=0.1446  mAp=85.0522\n",
      "val epoch 10 step 219, loss=0.1448  mAp=85.0704\n",
      "val epoch 10 step 239, loss=0.1415  mAp=85.6742\n",
      "val epoch 10 step 259, loss=0.1417  mAp=85.9440\n",
      "val epoch 10 step 279, loss=0.1445  mAp=85.9672\n",
      "val epoch 10 step 299, loss=0.1472  mAp=86.0419\n",
      "val epoch 10 step 319, loss=0.1508  mAp=85.8614\n",
      "val epoch 10 step 339, loss=0.1525  mAp=85.8756\n",
      "val epoch 10 step 359, loss=0.1578  mAp=85.5774\n",
      "epoch 10 done!\n",
      "train epoch 11 step 49, mean loss=0.018292, mean mAp=99.3083\n",
      "train epoch 11 step 99, mean loss=0.017447, mean mAp=99.3833\n",
      "train epoch 11 step 149, mean loss=0.016931, mean mAp=99.2139\n",
      "train epoch 11 step 199, mean loss=0.017731, mean mAp=99.0708\n",
      "train epoch 11 step 249, mean loss=0.017107, mean mAp=98.9293\n",
      "train epoch 11 step 299, mean loss=0.018509, mean mAp=98.8258\n",
      "train epoch 11 step 349, mean loss=0.017703, mean mAp=98.9340\n",
      "train epoch 11 step 399, mean loss=0.016607, mean mAp=98.9561\n",
      "train epoch 11 step 449, mean loss=0.015931, mean mAp=99.0258\n",
      "train epoch 11 step 499, mean loss=0.016170, mean mAp=99.0399\n",
      "train epoch 11 step 549, mean loss=0.016904, mean mAp=98.9900\n",
      "train epoch 11 step 599, mean loss=0.017576, mean mAp=98.9461\n",
      "train epoch 11 step 649, mean loss=0.018159, mean mAp=98.8561\n",
      "train epoch 11 step 699, mean loss=0.017713, mean mAp=98.9110\n",
      "train epoch 11 step 749, mean loss=0.017567, mean mAp=98.9336\n",
      "train epoch 11 step 799, mean loss=0.017481, mean mAp=98.9388\n",
      "train epoch 11 step 849, mean loss=0.018057, mean mAp=98.8625\n",
      "train epoch 11 step 899, mean loss=0.018832, mean mAp=98.8438\n",
      "train epoch 11 step 949, mean loss=0.019275, mean mAp=98.7580\n",
      "train epoch 11 step 999, mean loss=0.019091, mean mAp=98.7876\n",
      "train epoch 11 step 1049, mean loss=0.019044, mean mAp=98.7429\n",
      "train epoch 11 step 1099, mean loss=0.019106, mean mAp=98.7376\n",
      "train epoch 11 step 1149, mean loss=0.018896, mean mAp=98.7526\n",
      "train epoch 11 step 1199, mean loss=0.019182, mean mAp=98.6635\n",
      "train epoch 11 step 1249, mean loss=0.019230, mean mAp=98.6336\n",
      "train epoch 11 step 1299, mean loss=0.018969, mean mAp=98.6813\n",
      "train epoch 11 step 1349, mean loss=0.018855, mean mAp=98.6931\n",
      "train epoch 11 step 1399, mean loss=0.018788, mean mAp=98.7087\n",
      "train epoch 11 step 1449, mean loss=0.019124, mean mAp=98.6690\n",
      "train epoch 11 step 1499, mean loss=0.019557, mean mAp=98.6219\n",
      "train epoch 11 step 1549, mean loss=0.019620, mean mAp=98.6235\n",
      "train epoch 11 step 1599, mean loss=0.019821, mean mAp=98.6126\n",
      "train epoch 11 step 1649, mean loss=0.019685, mean mAp=98.6122\n",
      "train epoch 11 step 1699, mean loss=0.019614, mean mAp=98.5981\n",
      "train epoch 11 step 1749, mean loss=0.019450, mean mAp=98.6120\n",
      "train epoch 11 step 1799, mean loss=0.019583, mean mAp=98.6167\n",
      "train epoch 11 step 1849, mean loss=0.019635, mean mAp=98.6074\n",
      "train epoch 11 step 1899, mean loss=0.019683, mean mAp=98.6012\n",
      "train epoch 11 step 1949, mean loss=0.019729, mean mAp=98.5933\n",
      "train epoch 11 step 1999, mean loss=0.019721, mean mAp=98.5993\n",
      "train epoch 11 step 2049, mean loss=0.019750, mean mAp=98.5921\n",
      "train epoch 11 step 2099, mean loss=0.019697, mean mAp=98.5973\n",
      "train epoch 11 step 2149, mean loss=0.019680, mean mAp=98.5993\n",
      "train epoch 11 step 2199, mean loss=0.019747, mean mAp=98.5813\n",
      "train epoch 11 step 2249, mean loss=0.020001, mean mAp=98.5536\n",
      "train epoch 11 step 2299, mean loss=0.020016, mean mAp=98.5534\n",
      "train epoch 11 step 2349, mean loss=0.019940, mean mAp=98.5535\n",
      "train epoch 11 step 2399, mean loss=0.020064, mean mAp=98.5328\n",
      "train epoch 11 step 2449, mean loss=0.020090, mean mAp=98.5435\n",
      "train epoch 11 step 2499, mean loss=0.020241, mean mAp=98.5336\n",
      "train epoch 11 step 2549, mean loss=0.020325, mean mAp=98.5198\n",
      "train epoch 11 step 2599, mean loss=0.020599, mean mAp=98.4889\n",
      "val epoch 11 step 19, loss=0.1635  mAp=85.9155\n",
      "val epoch 11 step 39, loss=0.1707  mAp=83.2626\n",
      "val epoch 11 step 59, loss=0.1782  mAp=82.0180\n",
      "val epoch 11 step 79, loss=0.1826  mAp=82.1431\n",
      "val epoch 11 step 99, loss=0.1786  mAp=82.7008\n",
      "val epoch 11 step 119, loss=0.1686  mAp=83.5845\n",
      "val epoch 11 step 139, loss=0.1588  mAp=83.8699\n",
      "val epoch 11 step 159, loss=0.1554  mAp=83.9415\n",
      "val epoch 11 step 179, loss=0.1483  mAp=84.3255\n",
      "val epoch 11 step 199, loss=0.1488  mAp=84.1792\n",
      "val epoch 11 step 219, loss=0.1500  mAp=84.1252\n",
      "val epoch 11 step 239, loss=0.1463  mAp=84.6748\n",
      "val epoch 11 step 259, loss=0.1453  mAp=84.8040\n",
      "val epoch 11 step 279, loss=0.1490  mAp=84.9412\n",
      "val epoch 11 step 299, loss=0.1499  mAp=84.9420\n",
      "val epoch 11 step 319, loss=0.1524  mAp=84.6388\n",
      "val epoch 11 step 339, loss=0.1537  mAp=84.5683\n",
      "val epoch 11 step 359, loss=0.1571  mAp=84.2766\n",
      "epoch 11 done!\n",
      "train epoch 12 step 49, mean loss=0.008720, mean mAp=99.4167\n",
      "train epoch 12 step 99, mean loss=0.009950, mean mAp=99.5208\n",
      "train epoch 12 step 149, mean loss=0.011679, mean mAp=99.2667\n",
      "train epoch 12 step 199, mean loss=0.014637, mean mAp=98.9264\n",
      "train epoch 12 step 249, mean loss=0.014344, mean mAp=98.9554\n",
      "train epoch 12 step 299, mean loss=0.013628, mean mAp=99.0878\n",
      "train epoch 12 step 349, mean loss=0.013988, mean mAp=99.0658\n",
      "train epoch 12 step 399, mean loss=0.015510, mean mAp=98.9555\n",
      "train epoch 12 step 449, mean loss=0.015682, mean mAp=98.9650\n",
      "train epoch 12 step 499, mean loss=0.015515, mean mAp=98.9644\n",
      "train epoch 12 step 549, mean loss=0.015226, mean mAp=98.9979\n",
      "train epoch 12 step 599, mean loss=0.014675, mean mAp=99.0814\n",
      "train epoch 12 step 649, mean loss=0.015180, mean mAp=99.0784\n",
      "train epoch 12 step 699, mean loss=0.015607, mean mAp=99.0162\n",
      "train epoch 12 step 749, mean loss=0.015540, mean mAp=99.0429\n",
      "train epoch 12 step 799, mean loss=0.015695, mean mAp=99.0048\n",
      "train epoch 12 step 849, mean loss=0.015791, mean mAp=98.9426\n",
      "train epoch 12 step 899, mean loss=0.015927, mean mAp=98.9457\n",
      "train epoch 12 step 949, mean loss=0.016008, mean mAp=98.9385\n",
      "train epoch 12 step 999, mean loss=0.016103, mean mAp=98.8937\n",
      "train epoch 12 step 1049, mean loss=0.016152, mean mAp=98.8884\n",
      "train epoch 12 step 1099, mean loss=0.016175, mean mAp=98.8651\n",
      "train epoch 12 step 1149, mean loss=0.016166, mean mAp=98.8583\n",
      "train epoch 12 step 1199, mean loss=0.016504, mean mAp=98.8642\n",
      "train epoch 12 step 1249, mean loss=0.017015, mean mAp=98.8087\n",
      "train epoch 12 step 1299, mean loss=0.017103, mean mAp=98.7815\n",
      "train epoch 12 step 1349, mean loss=0.017345, mean mAp=98.7372\n",
      "train epoch 12 step 1399, mean loss=0.017651, mean mAp=98.6632\n",
      "train epoch 12 step 1449, mean loss=0.017742, mean mAp=98.6288\n",
      "train epoch 12 step 1499, mean loss=0.018097, mean mAp=98.5930\n",
      "train epoch 12 step 1549, mean loss=0.018067, mean mAp=98.5793\n",
      "train epoch 12 step 1599, mean loss=0.018222, mean mAp=98.5664\n",
      "train epoch 12 step 1649, mean loss=0.018182, mean mAp=98.5960\n",
      "train epoch 12 step 1699, mean loss=0.018193, mean mAp=98.5957\n",
      "train epoch 12 step 1749, mean loss=0.018561, mean mAp=98.5668\n",
      "train epoch 12 step 1799, mean loss=0.018647, mean mAp=98.5492\n",
      "train epoch 12 step 1849, mean loss=0.018620, mean mAp=98.5500\n",
      "train epoch 12 step 1899, mean loss=0.018634, mean mAp=98.5584\n",
      "train epoch 12 step 1949, mean loss=0.018654, mean mAp=98.5441\n",
      "train epoch 12 step 1999, mean loss=0.018614, mean mAp=98.5638\n",
      "train epoch 12 step 2049, mean loss=0.018682, mean mAp=98.5613\n",
      "train epoch 12 step 2099, mean loss=0.018805, mean mAp=98.5700\n",
      "train epoch 12 step 2149, mean loss=0.018787, mean mAp=98.5604\n",
      "train epoch 12 step 2199, mean loss=0.018822, mean mAp=98.5429\n",
      "train epoch 12 step 2249, mean loss=0.018640, mean mAp=98.5639\n",
      "train epoch 12 step 2299, mean loss=0.018626, mean mAp=98.5574\n",
      "train epoch 12 step 2349, mean loss=0.018953, mean mAp=98.5457\n",
      "train epoch 12 step 2399, mean loss=0.019192, mean mAp=98.4904\n",
      "train epoch 12 step 2449, mean loss=0.019434, mean mAp=98.4702\n",
      "train epoch 12 step 2499, mean loss=0.019394, mean mAp=98.4698\n",
      "train epoch 12 step 2549, mean loss=0.019273, mean mAp=98.4949\n",
      "train epoch 12 step 2599, mean loss=0.019199, mean mAp=98.4953\n",
      "val epoch 12 step 19, loss=0.2225  mAp=85.1796\n",
      "val epoch 12 step 39, loss=0.2024  mAp=83.0921\n",
      "val epoch 12 step 59, loss=0.1954  mAp=83.4607\n",
      "val epoch 12 step 79, loss=0.2060  mAp=83.2220\n",
      "val epoch 12 step 99, loss=0.1921  mAp=83.9904\n",
      "val epoch 12 step 119, loss=0.1804  mAp=84.0021\n",
      "val epoch 12 step 139, loss=0.1728  mAp=83.9331\n",
      "val epoch 12 step 159, loss=0.1732  mAp=84.2905\n",
      "val epoch 12 step 179, loss=0.1662  mAp=84.5092\n",
      "val epoch 12 step 199, loss=0.1655  mAp=84.3565\n",
      "val epoch 12 step 219, loss=0.1674  mAp=84.3276\n",
      "val epoch 12 step 239, loss=0.1628  mAp=84.7954\n",
      "val epoch 12 step 259, loss=0.1606  mAp=85.2121\n",
      "val epoch 12 step 279, loss=0.1649  mAp=85.1194\n",
      "val epoch 12 step 299, loss=0.1665  mAp=85.3482\n",
      "val epoch 12 step 319, loss=0.1709  mAp=85.1769\n",
      "val epoch 12 step 339, loss=0.1732  mAp=84.8964\n",
      "val epoch 12 step 359, loss=0.1788  mAp=84.6038\n",
      "epoch 12 done!\n",
      "train epoch 13 step 49, mean loss=0.010964, mean mAp=99.5333\n",
      "train epoch 13 step 99, mean loss=0.011248, mean mAp=99.4333\n",
      "train epoch 13 step 149, mean loss=0.011247, mean mAp=99.3861\n",
      "train epoch 13 step 199, mean loss=0.011474, mean mAp=99.5229\n",
      "train epoch 13 step 249, mean loss=0.012200, mean mAp=99.5592\n",
      "train epoch 13 step 299, mean loss=0.011354, mean mAp=99.5354\n",
      "train epoch 13 step 349, mean loss=0.011319, mean mAp=99.4292\n",
      "train epoch 13 step 399, mean loss=0.011894, mean mAp=99.4155\n",
      "train epoch 13 step 449, mean loss=0.012588, mean mAp=99.4313\n",
      "train epoch 13 step 499, mean loss=0.012575, mean mAp=99.4674\n",
      "train epoch 13 step 549, mean loss=0.012248, mean mAp=99.4832\n",
      "train epoch 13 step 599, mean loss=0.012738, mean mAp=99.4304\n",
      "train epoch 13 step 649, mean loss=0.014242, mean mAp=99.3153\n",
      "train epoch 13 step 699, mean loss=0.015350, mean mAp=99.1276\n",
      "train epoch 13 step 749, mean loss=0.015144, mean mAp=99.1024\n",
      "train epoch 13 step 799, mean loss=0.015069, mean mAp=99.1022\n",
      "train epoch 13 step 849, mean loss=0.014557, mean mAp=99.1477\n",
      "train epoch 13 step 899, mean loss=0.014682, mean mAp=99.1372\n",
      "train epoch 13 step 949, mean loss=0.015386, mean mAp=99.0583\n",
      "train epoch 13 step 999, mean loss=0.015655, mean mAp=99.0262\n",
      "train epoch 13 step 1049, mean loss=0.015547, mean mAp=99.0151\n",
      "train epoch 13 step 1099, mean loss=0.015753, mean mAp=99.0257\n",
      "train epoch 13 step 1149, mean loss=0.015986, mean mAp=99.0292\n",
      "train epoch 13 step 1199, mean loss=0.016085, mean mAp=98.9956\n",
      "train epoch 13 step 1249, mean loss=0.016375, mean mAp=98.9765\n",
      "train epoch 13 step 1299, mean loss=0.016248, mean mAp=98.9918\n",
      "train epoch 13 step 1349, mean loss=0.016377, mean mAp=98.9680\n",
      "train epoch 13 step 1399, mean loss=0.016681, mean mAp=98.9013\n",
      "train epoch 13 step 1449, mean loss=0.016691, mean mAp=98.9148\n",
      "train epoch 13 step 1499, mean loss=0.016708, mean mAp=98.9051\n",
      "train epoch 13 step 1549, mean loss=0.017063, mean mAp=98.8881\n",
      "train epoch 13 step 1599, mean loss=0.017261, mean mAp=98.8661\n",
      "train epoch 13 step 1649, mean loss=0.017471, mean mAp=98.8777\n",
      "train epoch 13 step 1699, mean loss=0.017876, mean mAp=98.8455\n",
      "train epoch 13 step 1749, mean loss=0.017780, mean mAp=98.8523\n",
      "train epoch 13 step 1799, mean loss=0.017689, mean mAp=98.8472\n",
      "train epoch 13 step 1849, mean loss=0.017594, mean mAp=98.8424\n",
      "train epoch 13 step 1899, mean loss=0.017638, mean mAp=98.8284\n",
      "train epoch 13 step 1949, mean loss=0.017645, mean mAp=98.8413\n",
      "train epoch 13 step 1999, mean loss=0.017458, mean mAp=98.8616\n",
      "train epoch 13 step 2049, mean loss=0.017456, mean mAp=98.8542\n",
      "train epoch 13 step 2099, mean loss=0.017472, mean mAp=98.8289\n",
      "train epoch 13 step 2149, mean loss=0.017480, mean mAp=98.8151\n",
      "train epoch 13 step 2199, mean loss=0.017483, mean mAp=98.8064\n",
      "train epoch 13 step 2249, mean loss=0.017784, mean mAp=98.7750\n",
      "train epoch 13 step 2299, mean loss=0.018262, mean mAp=98.7034\n",
      "train epoch 13 step 2349, mean loss=0.018341, mean mAp=98.7114\n",
      "train epoch 13 step 2399, mean loss=0.018383, mean mAp=98.7000\n",
      "train epoch 13 step 2449, mean loss=0.018279, mean mAp=98.7119\n",
      "train epoch 13 step 2499, mean loss=0.018373, mean mAp=98.7216\n",
      "train epoch 13 step 2549, mean loss=0.018276, mean mAp=98.7295\n",
      "train epoch 13 step 2599, mean loss=0.018115, mean mAp=98.7523\n",
      "val epoch 13 step 19, loss=0.1751  mAp=85.6523\n",
      "val epoch 13 step 39, loss=0.1656  mAp=86.2969\n",
      "val epoch 13 step 59, loss=0.1614  mAp=84.9352\n",
      "val epoch 13 step 79, loss=0.1649  mAp=85.4972\n",
      "val epoch 13 step 99, loss=0.1579  mAp=86.4436\n",
      "val epoch 13 step 119, loss=0.1523  mAp=87.2837\n",
      "val epoch 13 step 139, loss=0.1486  mAp=86.7243\n",
      "val epoch 13 step 159, loss=0.1542  mAp=86.4565\n",
      "val epoch 13 step 179, loss=0.1514  mAp=86.4439\n",
      "val epoch 13 step 199, loss=0.1503  mAp=86.5904\n",
      "val epoch 13 step 219, loss=0.1532  mAp=86.5236\n",
      "val epoch 13 step 239, loss=0.1502  mAp=86.8305\n",
      "val epoch 13 step 259, loss=0.1495  mAp=86.7943\n",
      "val epoch 13 step 279, loss=0.1564  mAp=86.8269\n",
      "val epoch 13 step 299, loss=0.1610  mAp=86.7408\n",
      "val epoch 13 step 319, loss=0.1645  mAp=86.6710\n",
      "val epoch 13 step 339, loss=0.1678  mAp=86.5293\n",
      "val epoch 13 step 359, loss=0.1737  mAp=86.1466\n",
      "epoch 13 done!\n",
      "train epoch 14 step 49, mean loss=0.008097, mean mAp=99.7500\n",
      "train epoch 14 step 99, mean loss=0.010920, mean mAp=99.3000\n",
      "train epoch 14 step 149, mean loss=0.015091, mean mAp=98.8667\n",
      "train epoch 14 step 199, mean loss=0.015554, mean mAp=98.7896\n",
      "train epoch 14 step 249, mean loss=0.015899, mean mAp=98.8900\n",
      "train epoch 14 step 299, mean loss=0.015727, mean mAp=98.9708\n",
      "train epoch 14 step 349, mean loss=0.015486, mean mAp=99.0155\n",
      "train epoch 14 step 399, mean loss=0.014842, mean mAp=99.0312\n",
      "train epoch 14 step 449, mean loss=0.015303, mean mAp=98.9556\n",
      "train epoch 14 step 499, mean loss=0.015047, mean mAp=98.9100\n",
      "train epoch 14 step 549, mean loss=0.014944, mean mAp=98.9504\n",
      "train epoch 14 step 599, mean loss=0.014971, mean mAp=98.9360\n",
      "train epoch 14 step 649, mean loss=0.014588, mean mAp=98.9922\n",
      "train epoch 14 step 699, mean loss=0.014592, mean mAp=99.0463\n",
      "train epoch 14 step 749, mean loss=0.014494, mean mAp=99.0794\n",
      "train epoch 14 step 799, mean loss=0.014222, mean mAp=99.1109\n",
      "train epoch 14 step 849, mean loss=0.014143, mean mAp=99.1141\n",
      "train epoch 14 step 899, mean loss=0.014285, mean mAp=99.0939\n",
      "train epoch 14 step 949, mean loss=0.015160, mean mAp=98.9509\n",
      "train epoch 14 step 999, mean loss=0.015794, mean mAp=98.8667\n",
      "train epoch 14 step 1049, mean loss=0.016128, mean mAp=98.8447\n",
      "train epoch 14 step 1099, mean loss=0.016196, mean mAp=98.8252\n",
      "train epoch 14 step 1149, mean loss=0.016349, mean mAp=98.8234\n",
      "train epoch 14 step 1199, mean loss=0.016417, mean mAp=98.8158\n",
      "train epoch 14 step 1249, mean loss=0.016847, mean mAp=98.7645\n",
      "train epoch 14 step 1299, mean loss=0.016904, mean mAp=98.7371\n",
      "train epoch 14 step 1349, mean loss=0.016867, mean mAp=98.7459\n",
      "train epoch 14 step 1399, mean loss=0.016746, mean mAp=98.7639\n",
      "train epoch 14 step 1449, mean loss=0.016684, mean mAp=98.7738\n",
      "train epoch 14 step 1499, mean loss=0.016426, mean mAp=98.7947\n",
      "train epoch 14 step 1549, mean loss=0.016321, mean mAp=98.8117\n",
      "train epoch 14 step 1599, mean loss=0.016355, mean mAp=98.8241\n",
      "train epoch 14 step 1649, mean loss=0.016441, mean mAp=98.8143\n",
      "train epoch 14 step 1699, mean loss=0.016207, mean mAp=98.8467\n",
      "train epoch 14 step 1749, mean loss=0.016256, mean mAp=98.8630\n",
      "train epoch 14 step 1799, mean loss=0.016080, mean mAp=98.8823\n",
      "train epoch 14 step 1849, mean loss=0.016240, mean mAp=98.8765\n",
      "train epoch 14 step 1899, mean loss=0.016265, mean mAp=98.8476\n",
      "train epoch 14 step 1949, mean loss=0.016205, mean mAp=98.8419\n",
      "train epoch 14 step 1999, mean loss=0.016120, mean mAp=98.8421\n",
      "train epoch 14 step 2049, mean loss=0.016295, mean mAp=98.8402\n",
      "train epoch 14 step 2099, mean loss=0.016347, mean mAp=98.8369\n",
      "train epoch 14 step 2149, mean loss=0.016312, mean mAp=98.8465\n",
      "train epoch 14 step 2199, mean loss=0.016376, mean mAp=98.8490\n",
      "train epoch 14 step 2249, mean loss=0.016557, mean mAp=98.8283\n",
      "train epoch 14 step 2299, mean loss=0.016503, mean mAp=98.8248\n",
      "train epoch 14 step 2349, mean loss=0.016602, mean mAp=98.8158\n",
      "train epoch 14 step 2399, mean loss=0.016681, mean mAp=98.7867\n",
      "train epoch 14 step 2449, mean loss=0.016975, mean mAp=98.7782\n",
      "train epoch 14 step 2499, mean loss=0.016942, mean mAp=98.7843\n",
      "train epoch 14 step 2549, mean loss=0.016891, mean mAp=98.7804\n",
      "train epoch 14 step 2599, mean loss=0.016944, mean mAp=98.7782\n",
      "val epoch 14 step 19, loss=0.2003  mAp=85.7564\n",
      "val epoch 14 step 39, loss=0.1945  mAp=82.6565\n",
      "val epoch 14 step 59, loss=0.2079  mAp=81.2742\n",
      "val epoch 14 step 79, loss=0.2107  mAp=81.4735\n",
      "val epoch 14 step 99, loss=0.1952  mAp=82.2063\n",
      "val epoch 14 step 119, loss=0.1904  mAp=82.5596\n",
      "val epoch 14 step 139, loss=0.1817  mAp=82.3607\n",
      "val epoch 14 step 159, loss=0.1779  mAp=82.4945\n",
      "val epoch 14 step 179, loss=0.1742  mAp=82.8943\n",
      "val epoch 14 step 199, loss=0.1731  mAp=82.7267\n",
      "val epoch 14 step 219, loss=0.1727  mAp=82.8995\n",
      "val epoch 14 step 239, loss=0.1683  mAp=83.6054\n",
      "val epoch 14 step 259, loss=0.1673  mAp=83.5939\n",
      "val epoch 14 step 279, loss=0.1708  mAp=83.6894\n",
      "val epoch 14 step 299, loss=0.1736  mAp=83.6572\n",
      "val epoch 14 step 319, loss=0.1763  mAp=83.3614\n",
      "val epoch 14 step 339, loss=0.1782  mAp=83.3429\n",
      "val epoch 14 step 359, loss=0.1827  mAp=83.1661\n",
      "epoch 14 done!\n",
      "train epoch 15 step 49, mean loss=0.014353, mean mAp=98.1042\n",
      "train epoch 15 step 99, mean loss=0.012282, mean mAp=98.5521\n",
      "train epoch 15 step 149, mean loss=0.010604, mean mAp=98.8681\n",
      "train epoch 15 step 199, mean loss=0.009444, mean mAp=99.0885\n",
      "train epoch 15 step 249, mean loss=0.010106, mean mAp=99.1958\n",
      "train epoch 15 step 299, mean loss=0.010874, mean mAp=99.0938\n",
      "train epoch 15 step 349, mean loss=0.010433, mean mAp=99.1661\n",
      "train epoch 15 step 399, mean loss=0.011043, mean mAp=99.1214\n",
      "train epoch 15 step 449, mean loss=0.011847, mean mAp=99.1171\n",
      "train epoch 15 step 499, mean loss=0.013154, mean mAp=99.0304\n",
      "train epoch 15 step 549, mean loss=0.013784, mean mAp=98.9822\n",
      "train epoch 15 step 599, mean loss=0.013376, mean mAp=99.0531\n",
      "train epoch 15 step 649, mean loss=0.013553, mean mAp=99.0266\n",
      "train epoch 15 step 699, mean loss=0.013536, mean mAp=99.0307\n",
      "train epoch 15 step 749, mean loss=0.013610, mean mAp=99.0742\n",
      "train epoch 15 step 799, mean loss=0.013482, mean mAp=99.0956\n",
      "train epoch 15 step 849, mean loss=0.013407, mean mAp=99.1194\n",
      "train epoch 15 step 899, mean loss=0.013668, mean mAp=99.1312\n",
      "train epoch 15 step 949, mean loss=0.013424, mean mAp=99.1638\n",
      "train epoch 15 step 999, mean loss=0.013434, mean mAp=99.1598\n",
      "train epoch 15 step 1049, mean loss=0.013387, mean mAp=99.1450\n",
      "train epoch 15 step 1099, mean loss=0.013830, mean mAp=99.0854\n",
      "train epoch 15 step 1149, mean loss=0.014167, mean mAp=99.0498\n",
      "train epoch 15 step 1199, mean loss=0.014283, mean mAp=99.0211\n",
      "train epoch 15 step 1249, mean loss=0.014574, mean mAp=98.9931\n",
      "train epoch 15 step 1299, mean loss=0.014771, mean mAp=98.9678\n",
      "train epoch 15 step 1349, mean loss=0.014755, mean mAp=98.9535\n",
      "train epoch 15 step 1399, mean loss=0.014993, mean mAp=98.9123\n",
      "train epoch 15 step 1449, mean loss=0.015000, mean mAp=98.9383\n",
      "train epoch 15 step 1499, mean loss=0.014936, mean mAp=98.9598\n",
      "train epoch 15 step 1549, mean loss=0.014782, mean mAp=98.9813\n",
      "train epoch 15 step 1599, mean loss=0.014750, mean mAp=98.9871\n",
      "train epoch 15 step 1649, mean loss=0.014732, mean mAp=98.9720\n",
      "train epoch 15 step 1699, mean loss=0.014975, mean mAp=98.9516\n",
      "train epoch 15 step 1749, mean loss=0.014936, mean mAp=98.9637\n",
      "train epoch 15 step 1799, mean loss=0.014987, mean mAp=98.9439\n",
      "train epoch 15 step 1849, mean loss=0.014877, mean mAp=98.9600\n",
      "train epoch 15 step 1899, mean loss=0.014767, mean mAp=98.9808\n",
      "train epoch 15 step 1949, mean loss=0.014829, mean mAp=98.9700\n",
      "train epoch 15 step 1999, mean loss=0.014804, mean mAp=98.9676\n",
      "train epoch 15 step 2049, mean loss=0.014779, mean mAp=98.9658\n",
      "train epoch 15 step 2099, mean loss=0.014609, mean mAp=98.9844\n",
      "train epoch 15 step 2149, mean loss=0.014395, mean mAp=99.0081\n",
      "train epoch 15 step 2199, mean loss=0.014329, mean mAp=99.0230\n",
      "train epoch 15 step 2249, mean loss=0.014362, mean mAp=99.0095\n",
      "train epoch 15 step 2299, mean loss=0.014400, mean mAp=99.0169\n",
      "train epoch 15 step 2349, mean loss=0.014296, mean mAp=99.0379\n",
      "train epoch 15 step 2399, mean loss=0.014216, mean mAp=99.0470\n",
      "train epoch 15 step 2449, mean loss=0.014606, mean mAp=99.0127\n",
      "train epoch 15 step 2499, mean loss=0.014665, mean mAp=99.0058\n",
      "train epoch 15 step 2549, mean loss=0.014789, mean mAp=98.9975\n",
      "train epoch 15 step 2599, mean loss=0.014920, mean mAp=98.9749\n",
      "val epoch 15 step 19, loss=0.2534  mAp=83.1258\n",
      "val epoch 15 step 39, loss=0.2145  mAp=81.8716\n",
      "val epoch 15 step 59, loss=0.2200  mAp=81.8098\n",
      "val epoch 15 step 79, loss=0.2292  mAp=81.0390\n",
      "val epoch 15 step 99, loss=0.2145  mAp=82.0549\n",
      "val epoch 15 step 119, loss=0.2005  mAp=82.7123\n",
      "val epoch 15 step 139, loss=0.1922  mAp=82.4181\n",
      "val epoch 15 step 159, loss=0.1939  mAp=82.1798\n",
      "val epoch 15 step 179, loss=0.1878  mAp=82.1156\n",
      "val epoch 15 step 199, loss=0.1826  mAp=82.4224\n",
      "val epoch 15 step 219, loss=0.1851  mAp=82.2293\n",
      "val epoch 15 step 239, loss=0.1825  mAp=82.5227\n",
      "val epoch 15 step 259, loss=0.1806  mAp=82.8905\n",
      "val epoch 15 step 279, loss=0.1830  mAp=83.1175\n",
      "val epoch 15 step 299, loss=0.1851  mAp=83.3051\n",
      "val epoch 15 step 319, loss=0.1907  mAp=82.9257\n",
      "val epoch 15 step 339, loss=0.1947  mAp=82.8910\n",
      "val epoch 15 step 359, loss=0.2008  mAp=82.4357\n",
      "epoch 15 done!\n",
      "train epoch 16 step 49, mean loss=0.009498, mean mAp=99.7500\n",
      "train epoch 16 step 99, mean loss=0.015693, mean mAp=99.1854\n",
      "train epoch 16 step 149, mean loss=0.012746, mean mAp=99.3181\n",
      "train epoch 16 step 199, mean loss=0.012694, mean mAp=99.2594\n",
      "train epoch 16 step 249, mean loss=0.012665, mean mAp=99.3825\n",
      "train epoch 16 step 299, mean loss=0.011497, mean mAp=99.4438\n",
      "train epoch 16 step 349, mean loss=0.010869, mean mAp=99.4280\n",
      "train epoch 16 step 399, mean loss=0.010957, mean mAp=99.4266\n",
      "train epoch 16 step 449, mean loss=0.010851, mean mAp=99.4532\n",
      "train epoch 16 step 499, mean loss=0.010700, mean mAp=99.4163\n",
      "train epoch 16 step 549, mean loss=0.011404, mean mAp=99.3797\n",
      "train epoch 16 step 599, mean loss=0.011136, mean mAp=99.4140\n",
      "train epoch 16 step 649, mean loss=0.010885, mean mAp=99.4206\n",
      "train epoch 16 step 699, mean loss=0.010765, mean mAp=99.3668\n",
      "train epoch 16 step 749, mean loss=0.011644, mean mAp=99.2887\n",
      "train epoch 16 step 799, mean loss=0.012095, mean mAp=99.2811\n",
      "train epoch 16 step 849, mean loss=0.012631, mean mAp=99.2656\n",
      "train epoch 16 step 899, mean loss=0.012930, mean mAp=99.2508\n",
      "train epoch 16 step 949, mean loss=0.013176, mean mAp=99.2238\n",
      "train epoch 16 step 999, mean loss=0.013427, mean mAp=99.2092\n",
      "train epoch 16 step 1049, mean loss=0.013461, mean mAp=99.2072\n",
      "train epoch 16 step 1099, mean loss=0.013289, mean mAp=99.2034\n",
      "train epoch 16 step 1149, mean loss=0.013110, mean mAp=99.1964\n",
      "train epoch 16 step 1199, mean loss=0.013556, mean mAp=99.1544\n",
      "train epoch 16 step 1249, mean loss=0.013821, mean mAp=99.0899\n",
      "train epoch 16 step 1299, mean loss=0.014136, mean mAp=99.0426\n",
      "train epoch 16 step 1349, mean loss=0.014469, mean mAp=99.0068\n",
      "train epoch 16 step 1399, mean loss=0.014514, mean mAp=99.0184\n",
      "train epoch 16 step 1449, mean loss=0.014662, mean mAp=99.0255\n",
      "train epoch 16 step 1499, mean loss=0.014982, mean mAp=98.9968\n",
      "train epoch 16 step 1549, mean loss=0.015391, mean mAp=98.9508\n",
      "train epoch 16 step 1599, mean loss=0.015325, mean mAp=98.9601\n",
      "train epoch 16 step 1649, mean loss=0.015643, mean mAp=98.9319\n",
      "train epoch 16 step 1699, mean loss=0.016048, mean mAp=98.8945\n",
      "train epoch 16 step 1749, mean loss=0.016226, mean mAp=98.8626\n",
      "train epoch 16 step 1799, mean loss=0.016321, mean mAp=98.8444\n",
      "train epoch 16 step 1849, mean loss=0.016216, mean mAp=98.8637\n",
      "train epoch 16 step 1899, mean loss=0.016183, mean mAp=98.8651\n",
      "train epoch 16 step 1949, mean loss=0.016188, mean mAp=98.8604\n",
      "train epoch 16 step 1999, mean loss=0.016156, mean mAp=98.8784\n",
      "train epoch 16 step 2049, mean loss=0.016117, mean mAp=98.8861\n",
      "train epoch 16 step 2099, mean loss=0.016038, mean mAp=98.8918\n",
      "train epoch 16 step 2149, mean loss=0.016233, mean mAp=98.8931\n",
      "train epoch 16 step 2199, mean loss=0.016193, mean mAp=98.8955\n",
      "train epoch 16 step 2249, mean loss=0.016091, mean mAp=98.9043\n",
      "train epoch 16 step 2299, mean loss=0.016039, mean mAp=98.9082\n",
      "train epoch 16 step 2349, mean loss=0.015875, mean mAp=98.9261\n",
      "train epoch 16 step 2399, mean loss=0.016204, mean mAp=98.8893\n",
      "train epoch 16 step 2449, mean loss=0.016291, mean mAp=98.8638\n",
      "train epoch 16 step 2499, mean loss=0.016261, mean mAp=98.8685\n",
      "train epoch 16 step 2549, mean loss=0.016161, mean mAp=98.8739\n",
      "train epoch 16 step 2599, mean loss=0.016109, mean mAp=98.8891\n",
      "val epoch 16 step 19, loss=0.2372  mAp=86.5010\n",
      "val epoch 16 step 39, loss=0.2244  mAp=83.5615\n",
      "val epoch 16 step 59, loss=0.2435  mAp=81.5271\n",
      "val epoch 16 step 79, loss=0.2426  mAp=81.9274\n",
      "val epoch 16 step 99, loss=0.2251  mAp=82.5613\n",
      "val epoch 16 step 119, loss=0.2133  mAp=83.7785\n",
      "val epoch 16 step 139, loss=0.2048  mAp=83.5433\n",
      "val epoch 16 step 159, loss=0.2011  mAp=83.5696\n",
      "val epoch 16 step 179, loss=0.1958  mAp=83.6913\n",
      "val epoch 16 step 199, loss=0.1953  mAp=83.6365\n",
      "val epoch 16 step 219, loss=0.1953  mAp=83.5769\n",
      "val epoch 16 step 239, loss=0.1913  mAp=83.9699\n",
      "val epoch 16 step 259, loss=0.1912  mAp=84.1272\n",
      "val epoch 16 step 279, loss=0.1947  mAp=84.3075\n",
      "val epoch 16 step 299, loss=0.1962  mAp=84.3191\n",
      "val epoch 16 step 319, loss=0.2032  mAp=84.1340\n",
      "val epoch 16 step 339, loss=0.2072  mAp=83.9374\n",
      "val epoch 16 step 359, loss=0.2141  mAp=83.6685\n",
      "epoch 16 done!\n",
      "train epoch 17 step 49, mean loss=0.007165, mean mAp=99.7917\n",
      "train epoch 17 step 99, mean loss=0.010124, mean mAp=99.6389\n",
      "train epoch 17 step 149, mean loss=0.010136, mean mAp=99.7593\n",
      "train epoch 17 step 199, mean loss=0.009939, mean mAp=99.7986\n",
      "train epoch 17 step 249, mean loss=0.011870, mean mAp=99.6556\n",
      "train epoch 17 step 299, mean loss=0.012480, mean mAp=99.5810\n",
      "train epoch 17 step 349, mean loss=0.012675, mean mAp=99.4087\n",
      "train epoch 17 step 399, mean loss=0.013040, mean mAp=99.3993\n",
      "train epoch 17 step 449, mean loss=0.013260, mean mAp=99.4105\n",
      "train epoch 17 step 499, mean loss=0.013199, mean mAp=99.2694\n",
      "train epoch 17 step 549, mean loss=0.014205, mean mAp=99.2237\n",
      "train epoch 17 step 599, mean loss=0.014751, mean mAp=99.1347\n",
      "train epoch 17 step 649, mean loss=0.014308, mean mAp=99.1692\n",
      "train epoch 17 step 699, mean loss=0.014202, mean mAp=99.1184\n",
      "train epoch 17 step 749, mean loss=0.014197, mean mAp=99.1161\n",
      "train epoch 17 step 799, mean loss=0.013733, mean mAp=99.1557\n",
      "train epoch 17 step 849, mean loss=0.013234, mean mAp=99.1980\n",
      "train epoch 17 step 899, mean loss=0.013045, mean mAp=99.2194\n",
      "train epoch 17 step 949, mean loss=0.012901, mean mAp=99.2561\n",
      "train epoch 17 step 999, mean loss=0.012754, mean mAp=99.2558\n",
      "train epoch 17 step 1049, mean loss=0.012597, mean mAp=99.2411\n",
      "train epoch 17 step 1099, mean loss=0.012366, mean mAp=99.2642\n",
      "train epoch 17 step 1149, mean loss=0.012260, mean mAp=99.2853\n",
      "train epoch 17 step 1199, mean loss=0.012333, mean mAp=99.2856\n",
      "train epoch 17 step 1249, mean loss=0.012148, mean mAp=99.2870\n",
      "train epoch 17 step 1299, mean loss=0.012252, mean mAp=99.3112\n",
      "train epoch 17 step 1349, mean loss=0.012048, mean mAp=99.3337\n",
      "train epoch 17 step 1399, mean loss=0.012012, mean mAp=99.3277\n",
      "train epoch 17 step 1449, mean loss=0.012506, mean mAp=99.2851\n",
      "train epoch 17 step 1499, mean loss=0.012487, mean mAp=99.2547\n",
      "train epoch 17 step 1549, mean loss=0.012548, mean mAp=99.2411\n",
      "train epoch 17 step 1599, mean loss=0.012731, mean mAp=99.2158\n",
      "train epoch 17 step 1649, mean loss=0.012726, mean mAp=99.2244\n",
      "train epoch 17 step 1699, mean loss=0.012817, mean mAp=99.2306\n",
      "train epoch 17 step 1749, mean loss=0.012746, mean mAp=99.2418\n",
      "train epoch 17 step 1799, mean loss=0.012496, mean mAp=99.2606\n",
      "train epoch 17 step 1849, mean loss=0.012644, mean mAp=99.2484\n",
      "train epoch 17 step 1899, mean loss=0.012566, mean mAp=99.2363\n",
      "train epoch 17 step 1949, mean loss=0.012500, mean mAp=99.2478\n",
      "train epoch 17 step 1999, mean loss=0.012982, mean mAp=99.1822\n",
      "train epoch 17 step 2049, mean loss=0.013143, mean mAp=99.1829\n",
      "train epoch 17 step 2099, mean loss=0.013130, mean mAp=99.1944\n",
      "train epoch 17 step 2149, mean loss=0.013109, mean mAp=99.2112\n",
      "train epoch 17 step 2199, mean loss=0.013236, mean mAp=99.2013\n",
      "train epoch 17 step 2249, mean loss=0.013188, mean mAp=99.2015\n",
      "train epoch 17 step 2299, mean loss=0.013420, mean mAp=99.1971\n",
      "train epoch 17 step 2349, mean loss=0.013416, mean mAp=99.1929\n",
      "train epoch 17 step 2399, mean loss=0.013461, mean mAp=99.1880\n",
      "train epoch 17 step 2449, mean loss=0.013546, mean mAp=99.1706\n",
      "train epoch 17 step 2499, mean loss=0.013710, mean mAp=99.1425\n",
      "train epoch 17 step 2549, mean loss=0.013749, mean mAp=99.1479\n",
      "train epoch 17 step 2599, mean loss=0.013665, mean mAp=99.1624\n",
      "val epoch 17 step 19, loss=0.2370  mAp=85.9294\n",
      "val epoch 17 step 39, loss=0.2187  mAp=84.1722\n",
      "val epoch 17 step 59, loss=0.2177  mAp=83.6614\n",
      "val epoch 17 step 79, loss=0.2241  mAp=83.0059\n",
      "val epoch 17 step 99, loss=0.2145  mAp=83.8215\n",
      "val epoch 17 step 119, loss=0.1985  mAp=85.1699\n",
      "val epoch 17 step 139, loss=0.1900  mAp=84.7098\n",
      "val epoch 17 step 159, loss=0.1888  mAp=84.5396\n",
      "val epoch 17 step 179, loss=0.1828  mAp=84.5987\n",
      "val epoch 17 step 199, loss=0.1799  mAp=84.7216\n",
      "val epoch 17 step 219, loss=0.1849  mAp=84.7245\n",
      "val epoch 17 step 239, loss=0.1833  mAp=84.9532\n",
      "val epoch 17 step 259, loss=0.1811  mAp=85.1562\n",
      "val epoch 17 step 279, loss=0.1873  mAp=85.3466\n",
      "val epoch 17 step 299, loss=0.1895  mAp=85.2877\n",
      "val epoch 17 step 319, loss=0.1959  mAp=84.8292\n",
      "val epoch 17 step 339, loss=0.1999  mAp=84.7449\n",
      "val epoch 17 step 359, loss=0.2053  mAp=84.2398\n",
      "epoch 17 done!\n",
      "train epoch 18 step 49, mean loss=0.015170, mean mAp=99.1667\n",
      "train epoch 18 step 99, mean loss=0.011232, mean mAp=99.4167\n",
      "train epoch 18 step 149, mean loss=0.010365, mean mAp=99.5972\n",
      "train epoch 18 step 199, mean loss=0.009448, mean mAp=99.5208\n",
      "train epoch 18 step 249, mean loss=0.010371, mean mAp=99.4083\n",
      "train epoch 18 step 299, mean loss=0.009669, mean mAp=99.4653\n",
      "train epoch 18 step 349, mean loss=0.009355, mean mAp=99.5119\n",
      "train epoch 18 step 399, mean loss=0.010238, mean mAp=99.3808\n",
      "train epoch 18 step 449, mean loss=0.010538, mean mAp=99.3570\n",
      "train epoch 18 step 499, mean loss=0.010369, mean mAp=99.3588\n",
      "train epoch 18 step 549, mean loss=0.010603, mean mAp=99.3830\n",
      "train epoch 18 step 599, mean loss=0.010238, mean mAp=99.3858\n",
      "train epoch 18 step 649, mean loss=0.011011, mean mAp=99.3459\n",
      "train epoch 18 step 699, mean loss=0.011283, mean mAp=99.3142\n",
      "train epoch 18 step 749, mean loss=0.011430, mean mAp=99.3016\n",
      "train epoch 18 step 799, mean loss=0.011361, mean mAp=99.2776\n",
      "train epoch 18 step 849, mean loss=0.011413, mean mAp=99.2759\n",
      "train epoch 18 step 899, mean loss=0.011288, mean mAp=99.2791\n",
      "train epoch 18 step 949, mean loss=0.011278, mean mAp=99.2622\n",
      "train epoch 18 step 999, mean loss=0.011275, mean mAp=99.2533\n",
      "train epoch 18 step 1049, mean loss=0.011216, mean mAp=99.2472\n",
      "train epoch 18 step 1099, mean loss=0.011300, mean mAp=99.2568\n",
      "train epoch 18 step 1149, mean loss=0.011457, mean mAp=99.2248\n",
      "train epoch 18 step 1199, mean loss=0.011432, mean mAp=99.2033\n",
      "train epoch 18 step 1249, mean loss=0.011396, mean mAp=99.2185\n",
      "train epoch 18 step 1299, mean loss=0.011620, mean mAp=99.2165\n",
      "train epoch 18 step 1349, mean loss=0.011509, mean mAp=99.2331\n",
      "train epoch 18 step 1399, mean loss=0.012000, mean mAp=99.2070\n",
      "train epoch 18 step 1449, mean loss=0.012037, mean mAp=99.2128\n",
      "train epoch 18 step 1499, mean loss=0.012029, mean mAp=99.2196\n",
      "train epoch 18 step 1549, mean loss=0.011907, mean mAp=99.2394\n",
      "train epoch 18 step 1599, mean loss=0.012007, mean mAp=99.2371\n",
      "train epoch 18 step 1649, mean loss=0.012302, mean mAp=99.2248\n",
      "train epoch 18 step 1699, mean loss=0.012420, mean mAp=99.2097\n",
      "train epoch 18 step 1749, mean loss=0.012976, mean mAp=99.1714\n",
      "train epoch 18 step 1799, mean loss=0.013206, mean mAp=99.1527\n",
      "train epoch 18 step 1849, mean loss=0.013445, mean mAp=99.1339\n",
      "train epoch 18 step 1899, mean loss=0.013415, mean mAp=99.1479\n",
      "train epoch 18 step 1949, mean loss=0.013243, mean mAp=99.1591\n",
      "train epoch 18 step 1999, mean loss=0.013131, mean mAp=99.1776\n",
      "train epoch 18 step 2049, mean loss=0.013137, mean mAp=99.1743\n",
      "train epoch 18 step 2099, mean loss=0.013069, mean mAp=99.1741\n",
      "train epoch 18 step 2149, mean loss=0.012922, mean mAp=99.1759\n",
      "train epoch 18 step 2199, mean loss=0.012933, mean mAp=99.1624\n",
      "train epoch 18 step 2249, mean loss=0.012939, mean mAp=99.1514\n",
      "train epoch 18 step 2299, mean loss=0.012989, mean mAp=99.1345\n",
      "train epoch 18 step 2349, mean loss=0.013146, mean mAp=99.1214\n",
      "train epoch 18 step 2399, mean loss=0.013443, mean mAp=99.0953\n",
      "train epoch 18 step 2449, mean loss=0.013509, mean mAp=99.0972\n",
      "train epoch 18 step 2499, mean loss=0.013459, mean mAp=99.1061\n",
      "train epoch 18 step 2549, mean loss=0.013395, mean mAp=99.1108\n",
      "train epoch 18 step 2599, mean loss=0.013636, mean mAp=99.0892\n",
      "val epoch 18 step 19, loss=0.1937  mAp=83.3388\n",
      "val epoch 18 step 39, loss=0.1755  mAp=81.6770\n",
      "val epoch 18 step 59, loss=0.1867  mAp=81.0955\n",
      "val epoch 18 step 79, loss=0.2038  mAp=81.3153\n",
      "val epoch 18 step 99, loss=0.1863  mAp=82.1423\n",
      "val epoch 18 step 119, loss=0.1774  mAp=83.1058\n",
      "val epoch 18 step 139, loss=0.1715  mAp=82.9076\n",
      "val epoch 18 step 159, loss=0.1687  mAp=82.6595\n",
      "val epoch 18 step 179, loss=0.1657  mAp=82.9394\n",
      "val epoch 18 step 199, loss=0.1651  mAp=82.8731\n",
      "val epoch 18 step 219, loss=0.1697  mAp=82.7492\n",
      "val epoch 18 step 239, loss=0.1678  mAp=82.9463\n",
      "val epoch 18 step 259, loss=0.1681  mAp=82.9388\n",
      "val epoch 18 step 279, loss=0.1729  mAp=82.9601\n",
      "val epoch 18 step 299, loss=0.1747  mAp=83.2107\n",
      "val epoch 18 step 319, loss=0.1775  mAp=83.0132\n",
      "val epoch 18 step 339, loss=0.1810  mAp=82.9482\n",
      "val epoch 18 step 359, loss=0.1871  mAp=82.5708\n",
      "epoch 18 done!\n",
      "train epoch 19 step 49, mean loss=0.008290, mean mAp=99.5833\n",
      "train epoch 19 step 99, mean loss=0.008816, mean mAp=99.4792\n",
      "train epoch 19 step 149, mean loss=0.010698, mean mAp=99.0556\n",
      "train epoch 19 step 199, mean loss=0.010011, mean mAp=99.0729\n",
      "train epoch 19 step 249, mean loss=0.009171, mean mAp=99.1917\n",
      "train epoch 19 step 299, mean loss=0.010614, mean mAp=99.1583\n",
      "train epoch 19 step 349, mean loss=0.011668, mean mAp=98.9971\n",
      "train epoch 19 step 399, mean loss=0.012871, mean mAp=98.9072\n",
      "train epoch 19 step 449, mean loss=0.012553, mean mAp=98.9731\n",
      "train epoch 19 step 499, mean loss=0.012053, mean mAp=99.0133\n",
      "train epoch 19 step 549, mean loss=0.011397, mean mAp=99.0727\n",
      "train epoch 19 step 599, mean loss=0.012075, mean mAp=99.0377\n",
      "train epoch 19 step 649, mean loss=0.011597, mean mAp=99.0925\n",
      "train epoch 19 step 699, mean loss=0.011958, mean mAp=99.0978\n",
      "train epoch 19 step 749, mean loss=0.012270, mean mAp=99.0646\n",
      "train epoch 19 step 799, mean loss=0.012628, mean mAp=98.9876\n",
      "train epoch 19 step 849, mean loss=0.012328, mean mAp=99.0423\n",
      "train epoch 19 step 899, mean loss=0.012579, mean mAp=99.0061\n",
      "train epoch 19 step 949, mean loss=0.012679, mean mAp=98.9883\n",
      "train epoch 19 step 999, mean loss=0.012404, mean mAp=99.0326\n",
      "train epoch 19 step 1049, mean loss=0.012120, mean mAp=99.0628\n",
      "train epoch 19 step 1099, mean loss=0.012071, mean mAp=99.0618\n",
      "train epoch 19 step 1149, mean loss=0.011941, mean mAp=99.0990\n",
      "train epoch 19 step 1199, mean loss=0.011835, mean mAp=99.1227\n",
      "train epoch 19 step 1249, mean loss=0.011657, mean mAp=99.1544\n",
      "train epoch 19 step 1299, mean loss=0.011718, mean mAp=99.1517\n",
      "train epoch 19 step 1349, mean loss=0.011663, mean mAp=99.1461\n",
      "train epoch 19 step 1399, mean loss=0.012194, mean mAp=99.0822\n",
      "train epoch 19 step 1449, mean loss=0.012091, mean mAp=99.0995\n",
      "train epoch 19 step 1499, mean loss=0.011908, mean mAp=99.1101\n",
      "train epoch 19 step 1549, mean loss=0.012050, mean mAp=99.0971\n",
      "train epoch 19 step 1599, mean loss=0.012157, mean mAp=99.0993\n",
      "train epoch 19 step 1649, mean loss=0.012196, mean mAp=99.1114\n",
      "train epoch 19 step 1699, mean loss=0.012363, mean mAp=99.0902\n",
      "train epoch 19 step 1749, mean loss=0.012413, mean mAp=99.1114\n",
      "train epoch 19 step 1799, mean loss=0.012436, mean mAp=99.1164\n",
      "train epoch 19 step 1849, mean loss=0.012328, mean mAp=99.1335\n",
      "train epoch 19 step 1899, mean loss=0.012222, mean mAp=99.1439\n",
      "train epoch 19 step 1949, mean loss=0.012139, mean mAp=99.1584\n",
      "train epoch 19 step 1999, mean loss=0.011975, mean mAp=99.1669\n",
      "train epoch 19 step 2049, mean loss=0.011794, mean mAp=99.1872\n",
      "train epoch 19 step 2099, mean loss=0.012086, mean mAp=99.1401\n",
      "train epoch 19 step 2149, mean loss=0.012336, mean mAp=99.1049\n",
      "train epoch 19 step 2199, mean loss=0.012388, mean mAp=99.1012\n",
      "train epoch 19 step 2249, mean loss=0.012426, mean mAp=99.0990\n",
      "train epoch 19 step 2299, mean loss=0.012315, mean mAp=99.1068\n",
      "train epoch 19 step 2349, mean loss=0.012420, mean mAp=99.1116\n",
      "train epoch 19 step 2399, mean loss=0.012584, mean mAp=99.0942\n",
      "train epoch 19 step 2449, mean loss=0.012696, mean mAp=99.0761\n",
      "train epoch 19 step 2499, mean loss=0.013010, mean mAp=99.0716\n",
      "train epoch 19 step 2549, mean loss=0.013060, mean mAp=99.0640\n",
      "train epoch 19 step 2599, mean loss=0.013038, mean mAp=99.0619\n",
      "val epoch 19 step 19, loss=0.2455  mAp=82.3191\n",
      "val epoch 19 step 39, loss=0.2167  mAp=80.7572\n",
      "val epoch 19 step 59, loss=0.2038  mAp=81.6713\n",
      "val epoch 19 step 79, loss=0.2202  mAp=80.5529\n",
      "val epoch 19 step 99, loss=0.2062  mAp=82.1627\n",
      "val epoch 19 step 119, loss=0.1959  mAp=83.0398\n",
      "val epoch 19 step 139, loss=0.1885  mAp=82.3637\n",
      "val epoch 19 step 159, loss=0.1888  mAp=82.0073\n",
      "val epoch 19 step 179, loss=0.1826  mAp=82.8896\n",
      "val epoch 19 step 199, loss=0.1815  mAp=83.0102\n",
      "val epoch 19 step 219, loss=0.1833  mAp=83.0645\n",
      "val epoch 19 step 239, loss=0.1798  mAp=83.2426\n",
      "val epoch 19 step 259, loss=0.1756  mAp=83.6451\n",
      "val epoch 19 step 279, loss=0.1799  mAp=83.7162\n",
      "val epoch 19 step 299, loss=0.1818  mAp=83.7851\n",
      "val epoch 19 step 319, loss=0.1858  mAp=83.4719\n",
      "val epoch 19 step 339, loss=0.1887  mAp=83.3848\n",
      "val epoch 19 step 359, loss=0.1957  mAp=83.0380\n",
      "epoch 19 done!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print '开始训练'\n",
    "print 'red==train;green==val...initial plt'\n",
    "fig=plt.figure()\n",
    "ax_loss=fig.add_subplot(1,2,1)\n",
    "plt.title('loss')\n",
    "plt.xlabel(\"epoch\") \n",
    "ax_acc=fig.add_subplot(1,2,2)\n",
    "plt.title('acc')\n",
    "plt.xlabel(\"epoch\")  \n",
    "ax_loss.grid(True) #添加网格\n",
    "ax_acc.grid(True) #添加网格\n",
    "for t in range(epoch_num):\n",
    "    model.train()\n",
    "    mAP_train = []\n",
    "    loss_train= []\n",
    "    for i, (img, label) in enumerate(train_loader):\n",
    "        img = img.cuda()\n",
    "        label = label.cuda()\n",
    "        label = Variable(label)\n",
    "        input_img = Variable(img)\n",
    "\n",
    "        output = model(input_img)\n",
    "        model.zero_grad()\n",
    "        mAP_train.append(compute_mAP(label.data, output.data))#计算准确率\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_train.append(loss.data.cpu()[0])\n",
    "        if (i+1)%50 == 0:\n",
    "            print \"train epoch %d step %d, mean loss=%.6f, mean mAp=%.4f\"%(t, i, np.mean(loss_train),100 * np.mean(mAP_train))\n",
    "    #绘图\n",
    "    ax_loss.scatter(t,np.mean(loss_train),c='r')\n",
    "    ax_acc.scatter(t,100 * np.mean(mAP_train),c='r')\n",
    "    #测试\n",
    "    model.eval()\n",
    "    mAP_val = []\n",
    "    loss_val= []\n",
    "    for i, (img, label) in enumerate(val_loader):\n",
    "        img = img.cuda()\n",
    "        label = label.cuda()\n",
    "        label = Variable(label)\n",
    "        input_img = Variable(img)\n",
    "        output = model(input_img)\n",
    "        mAP_val.append(compute_mAP(label.data, output.data))  # 计算准确率\n",
    "        loss = criterion(output, label)\n",
    "        loss_val.append(loss.data.cpu()[0])\n",
    "        if (i+1)%20 == 0:\n",
    "            print \"val epoch %d step %d, loss=%.4f  mAp=%.4f\" %(t, i, np.mean(loss_val),100 * np.mean(mAP_val))\n",
    "    #绘图\n",
    "    ax_loss.scatter(t,np.mean(loss_val),c='g')\n",
    "    ax_acc.scatter(t,100 * np.mean(mAP_val),c='g')\n",
    "    print('epoch %d done!'%t)\n",
    "    model.save_v2(save_dir=ex_dir,epoch=t,train_stat=mAP_train,val_stat=mAP_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
